article id="http://dx.doi.org/10.1073/pnas.1512144113"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Reconstructing representations of dynamic visual objects in early visual cortex  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
The visual system is often presented with degraded and partial visual information; thus, it must extensively fill in details not present in the physical input.  #@NEW_LINE#@#  Does the visual system fill in object-specific information as an objects features change in motion?  #@NEW_LINE#@#  Here, we show that object-specific features (orientation) can be reconstructed from neural activity in early visual cortex (V1) while objects undergo dynamic transformations.  #@NEW_LINE#@#  Furthermore, our results suggest that this information is not generated by averaging the physically present stimuli or by mechanisms involved in visual imagery, which also requires internal reconstruction of information not physically present.  #@NEW_LINE#@#  Our study provides evidence that V1 plays a unique role in dynamic filling-in of integrated visual information during kinetic object transformations via feedback signals.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
As raw sensory data are partial, our visual system extensively fills in missing details, creating enriched percepts based on incomplete bottom-up information.  #@NEW_LINE#@#  Despite evidence for internally generated representations at early stages of cortical processing, it is not known whether these representations include missing information of dynamically transforming objects.  #@NEW_LINE#@#  Long-range apparent motion (AM) provides a unique test case because objects in AM can undergo changes both in position and in features.  #@NEW_LINE#@#  Using fMRI and encoding methods, we found that the intermediate orientation of an apparently rotating grating, never presented in the retinal input but interpolated during AM, is reconstructed in population-level, feature-selective tuning responses in the region of early visual cortex (V1) that corresponds to the retinotopic location of the AM path.  #@NEW_LINE#@#  This neural representation is absent when AM inducers are presented simultaneously and when AM is visually imagined.  #@NEW_LINE#@#  Our results demonstrate dynamic filling-in in V1 for object features that are interpolated during kinetic transformations.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Reconstructing_Dynamically_Interpolated_Visual_Features_During_AM  #@NEW_LINE#@#  
Does V1 hold interpolated visual information that is not present in the bottom-up input but is dynamically reconstructed as an objects features change in motion?  #@NEW_LINE#@#  To address this question, we investigated whether neural activity in V1 can hold reconstructed interpolated feature information of an object engaged in AM.  #@NEW_LINE#@#  Specifically, using fMRI and a forward-encoding model of orientation (29, 31), we reconstructed population-level, orientation-selective tuning responses in the nonstimulated region of V1 corresponding to the AM path and investigated whether these responses were tuned to the presumed intermediate orientations when viewing apparent rotation.  #@NEW_LINE#@#  
To examine feature-specific representations on the AM path, we induced apparent rotation in the right visual field by alternately presenting two vertically aligned gratings.  #@NEW_LINE#@#  By tilting the gratings from 45° and 135° by ±10°, we induced two directions of rotation, inward or outward, and subjects were cued to see either direction in a given block (Fig 1).  #@NEW_LINE#@#  Critically, with this manipulation, there were two possible intermediate orientations depending on the direction of AM; horizontal (90°) for outward and vertical (0°) for inward rotation.  #@NEW_LINE#@#  If orientation-selective populations of neurons in V1 reconstruct the intermediate representation during the perception of AM, feature-selective population responses tuned to the presumed intermediate orientations (vertical or horizontal) would be found in the region of V1 corresponding to the AM path.  #@NEW_LINE#@#  
We first assessed the mean amplitude of the BOLD activation for each AM direction, separately for three regions of interest (ROIs) in V1, corresponding to the locations of the stimuli in the upper (Upper-Stim) or lower (Lower-Stim) visual quadrants or the midpoint between the stimuli on the AM path (Fig 2A).  #@NEW_LINE#@#  All ROIs were defined based on independent functional localizers and standard retinotopic mapping procedures (32, 33) (SI Materials and Methods).  #@NEW_LINE#@#  Whereas there was substantial activation in the stimulus location ROIs (Upper-Stim and Lower-Stim) during AM-stimulus presentation (016 s) for both vertical and horizontal (Fig 2B), because of presentation of the stimulus at these locations, activation remained near baseline in the AM-path ROI for both AM directions (Fig 2C and Fig S1), because no bottom-up stimulus was presented at this location.  #@NEW_LINE#@#  For the stimulus locations and AM-path ROI, a repeated-measures analysis of variance (ANOVA) comparing the mean activation during stimulus presentation between vertical and horizontal revealed no effect of AM direction [Upper-Stim: F(1, 6) = 0.021, P = 0.889; Lower-Stim: F(1, 7) = 0.035, P = 0.857; AM path: F(1, 7) = 1.305, P = 0.704].  #@NEW_LINE#@#  This finding indicates that any differences found in estimated orientation-tuning responses between vertical and horizontal cannot be attributed to global differences in mean activation.  #@NEW_LINE#@#  
Next, we examined our main question of whether population-level, orientation-channel responses in the AM-path ROI are tuned to the interpolated orientations when AM is viewed.  #@NEW_LINE#@#  To build an encoding model of orientation (29, 31), each subject completed tuning runs in which the subject viewed whole-field gratings in eight different orientations (SI Materials and Methods).  #@NEW_LINE#@#  Using these data, we generated encoding models for hypothetical orientation channels based on the responses to each orientation for each voxel (validated with a leave-one-run-out procedure; Fig S2).  #@NEW_LINE#@#  We then used these models to extract the estimated responses for each orientation channel and reconstruct the population-level, orientation-tuning responses during the experimental conditions, for each ROI separately (Materials and Methods).  #@NEW_LINE#@#  
The orientation tuning of the AM-path ROI peaks at 112.5° for the horizontal condition and 135° for the vertical condition (Fig 3C).  #@NEW_LINE#@#  An ANOVA comparing channel responses between vertical and horizontal revealed a significant interaction between orientation and AM direction (P = 0.008)* but no effect of AM direction (P = 0.08), indicating no global differences between vertical and horizontal but crucially a shift of tuning toward the presumed intermediate orientation.  #@NEW_LINE#@#  Paired t tests also confirmed higher responses at channels corresponding to the presupposed interpolated orientations for each AM direction (90° in vertical: P = 0.011; 0° in horizontal: P = 0.016).  #@NEW_LINE#@#  These tuning responses suggest that when viewing AM, intermediate orientations, not physically present, are interpolated and represented in regions of V1 retinotopic to the AM path.  #@NEW_LINE#@#  Tuning responses for the intermediate orientations were much degraded in an ROI corresponding to the AM path in V2 (Fig S3).  #@NEW_LINE#@#  This finding is consistent with prior studies that found hypothetically weaker pattern of responses for other high-level representations in V2 (refs.  #@NEW_LINE#@#  6 and 9 but see ref.  #@NEW_LINE#@#  8).  #@NEW_LINE#@#  The result also suggests an intriguing possibility that V1 has a privileged role in cortical filling-in processes that are required for dynamic transitions of stimuli (Discussion).  #@NEW_LINE#@#  
In the stimulus location ROIs (Upper-Stim and Lower-Stim), the channel responses peak at the orientations closest to the actual orientations of the stimulus gratings (45° for Upper-Stim, 135° for Lower-Stim; Fig 3 B and D).  #@NEW_LINE#@#  The tuning curves at the stimulus ROIs are indiscriminable between the vertical and horizontal conditions, likely because of the small difference in the orientation of the presented gratings (20°).  #@NEW_LINE#@#  For the Upper-Stim ROI, comparison between vertical and horizontal revealed no significant effect of AM direction (P = 0.269), or interaction between AM direction and orientation (P = 0.212).  #@NEW_LINE#@#  For the Lower-Stim ROI, there was a significant effect of AM direction (P = 0.006), as well as marginally significant interaction between AM direction and orientation (P = 0.06).  #@NEW_LINE#@#  This effect may be because the extent of the perceived rotation differs slightly from trial to trial; at or near the terminal position, subjects may on occasion perceive incomplete rotation of the stimulus, leading to a representation of the intermediate orientation at the terminal position.  #@NEW_LINE#@#  Early behavioral studies on AM using stimuli appearing to transform also show inter- and intrasubject variability on the transition points of these illusory feature transformations (17).  #@NEW_LINE#@#  Note however that the tuning curve for the population response still peaks at 135° regardless of AM direction, suggesting that the terminal stimulus still dominates the response.  #@NEW_LINE#@#  A paired t test confirmed no significant difference between vertical and horizontal at the stimulus orientation (135°) (P = 0.705).  #@NEW_LINE#@#  
The pattern of tuning responses on the AM path was also absent in the corresponding location in the nonstimulated left visual field (LVF) (Fig S4), indicating that the tuning responses found on the AM path is not likely to arise from inherent tuning biases of the AM-path ROI but from perceiving AM.  #@NEW_LINE#@#  

Feature_Interpolation_Versus_Spatial_Averaging_of_Features  #@NEW_LINE#@#  
An alternative account of the AM-path orientation responses found during AM is that these responses are not generated by dynamic interpolation of the intermediate features but by spatial averaging of the physically present AM inducers (34).  #@NEW_LINE#@#  To test this spatial pooling account, we tested whether a similar tuning profile is found when the same two gratings were flashed simultaneously, abolishing the percept of AM.  #@NEW_LINE#@#  When AM was not perceived between the gratings (Flicker), the tuning curve of the AM-path ROI no longer peaks at the intermediate orientation but instead peaks at the orientations of the stimulus gratings (45° and 135°) in both horizontal and vertical conditions (Fig 3C).  #@NEW_LINE#@#  An ANOVA comparing channel responses of the AM-path ROI between stimulus pair condition revealed a marginally significant overall difference between the different pairs of AM inducers (P = 0.059).  #@NEW_LINE#@#  However, unlike AM, the interaction between condition and orientation did not reach significance (P = 0.118), indicating no significant shift in tuning attributable to stimulus pair.  #@NEW_LINE#@#  Similar to the AM condition, channel responses for the stimulus location ROIs peak at the orientations of those gratings (45° for Upper-Stim, 135° for Lower-Stim; Fig 3 B and D).  #@NEW_LINE#@#  For both ROIs, there was no significant difference between vertical and horizontal at these orientations (Upper: P = 0.472; Lower: P = 0.945).  #@NEW_LINE#@#  
To assess the degree to which channel responses differed between AM and Flicker for the AM-path ROI, an ANOVA was conducted comparing channel responses between vertical and horizontal in AM and Flicker.  #@NEW_LINE#@#  This comparison revealed a significant three-way interaction between AM direction, condition, and orientation (P = 0.05), indicating that the shift in tuning depending on AM direction during AM is different from that during Flicker.  #@NEW_LINE#@#  Next, to further examine whether there is a shift in tuning toward the presumed intermediate orientation during AM compared with Flicker, we centered the tuning curves for each AM direction on the hypothesized peak orientation (90° for horizontal and 0° for vertical) and averaged the channel responses across vertical and horizontal separately for AM and Flicker (Fig 3E).  #@NEW_LINE#@#  Whereas responses peak close to the hypothesized orientation for AM, the responses peak at offsets 45° and 45° for Flicker, because of increased channel responses corresponding to the orientations of the stimulus gratings.  #@NEW_LINE#@#  An ANOVA comparing the channel offset between AM and Flicker revealed a significant effect of orientation (P = 0.006) and a significant interaction between condition and orientation (P = 0.046) but no effect of condition (P = 0.908), indicating differential tuning in AM and Flicker.  #@NEW_LINE#@#  Subsequent paired t tests confirmed responses at the hypothesized peak (0° offset) were significantly larger in AM compared with Flicker (P = 0.008), and responses at offset 45° were higher for Flicker compared with AM (P = 0.007).  #@NEW_LINE#@#  These results suggest the same physical stimuli in the absence of AM (Flicker) elicit no feature interpolation but presumably the spreading activity from the stimulus gratings to the midpoint between them (35).  #@NEW_LINE#@#  This confirms that when perceiving apparent rotation, population activity in V1 corresponding to the AM path represent interpolated orientations not present in the retinal input, and these reconstructed representations are not likely to be generated by spatial averaging of the AM inducers.  #@NEW_LINE#@#  
However, another possibility is that the difference between AM and Flicker may be attributable to increased attention on the AM path when viewing AM compared with Flicker, which could potentially facilitate spatial averaging between the gratings.  #@NEW_LINE#@#  To test this possibility, we examined channel responses for Attention-Controlled Flicker, where the stimuli and procedures were the same as Flicker except subjects monitored for a cross that was briefly presented between the inducers during stimulus presentation to ensure the subjects continuous attention to the spatial locations between the gratings (Materials and Methods).  #@NEW_LINE#@#  Orientation-channel responses for Attention-Controlled Flicker peak at 45° for the AM-path ROI for both horizontal and vertical, exhibiting no tuning for the hypothesized intermediate orientation (Fig 4A).  #@NEW_LINE#@#  An ANOVA comparing original Flicker and Attention-Controlled Flicker showed no effect of condition (P = 0.082) or interaction between condition and orientation (P = 0.591).  #@NEW_LINE#@#  Paired t tests confirmed no significant differences between original and Attention-Controlled Flicker at any orientation channel [including 45° (P = 0.827) and 45° (P = 0.533)].  #@NEW_LINE#@#  Thus, differences in tuning between AM and original Flicker cannot be accounted for by varying degrees of attention across tasks.  #@NEW_LINE#@#  

Feature_Tuning_Induced_by_AM_and_Visual_Imagery  #@NEW_LINE#@#  
Earlier fMRI research on visual imagery suggests that early visual areas, including V1, are activated through imagery (11, 13, 36), and more recent studies using MVPA have found stimulus feature information in V1 in response to imagined orientation (10) or complex images (12).  #@NEW_LINE#@#  Although visual imagery is involved in generating representations from long-term memory and AM is involved in interpolating features from immediate perception, both imagery and AM involve internally generated representations in V1 via top-down processes.  #@NEW_LINE#@#  To address the relation of AM to visual imagery, we compared orientation tuning elicited by actual AM with tuning obtained when the same AM is imagined without any physical stimuli.  #@NEW_LINE#@#  In Visual Imagery runs, subjects did not view but instead only imagined the same stimuli used in AM relative to a cue at the beginning of each block (Fig S5).  #@NEW_LINE#@#  If visually imagining AM without actually viewing it yields similar orientation tuning for the intermediate orientation, this finding would suggest a possibility that the processes involved in imagery may also contribute to the interpolation process in AM.  #@NEW_LINE#@#  
Contrary to this prediction, tuning for the intermediate orientation was not found during imagery.  #@NEW_LINE#@#  Instead, channel responses for the AM-path ROI peaked at 67.5° for both horizontal and vertical (Fig 4B), indicating that orientation tuning for the presumed intermediate orientation is substantially degraded when generated entirely by imagery.  #@NEW_LINE#@#  Unlike tuning responses induced by AM, neither a significant effect of AM direction (P = 0.922) nor an interaction between orientation and AM direction (P = 0.756) was found.  #@NEW_LINE#@#  Whereas feature interpolation during AM elicits population-scale tuning profiles, feature representations during visual imagery elicit only a seemingly uninformative signal in the same population of neurons.  #@NEW_LINE#@#  This result indicates that visual imagery alone may not be sufficient to elicit similar tuning responses to those evoked by AM for interpolated features.  #@NEW_LINE#@#  


SI_Materials_and_Methods  #@NEW_LINE#@#  
Tuning_Runs  #@NEW_LINE#@#  
To construct a forward-encoding model of orientation (29, 31), subjects were scanned on 10 model estimation runs in which a whole-field, full-contrast gray-scale sinusoidal grating (0.5 cpd) was flickered at 2 Hz in one of eight possible orientations (covering 180° in steps of 22.5°) for 12 s. Each run contained three blocks per stimulus orientation.  #@NEW_LINE#@#  On each presentation, the spatial phase of the grating was randomly selected from four possible phases to attenuate the perception of AM (9).  #@NEW_LINE#@#  The grating had a diameter of 24°, with a small circular aperture removed around fixation (2.5° radius).  #@NEW_LINE#@#  The order of orientations was randomized on each scan with the constraint that the same orientation would not be presented on successive trials.  #@NEW_LINE#@#  Subjects responded to an occasional contrast reduction in the grating (21% of the trials).  #@NEW_LINE#@#  

Identifying_ROIs  #@NEW_LINE#@#  
In addition to the main experimental runs, all subjects were scanned with retinotopic mapping and a visual field localizer to map the ROIs in retinotopic visual cortex.  #@NEW_LINE#@#  ROIs were defined for the stimulus locations and AM path in the right visual field, as well as the horizontal meridian in the LVF, where no stimuli were presented to allow for a controlled comparison region.  #@NEW_LINE#@#  First, ROIs were identified separately in each subject using visual-field localizer runs in which subjects monitored a central fixation dot for an occasional color change while square checkerboard patterns flickering at 8 Hz were presented either at the stimulus locations (4.5° × 4.5°), at the midpoint between the centers of the stimulus locations on the right and mirrored on the left (4.5° height, 9° width), or in an aperture around fixation (1° radius), for 16 s, followed by a 16-s blank fixation.  #@NEW_LINE#@#  Each ROI was defined by the conjunction of direct comparisons of the response to one condition versus each of the others (false-discovery rate: q less_than 0.001).  #@NEW_LINE#@#  There were three blocks per condition per run, and two runs were acquired per subject.  #@NEW_LINE#@#  The retinotopic ROIs were separated into V1 and V2 using retinotopic maps derived from standard phase-mapping procedures (32) (five subjects) or meridian mapping with counterphase flashing horizontal and vertical bowtie-shaped checkerboards (33) (three subjects).  #@NEW_LINE#@#  One subjects V1 Upper-Stim ROI was excluded from analysis because of its small number of voxels.  #@NEW_LINE#@#  The average ROI sizes in V1 were 39 voxels (SEM: 4.8) for the Upper-Stim location, 63 voxels (SEM: 12.6) for the Lower-Stim location, 131 voxels (SEM: 8.7) for the AM path, and 169 voxels (SEM: 18.2) for the LVF control.  #@NEW_LINE#@#  The average size of the AM-path ROI in V2 was 134 voxels (SEM: 25.5).  #@NEW_LINE#@#  

Statistical_Significance_Using_Permutation_Procedure  #@NEW_LINE#@#  
Because our eight basis (half-cycle sinusoid) functions were not independent, the assumption of independence when comparing the channel responses across conditions was violated.  #@NEW_LINE#@#  To evaluate the significance of the reported effects, we first obtained a P value from a standard t test or ANOVA.  #@NEW_LINE#@#  Next, we randomly permuted orientation labels of the tuning data and the forward model was built using this data.  #@NEW_LINE#@#  Then, we randomly permuted the condition labels in the main experimental data and conducted the same statistical analysis on this relabeled dataset.  #@NEW_LINE#@#  We repeated this 10,000 times to generate a distribution of P values and found the probability of obtaining the original P value given this distribution (31).  #@NEW_LINE#@#  The P values reported reflect this probability.  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
Here, we demonstrate that representations of features, which are absent in the retinal input but are interpolated during dynamic object transitions, can be reconstructed in population-scale feature responses in V1.  #@NEW_LINE#@#  Tuning profiles in areas of V1 retinotopically mapped to the AM path revealed selectivity for the internally reconstructed orientation during apparent rotation.  #@NEW_LINE#@#  Furthermore, no such tuning was found when the same gratings were presented simultaneously (Flicker), ruling out the alternative account that the orientation tuning observed during AM is generated by spatial averaging of the AM inducers.  #@NEW_LINE#@#  The same Flicker stimuli with subjects attention directed between the two gratings similarly did not yield the same tuning responses as AM.  #@NEW_LINE#@#  These results confirm that the population tuning for interpolated orientations found during AM is specific to perceiving AM.  #@NEW_LINE#@#  
V1 has traditionally been viewed as a site for processing rudimentary, local visual features.  #@NEW_LINE#@#  However, there is increasing evidence that a wide variety of high-level information is also represented in V1, suggesting that neural activity in V1 reflects feedback or top-down influence from high-level cortical areas (47).  #@NEW_LINE#@#  Consistently, our study showed that visual features that are absent but inferred from the physical stimulus can be represented in V1.  #@NEW_LINE#@#  However, although our study is in line with this earlier work, our study is also distinct in several key respects.  #@NEW_LINE#@#  
First, our study involves the visual inference of stimulus features that are not actually present anywhere in the retinal input.  #@NEW_LINE#@#  This may be distinguished from the filling-in of information that was presented at other locations in the visual field (57) or maintaining representations of stimuli that were presented but are no longer in view (810).  #@NEW_LINE#@#  These representations may also differ from other types of AM attributed to lateral V1 connectivity (37), which do not involve integration and interpolation of changes in form and motion over large spatial distances.  #@NEW_LINE#@#  Unlike such studies, our experimental manipulation allows us to exclude the possibility of local neuronal activity underlying the formation of content-specific representations in early cortical processing.  #@NEW_LINE#@#  Whereas lateral interactions within V1, mediated by horizontal connections, can contribute to previously reported cortical filling-in (6, 3841), such local interactions in V1 are unlikely to explain the intermediate orientation representations reconstructed during the translational and rotational AM in our study.  #@NEW_LINE#@#  First, the distance between two stimulus gratings (6.5°) exceeds the spatial range suggested for monosynaptic horizontal connections (38).  #@NEW_LINE#@#  More importantly, the interpolated orientations on the AM path are different from the orientation of either AM-inducing grating that is physically presented.  #@NEW_LINE#@#  Hence, tuning for this interpolated representation is unlikely to be a direct result of lateral input from remote neurons activated by the AM inducers.  #@NEW_LINE#@#  In the case of working memory representations in V1, one possible mechanism is a continuation of activity in the neural populations that originally responded to the stimulus perceptually (8, 9).  #@NEW_LINE#@#  This explanation is also unlikely to account for our results because the interpolated orientations in our study were never presented in the bottom-up input.  #@NEW_LINE#@#  
Our finding is more consistent with long-range feedback from higher-order visual areas.  #@NEW_LINE#@#  Higher-order cortical regions specialize in different aspects of the sensory input, and V1 is reciprocally connected to these expert visual modules either directly or indirectly (42, 43).  #@NEW_LINE#@#  The predictive-coding theory of vision suggests that V1 does not just produce the results of local visual feature analysis but instead can serve as a high-resolution buffer that integrates bottom-up sensory input and top-down predictive signals (4446).  #@NEW_LINE#@#  Under this framework, higher-level visual areas project predictions about sensory input to V1, and these internally generated predictions are compared with the externally generated signal.  #@NEW_LINE#@#  We found feature information that was perceived by observers but was not explicitly present in the sensory signal, confirming the presence of predictive signals that carry representations of expected as opposed to actual input.  #@NEW_LINE#@#  
Another key distinction of our study is that, unlike former work using static stimuli, we examined dynamic visual representations that require the integration of both form and motion information.  #@NEW_LINE#@#  Previous demonstrations of cortical filling-in in V1, such as translational AM (23) or representations of working memory and image context (6, 8, 9) focus on either spatiotemporal motion information or object featural information only.  #@NEW_LINE#@#  However, objects in motion are often accompanied by changes in form (e.g., rotation), requiring the visual system to integrate spatiotemporal motion and object-specific feature information, and our study asks whether this class of dynamic percepts can be reconstructed based on sparse sensory input using the phenomenon of transitional AM where objects appear to change both in their position and in their features (orientation).  #@NEW_LINE#@#  
Prior fMRI research using translational AM found that AM leads to increased BOLD response in the region of V1 corresponding to the AM path (22, 23, 25).  #@NEW_LINE#@#  Receptive field (RF) sizes in early visual areas, particularly V1, are too small to account for interactions between the AM inducers in long-range AM (37, 39).  #@NEW_LINE#@#  Thus, later studies using electroencephalography (EEG), dynamic causal modeling with fMRI, and optical imaging on ferrets proposed this AM-induced BOLD signal in V1 is driven by feedback from higher-order visual areas with larger RF sizes, such as motion-processing area human middle temporal complex (hMT+)/V5 (24, 26, 47).  #@NEW_LINE#@#  Furthermore, disruption of these signals in V5 via transcranial magnetic stimulation was recently shown to affect the AM percept (48).  #@NEW_LINE#@#  
Despite these findings, the representational content of the AM-induced signal in V1 and the cortical feedback mechanisms required for the reconstruction of feature information in dynamic transitions remained largely unknown.  #@NEW_LINE#@#  Specifically, it was not clear in this earlier work whether the AM-induced activation reflected spatiotemporal motion signals alone or contained information about features of the objects engaged in AM.  #@NEW_LINE#@#  The latter possibility is suggested by behavioral evidence showing feature-specific impairment of target detection on the AM path (18, 19).  #@NEW_LINE#@#  The results of the present study provide the first neuroimaging evidence to our knowledge for the neural reconstruction of interpolated object features on the AM path in V1, which should involve information not only from dorsal (motion) but also from ventral (object feature) processing streams.  #@NEW_LINE#@#  
Because the perception of AM in the present study requires the integration of motion and shape information, the suggested role of feedback from hMT+/V5 outlined above does not provide a sufficient explanation of our findings.  #@NEW_LINE#@#  Given abundant feedback connections from the ventral visual stream to V1, including projections from V4 and the inferotemporal cortex (49, 50), we hypothesize that object-specific featural information fed back from these high-level ventral areas, such as V4 and the lateral occipital complex (LOC) to V1, may also play a critical role in reconstructing dynamic visual representation in V1 during transitional AM (51).  #@NEW_LINE#@#  Object form and motion information can be integrated through interactions between multiple regions of visual cortex, including recurrent reciprocal interactions between high-level visual areas processing form or motion.  #@NEW_LINE#@#  Considering the unique status of V1 as a high-resolution, integrating buffer and the need to process form relationships and motion trajectories to perceive transitional AM, our study suggests an alternative possibility that motion and feature information from dorsal and ventral streams are separately fed back to V1, where these sets of information are combined.  #@NEW_LINE#@#  Transitional AM provides an ideal stimulus to further investigate the mechanisms underlying the interpolation of integrated visual information in future research.  #@NEW_LINE#@#  
The interpolated AM percept might be generated by mental imagery processes, which have also been argued to involve predictive coding (10, 12).  #@NEW_LINE#@#  However, in our study, when AM was purely imagined without AM inducers, tuning responses appear to be largely absent.  #@NEW_LINE#@#  Although the precise mechanisms underlying visual imagery and its relation to other top-down processes remain to be examined (10, 12, 36), these results suggest that AM may either involve different underlying neural mechanisms from imagery or induce a different magnitude of responses from a common or partially shared mechanism.  #@NEW_LINE#@#  
It is worth noting that even though the level of mean activation in the AM-path ROI remains near baseline when perceiving AM, reliable population tuning responses for the interpolated orientation were still found.  #@NEW_LINE#@#  Recent VSD imaging studies demonstrate that subthreshold cortical activity in ferret area 17, cat area 18, and in mouse V1 could contribute to the perception of AM and other types of illusory motion (26, 27, 39).  #@NEW_LINE#@#  The signals measured with VSD imaging likely reflect synchronized, subthreshold synaptic activity that can lead to observable local field potentials (LFPs), which are known to correlate with fMRI signals (38, 52).  #@NEW_LINE#@#  If interpolated features during AM are reconstructed through this synaptic activity along retinotopically organized populations of orientation-selective neurons in V1 representing the AM path, fMRI reflecting LFPs is well suited for measuring such neural responses in V1.  #@NEW_LINE#@#  
In sum, these results provide novel evidence that object-specific representations can be interpolated and reconstructed in population-scale feature tuning in V1 when an object transforms in motion.  #@NEW_LINE#@#  This finding is consistent with the predictive coding theory that V1 can hold visual information that is not physically present but is spatiotemporally filled-in to sustain our perceptual interpretations of dynamic visual objects via predictive signals from higher cortical areas.  #@NEW_LINE#@#  By examining how visual features of an object that are interpolated during dynamic transitions, are represented in large-scale, feature-selective responses in V1, this study provides clues on the role of V1 in generating dynamic object representations based on both motion and shape information via feedback processes.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
Subjects  #@NEW_LINE#@#  
Nine subjects were recruited from Dartmouth College, and one was excluded because of incompletion of the experiment.  #@NEW_LINE#@#  All subjects gave informed consent (approved by Dartmouth Committee for the Protection of Human Subjects) and were financially compensated for their time.  #@NEW_LINE#@#  

AM_and_Flicker  #@NEW_LINE#@#  
All stimuli were created and presented with MATLAB using Psychophysics Toolbox (53, 54).  #@NEW_LINE#@#  Each subject completed two sessions: one for the AM condition and one for the control condition (Flicker).  #@NEW_LINE#@#  In the AM session, subjects viewed alternating presentations of a full-contrast sinusoidal grating at the upper and lower right corners in the right visual field [2.5° radius, 9° eccentricity, 0.9 cycles per degree (cpd)].  #@NEW_LINE#@#  To bias the subject to view one of two directions of rotation (outward or inward), we used slightly different orientation pairs (55° and 125° for outward, 35° and 145° for inward).  #@NEW_LINE#@#  The different perceived directions of rotation led to different intermediate orientations on the AM path: horizontal (90°) for outward and vertical (0°) for inward rotation (Fig 1, Inset).  #@NEW_LINE#@#  Each subject received practice trials before the start of the session until the subject could view either direction at will.  #@NEW_LINE#@#  Subjects fixated at the center of the screen throughout the experiment.  #@NEW_LINE#@#  At the beginning of each block, one of two arrow cues was briefly presented (0.5 s) to indicate the intended direction of AM.  #@NEW_LINE#@#  The cue was followed by 1.5 s of fixation and then 14 s of AM in which the two gratings were successively presented for 0.2 s with a 0.2-s interstimulus interval (ISI).  #@NEW_LINE#@#  Each block was followed by a 16-s blank fixation period.  #@NEW_LINE#@#  Subjects reported the perceived direction of rotation throughout blocks, and blocks in which subjects perceived direction of AM was unstable or different from the cued direction were excluded from analysis (average: 4% per subject).  #@NEW_LINE#@#  
The same stimuli were used in the second session (Flicker), except that gratings were flashed simultaneously instead of sequentially, at the same rate as AM (ISI: 0.2 s).  #@NEW_LINE#@#  This abolished the percept of AM.  #@NEW_LINE#@#  The same arrow cues were used with the specific grating pairs corresponding to outward and inward rotation in AM.  #@NEW_LINE#@#  Subjects reported which cue was presented, and blocks with incorrect responses were excluded from analysis (average: 1% per subject).  #@NEW_LINE#@#  Both sessions consisted of eight runs, each including three blocks per condition (AM: inward vs. outward; Flicker: inward inducers vs. outward inducers) for a total of six blocks per run.  #@NEW_LINE#@#  The order of blocks was randomized for each subject.  #@NEW_LINE#@#  Each session additionally contained five tuning runs (see Forward Model), interleaved with the eight main experimental runs.  #@NEW_LINE#@#  

Attention-Controlled_Flicker  #@NEW_LINE#@#  
To examine possible influences of attention on the neural responses found in the Flicker condition, five of the subjects completed an additional session of six runs of Attention-Controlled Flicker.  #@NEW_LINE#@#  The design was identical to the original Flicker except that in 22% of the trials, a cross appeared at any location on the AM path.  #@NEW_LINE#@#  Subjects reported whether the vertical or horizontal arm was longer (average accuracy: 93%).  #@NEW_LINE#@#  Blocks in which a cross appeared were excluded from analysis.  #@NEW_LINE#@#  

Visual_Imagery  #@NEW_LINE#@#  
The Visual Imagery experiment was the same as the AM experiment, except that instead of viewing the AM-inducing gratings, subjects visually imagined the AM stimulus while centrally fixating.  #@NEW_LINE#@#  During 16-s blocks, subjects imagined outward or inward apparent rotation of the same gratings used in the AM session corresponding to the cue at the beginning of each block.  #@NEW_LINE#@#  The end of the block was marked by a brief change in the color of the fixation dot.  #@NEW_LINE#@#  To ensure attention to the spatial locations that correspond to the AM path, subjects performed the same cross-task as in Attention-Controlled Flicker (average accuracy: 72%).  #@NEW_LINE#@#  Blocks in which subjects did not imagine the intended direction of AM (average: 2%) or a cross appeared were excluded from analysis.  #@NEW_LINE#@#  Each subject completed six runs, each consisting of three blocks per AM direction (inward vs. outward).  #@NEW_LINE#@#  The order of AM directions was randomized per subject.  #@NEW_LINE#@#  

fMRI_Scanning_and_Data_Analysis  #@NEW_LINE#@#  
fMRI scanning was conducted on a 3T scanner (Philips Achieva) at the Dartmouth Brain Imaging Center at Dartmouth College, using a 32-channel head coil.  #@NEW_LINE#@#  For each subject, anatomical scans were obtained using a high-resolution T1-weighted magnetization-prepared rapid gradient-echo sequence (1 × 1 × 1 mm).  #@NEW_LINE#@#  Functional runs were obtained with a gradient-echo echo-planar imaging (EPI) sequence (repetition time: 2 s; echo time: 30 ms; field of view: 250 × 250 mm; voxel size: 2 × 2 × 1.5 mm; 0.5-mm gap; flip angle: 70°; 25 transverse slices, approximately aligned to the calcarine sulcus).  #@NEW_LINE#@#  fMRI data analysis was conducted using AFNI (55) and in-house Python scripts.  #@NEW_LINE#@#  Data were preprocessed to correct for head motion.  #@NEW_LINE#@#  To account for different distances from the radio-frequency coil, each voxels time series was divided by its mean intensity.  #@NEW_LINE#@#  To remove low-frequency drift, the data were then high pass-filtered (cutoff frequency of 0.01 Hz) on a voxel-wise basis.  #@NEW_LINE#@#  Only the localizer data were smoothed with a 4-mm full-width at half-maximum Gaussian kernel.  #@NEW_LINE#@#  Forward-encoding analysis was conducted on unsmoothed data to retain the highest level of spatial resolution available.  #@NEW_LINE#@#  The cortical surface of each subjects brain was reconstructed with FreeSurfer (56).  #@NEW_LINE#@#  

Forward_Model  #@NEW_LINE#@#  
To construct a forward model, we used data from the tuning runs (SI Materials and Methods) to model each voxels orientation selectivity as a weighted sum of eight hypothetical orientation channels, each with an idealized orientation tuning curve (a sinusoid raised to the fifth power and centered around one of the eight orientations).  #@NEW_LINE#@#  Following Brouwer and Heeger (29), let m be the number of voxels, n be the number of runs × conditions, and k be the number of orientation channels.  #@NEW_LINE#@#  Ten tuning runs comprised the dataset for model estimation (m × n matrix B1).  #@NEW_LINE#@#  To maximize the signal-to-noise ratio of channel responses, we selected the top 50% of voxels within each ROI that could best discriminate the different orientations in the tuning runs (the top 50% of voxels with highest F statistic in ANOVA of response amplitudes in tuning runs) (31).  #@NEW_LINE#@#  
After preprocessing,  coefficients of each voxel in the ROIs were estimated using a generalized linear model (GLM) analysis that included a linear baseline fit and motion parameters as covariates of no interest;  coefficients for stimulus blocks, extracted per condition, per run, were used as voxel responses.  #@NEW_LINE#@#  coefficients were mapped onto the full rank matrix of channel outputs (C1, k × n), by the weight matrix W (m × k), estimated using a GLM:B1=WC1.  #@NEW_LINE#@#  [1]  #@NEW_LINE#@#  
The ordinary least-squares estimate of W is computed as follows:W^=B1C1T(C1C1T)1.  #@NEW_LINE#@#  [2]  #@NEW_LINE#@#  
Finally, the channel responses (C2) associated with AM and the control experiments were estimated using the weights obtained from Eq.  #@NEW_LINE#@#  2:C^2=(W^TW^)1W^TB2.  #@NEW_LINE#@#  [3]  #@NEW_LINE#@#  
To validate the model in accounting for orientation tuning, a leave-one-run-out procedure was performed on the tuning runs.  #@NEW_LINE#@#  


SI_Results  #@NEW_LINE#@#  
Orientation-Channel_Responses_on_the_AM_Path_in_V2  #@NEW_LINE#@#  
Inspecting the channel outputs for an ROI corresponding to the AM path in V2, we found that whereas tuning for both vertical and horizontal conditions peak at 112.5° for Flicker, channel responses are tuned to 112.5° for horizontal and to 45° for vertical during AM (Fig S3A).  #@NEW_LINE#@#  After averaging across vertical and horizontal for AM and Flicker, responses for both Flicker and AM peak at offsets of 45° and 45° (Fig S3B).  #@NEW_LINE#@#  An ANOVA comparing channel responses between AM and Flicker showed neither an effect of condition (P = 0.510) nor an interaction between condition and orientation (P = 0.406).  #@NEW_LINE#@#  Paired t tests also revealed no significant difference between AM and Flicker at the hypothesized intermediate channel (P = 0.335).  #@NEW_LINE#@#  There are several possible accounts for this lack of tuning in V2 for interpolated orientations in AM.  #@NEW_LINE#@#  This result is consistent with prior studies that found successful classification of activation patterns in nonstimulated regions in V1 but weaker results in V2.  #@NEW_LINE#@#  Serences et al.  #@NEW_LINE#@#  (9) classified the orientation of a grating held in working memory and found above-chance classification only in V1 (but see ref.  #@NEW_LINE#@#  8).  #@NEW_LINE#@#  Also, activation patterns in V1 retinotopic to occluded regions of images were reported to better classify which image was shown in the other quadrants compared with those in V2 (6).  #@NEW_LINE#@#  Thus, it is possible that neither multivoxel pattern-decoding nor forward-encoding methods were able to discriminate the hypothetically weaker pattern of responses for such high-level representations in V2.  #@NEW_LINE#@#  On the other hand, a more intriguing possibility, not mutually exclusive with the above, is that V1 has a privileged role in cortical filling-in processes that are required for dynamic transitions of stimuli.  #@NEW_LINE#@#  Interaction between dorsal and ventral streams of visual processing, combining motion information from the former with object feature information from the latter, required for perceiving AM may occur in V1 via feedback from higher cortical areas to V1 (Discussion).  #@NEW_LINE#@#  

Orientation-Channel_Responses_in_a_Control_ROI  #@NEW_LINE#@#  
To allow for a retinotopically comparable control region, we looked at orientation-selective responses for an ROI corresponding to the AM path mirrored on the LVF, where no stimuli were presented (LVF control).  #@NEW_LINE#@#  During AM, channel responses for the LVF control ROI peak at the 90° channel regardless of AM direction (Fig S4).  #@NEW_LINE#@#  The AM tuning curves are largely different in shape compared with those found in the right visual field AM-path ROI.  #@NEW_LINE#@#  Comparing the averaged AM tuning between left and right visual field ROIs revealed no effect of visual field (P = 0.870) but a significant interaction between visual field and orientation (P = 0.010), indicating differential tuning responses in LVF control and RVF AM-path ROI.  #@NEW_LINE#@#  The results for the LVF control ROI are likely attributable to radial bias of neurons along the horizontal meridian that preferably respond to the 90° orientation (57, 58).  #@NEW_LINE#@#  Hence, the pattern of responses found on the original AM-path ROI is not likely to arise from inherent tuning biases but from perceiving AM.  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
We thank George Wolford and John Serences for their helpful feedback on the statistical analyses as well as Patrick Cavanagh for his beneficial feedback on the manuscript.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  



