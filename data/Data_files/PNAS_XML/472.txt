article id="http://dx.doi.org/10.1073/pnas.1616647113"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Framework for making better predictions by directly estimating variables predictivity  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Good prediction, especially in the context of big data, is important.  #@NEW_LINE#@#  Common approaches to prediction include using a significance-based criterion for evaluating variables to use in models and evaluating variables and models simultaneously for prediction using cross-validation or independent test data.  #@NEW_LINE#@#  The first approach can lead to choosing less-predictive variables, because significance does not imply predictivity.  #@NEW_LINE#@#  The second approach can be improved through considering a variables predictivity as a parameter to be estimated.  #@NEW_LINE#@#  The literature currently lacks measures that do this.  #@NEW_LINE#@#  We suggest a measure that evaluates variables abilities to predict, the I-score.  #@NEW_LINE#@#  The I-score is effective in differentiating between noisy and predictive variables in big data and can be related to a lower bound for the correct prediction rate.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
We propose approaching prediction from a framework grounded in the theoretical correct prediction rate of a variable set as a parameter of interest.  #@NEW_LINE#@#  This framework allows us to define a measure of predictivity that enables assessing variable sets for, preferably high, predictivity.  #@NEW_LINE#@#  We first define the prediction rate for a variable set and consider, and ultimately reject, the naive estimator, a statistic based on the observed sample data, due to its inflated bias for moderate sample size and its sensitivity to noisy useless variables.  #@NEW_LINE#@#  We demonstrate that the I-score of the PR method of VS yields a relatively unbiased estimate of a parameter that is not sensitive to noisy variables and is a lower bound to the parameter of interest.  #@NEW_LINE#@#  Thus, the PR method using the I-score provides an effective approach to selecting highly predictive variables.  #@NEW_LINE#@#  We offer simulations and an application of the I-score on real data to demonstrate the statistics predictive performance on sample data.  #@NEW_LINE#@#  We conjecture that using the partition retention and I-score can aid in finding variable sets with promising prediction rates; however, further research in the avenue of sample-based measures of predictivity is much desired.  #@NEW_LINE#@#  

Illustration of the relationship between predictive and significant sets of variable sets.  #@NEW_LINE#@#  Rectangular space denotes all candidate variable sets.  #@NEW_LINE#@#  Significant sets are identified through traditional significance-tests.  #@NEW_LINE#@#  
A_Brief_Literature_Review_on_VS  #@NEW_LINE#@#  
A related and extremely important literature is that of VS or feature selection, which refers to the practice of selecting a subset of an original group of variables that is later used to construct a model.  #@NEW_LINE#@#  Often VS is used on data of large dimensionality with modest sample sizes (7).  #@NEW_LINE#@#  In the context of high-dimensional data, such as GWAS, this dimensionality reduction can be a crucial step.  #@NEW_LINE#@#  VS approaches are commonly proposed to efficiently search for the best variable sets according to a specified criterion.  #@NEW_LINE#@#  Most performance measures are developed to maximize the probability of selecting the truly important variables but are not direct measures of predictivity.  #@NEW_LINE#@#  Therefore, popular VS approaches do not return reliable assessment of the predictivity of variable sets.  #@NEW_LINE#@#  In contrast, we will propose considering VSA through a reliable, model-free measure used to assess the potential predictivity of a variable set.  #@NEW_LINE#@#  Unlike projection- or compression-based approaches (such as principal component analysis or use of information theory), VSA methods do not change the variables themselves.  #@NEW_LINE#@#  
The types of approaches and tools developed for feature selection are both diverse and varying in degrees of complexity.  #@NEW_LINE#@#  However, there is general agreement that three broad categories of feature selection methods exist: filter, wrapper, and embedded methods.  #@NEW_LINE#@#  Filter approaches tend to select variables through ranking them by various measures (correlation coefficients, entropy, information gains, chi-square, etc.).  #@NEW_LINE#@#  Wrapper methods use black box learning machines to ascertain the predictivity of groups of variables; because wrapper methods often involve retraining prediction models for different variable sets considered, they can be computationally intensive.  #@NEW_LINE#@#  Embedded techniques search for optimal sets of variables via a built-in classifier construction.  #@NEW_LINE#@#  A popular example of an embedded approach is the LASSO method for constructing a linear model, which penalizes the regression coefficients, shrinking many to zero.  #@NEW_LINE#@#  Often cross-validation is used to evaluate the prediction rates.  #@NEW_LINE#@#  
Often, though not always, the goal of these approaches is statistical inference.  #@NEW_LINE#@#  When this is the case, the researcher might be interested in understanding the mechanism relating the explanatory variables with a response.  #@NEW_LINE#@#  Although inference is clearly important, prediction is an important objective as well.  #@NEW_LINE#@#  In this case, the goal of these VS approaches is in inferring the membership of variables in the important set.  #@NEW_LINE#@#  Various numerical criteria have been proposed to identify such variables [e.g., Akaike information criterion (AIC) and Bayesian information criterion (BIC), among others; see chapter 7 in ref.  #@NEW_LINE#@#  8 for a review], which are associated with predictive performance under model assumptions made for the derivation of these criteria.  #@NEW_LINE#@#  However, these criteria were not designed to specifically correlate with predictivity.  #@NEW_LINE#@#  Indeed, we are unaware of a measure that directly attempts to evaluate a variable sets theoretical level of predictivity.  #@NEW_LINE#@#  This paper proposes a model-free parameter for predictivity and its sample estimate.  #@NEW_LINE#@#  For a more comprehensive survey of the feature/VS literature see, among others, refs.  #@NEW_LINE#@#  7, 9, 10, and 11.  #@NEW_LINE#@#  
Although a spectrum of VS approaches exists, many scientists have taken the approach of tackling prediction through the use of important and hard-to-discover influential variables found to be statistically significant in previous studies.  #@NEW_LINE#@#  When these efforts are in the context of high-dimensional data and alongside work investigating variables known to be influential, it might seem reasonable to hope that variables found to be significant can prove useful for predictive purposes as well.  #@NEW_LINE#@#  This approach is in some ways most similar to a univariate filter method, because it is independent of the classifier and has no cross-validation or prediction step for VS. We show in our related work (3) how and why the popular filter approach of VS through statistical significance does not serve the purpose of prediction well.  #@NEW_LINE#@#  For an intuitive illustration of the relationship between predictive and significant sets of variables, see Fig 1.  #@NEW_LINE#@#  Under the context of a significance-test based search for variable sets, the set of variables found to be significant expands as the sample size grows (Fig 1, widening orange dotted ovals).  #@NEW_LINE#@#  However, the set of predictive variables (Fig 1, blue circle) are not susceptible to sample-size changes in the same waybecause predictivity is a population parameterand overlaps, but is not perfectly aligned with, significant sets.  #@NEW_LINE#@#  It is easy to see that in this scenario targeting significant sets may miss the goal of prediction entirely.  #@NEW_LINE#@#  Instead, we suggest that emphasis must be placed on designing measures that directly evaluate variable sets predictivity.  #@NEW_LINE#@#  
Many methods also use out-of-sample testing error rates or cross-validation to ascertain whether prediction is done well.  #@NEW_LINE#@#  This approach was not designed to specifically find a theoretically correct prediction rate for a given variable set; rather, it is simply a performance evaluation of future predictions from a pattern recognition technique on selected variable sets (trained on training data).  #@NEW_LINE#@#  Sometimes the variable sets in the training data are selected through statistics such as the adjusted R squared, AIC, or BIC.  #@NEW_LINE#@#  When pn (or even in instances where pn), a standard in big data, however, these statistics can fail to be useful.  #@NEW_LINE#@#  Again, these criteria were not designed to be directly correlated with a given variable sets predicitivity.  #@NEW_LINE#@#  Using out-of-sampling testing and/or cross-validation techniques additionally requires either setting aside valuable sample data to make sure the variable sets selected under the training set are indeed highly predictive and not just overfitting the data or is often computationally burdensome.  #@NEW_LINE#@#  It becomes important then that we have a good screening mechanism when conducting VSA for removing noisy variables (and thus finding predictive ones), even with constrained amounts of sample data.  #@NEW_LINE#@#  We show in our simulations how poorly we can do in VSA for prediction through training set compared with out-of-sample testing prediction rates (with infinite future testing dataa mostly unattainable, but ideal, scenario).  #@NEW_LINE#@#  An ideal measure for predictivity (or a good VSA measure) reflects a variable sets predictivity.  #@NEW_LINE#@#  In doing so, it would also guide VSA through screening out noisy variables and should correlate well with the out-of-sample correct prediction rate.  #@NEW_LINE#@#  We present a potential candidate measure, the I-score, for evaluating the predictivity of a given variable set in this paper.  #@NEW_LINE#@#  

Toy_Example  #@NEW_LINE#@#  
To highlight some of our key issues, consider a small artificial example.  #@NEW_LINE#@#  Suppose an observed variable Y is defined asY={X1+X2(modulo 2)with prob.1/2,X2+X3+X4(modulo 2)with prob.1/2,[1]where X1,X2,X3 and X4 are 4 of 50 observed and potentially influential variables {Xi;1i50}.  #@NEW_LINE#@#  Each Xi can take values 0 and 1.  #@NEW_LINE#@#  A collection of discrete variables S may be regarded as a discrete variable that takes on a finite number of values.  #@NEW_LINE#@#  Each value defined by S constitutes a cell.  #@NEW_LINE#@#  The collection of all cells forms a partition, S, based on the discrete variables in S. We also assume that the Xi were selected independently to be 1 with probability 0.5, again the simplest case without affecting the general results.  #@NEW_LINE#@#  Clearly, none of the individual Xi has a marginal effect on Y.  #@NEW_LINE#@#  
Scenario I.  #@NEW_LINE#@#  A statistician knows the model and wishes to compute which variable sets are predictive of Y, and how predictive, when =(X1,X2,,X50) is given.  #@NEW_LINE#@#  Because Y depends only on the first four X variables, it is obvious there are two clusters of variable sets S1={X1,X2} and S2={X2,X3,X4} that are potentially useful in his prediction.  #@NEW_LINE#@#  We treat the highest correct prediction rate possible for a given variable set as an important parameter and call this predictivity (c).  #@NEW_LINE#@#  Using the knowledge of the model, we can compute the predictivity for S1 as c(S1)=0.75.  #@NEW_LINE#@#  The predictivity for S2 is c(S2)=0.75 also.  #@NEW_LINE#@#  Incidentally, the predictivity of the union of S1 and S2, c(S1S2), is also 0.75.  #@NEW_LINE#@#  
The statistician realizes that using variable sets S1 and S2 he can predict Y correctly 75% of the time.  #@NEW_LINE#@#  This is indeed the case because, for instance, upon observing =(X1,,X50) the statistician predictsY^=X1+X2(modulo 2).  #@NEW_LINE#@#  
It is easy to verify that the strategy of predicting with S1 returns a 75% prediction accuracy in expectation.  #@NEW_LINE#@#  This is also the highest percent accuracy S1 can theoretically achieve.  #@NEW_LINE#@#  We discuss this in depth shortly.  #@NEW_LINE#@#  This result extends to S2 as well.  #@NEW_LINE#@#  
Scenario II.  #@NEW_LINE#@#  In practice, the statistician rarely has knowledge of the model and instead observes only the data.  #@NEW_LINE#@#  We suggest that the statistician use the partition retention (PR) approach and its corresponding I-score (which we present formally in Alternative Measure: I-Score; see ref.  #@NEW_LINE#@#  4 for the original presentation of the approach or see Eq.  #@NEW_LINE#@#  S2 for I) to identify the influential variable sets.  #@NEW_LINE#@#  Suppose with 400 observations the researcher wishes to identify variable sets with high predictivity and to infer their abilities to predict.  #@NEW_LINE#@#  Using the PR approach he can use the I-score to screen for variable sets with high potential predictivity.  #@NEW_LINE#@#  In this example, S1 and S2 are consistently returned with the highest I-scores (23.71 and 12.79) in simulations.  #@NEW_LINE#@#  Using the inequality in Eq.  #@NEW_LINE#@#  7, which we derive in the following section, the lower bounds for the predictivity of c(S1) and c(S2) are calculated to be 67 and 62%, respectively.  #@NEW_LINE#@#  Eq.  #@NEW_LINE#@#  7 does not require knowledge of the true model as defined in Eq.  #@NEW_LINE#@#  1.  #@NEW_LINE#@#  

Theoretical_Prediction_Rates  #@NEW_LINE#@#  
We contribute to the prediction literature by introducing the prediction rate as a parameter to be directly estimated.  #@NEW_LINE#@#  We show that the PR methods I-score, a sample-based statistic, can be used to construct an asymptotically consistent lower bound for the prediction rate.  #@NEW_LINE#@#  
We deal here with the special case of case control studies where the explanatory variables are discrete, and the outcome variable takes only two values, case or control.  #@NEW_LINE#@#  These results are easily generalized for classification problems, where the dependent variable can take on a finite number of possible values.  #@NEW_LINE#@#  Consider GWAS data of the usual type, with cases and controls.  #@NEW_LINE#@#  Assume that there are nd cases and nu controls.  #@NEW_LINE#@#  Using the traditional Bayesian binary classification setting, we ideally have a prior probability, (w=d), that the state of the next individual, w, is a disease case, d, and (w=u)=1(w=d) that the next individual is a control, u.  #@NEW_LINE#@#  In the following we shall assume that both d and u are equally likely and that the cost of an incorrect classification is the same for both possibilities.  #@NEW_LINE#@#  We generalize to different cost functions and priors for d and u in Generalization to Arbitrary Priors and Generalization to Different Loss and Cost Functions.  #@NEW_LINE#@#  Let the joint distribution of the feature value  and w be P(,w).  #@NEW_LINE#@#  The joint distribution can be expressed as P(w,)=(w|)P()=P(|w)(w), where (w|) is the posterior distribution and (w) is the prior.  #@NEW_LINE#@#  It is easy to see that the best classification rule can be derived by Bayes decision rule for minimizing the posterior probability of error: d if (d|)(u|), otherwise u.  #@NEW_LINE#@#  Here the variable set =(X1,X2,,Xm), with each Xi taking one of the values in {0,1,2}, corresponding to the three possible genotypes for each SNP.  #@NEW_LINE#@#  In this way,  forms a partition, denoted by X, with 3m=m1 elements: ={=j,j=1,,m1:j=(xj1,xj2,,xjm),xjk{0,1,2},1km}.  #@NEW_LINE#@#  
Assuming equal priors, that is, (d)=(u)=12, the correct prediction rate c on  using the full Bayes decision rule can be calculated asc()=c[pd,pu]=12max{pd(),pu()},where pd() and pu() stand for P(|w=d) and P(|w=u), respectively.  #@NEW_LINE#@#  We can easily derive (see Technical Notes, Technical Note 1)c[pd,pu]=12+14j|P(j|d)P(j|u)|.  #@NEW_LINE#@#  [2]  #@NEW_LINE#@#  
This suggests that we can achieve better prediction rates by choosing variable sets corresponding to the probability pairs that lead to large values of j|P(j|d)P(j|u)|.  #@NEW_LINE#@#  In this theoretical setting, it is easy to show that c increases or stays the same when another variable is added to the current variable set.  #@NEW_LINE#@#  This means adding many noisy variables leads to maintaining the same c. Therefore, when sample size is no constraint, we are never hurt in our search for highly predictive variables by simply adding explanatory variables to our current set.  #@NEW_LINE#@#  However, in the realistic world of sample size constraints, a direct search for a variable set with a larger sample estimate of c will fail; we offer a heuristic explanation as to why in the following section.  #@NEW_LINE#@#  We refer to this direct search of c with sample data as the sample analog throughout.  #@NEW_LINE#@#  
Problems_with_the_Sample_Analog  #@NEW_LINE#@#  
The value of c is unknown and must be estimated.  #@NEW_LINE#@#  We may naturally turn to the naive sample estimate of its true theoretical values, which is sometimes referred to as the training rate.  #@NEW_LINE#@#  However, this estimated value of c (where the cell probabilities are replaced by the observed proportions) is nondecreasing with the addition of more variables to a given variable set under evaluation.  #@NEW_LINE#@#  As the partition becomes increasingly finer, we reach a point where there is at maximum a single observation within each partition cell and 100% correct sample prediction rate is attained.  #@NEW_LINE#@#  This is true regardless of the true prediction rate.  #@NEW_LINE#@#  Then, the final estimated prediction rate is equivalent to 100%, rendering it useless as a method for finding predictive variable sets and screening out noisy ones.  #@NEW_LINE#@#  This is a direct result of a sparsity problem that does not occur in our theoretical world but certainly plagues the sample-size-constrained real world.  #@NEW_LINE#@#  (See Technical Notes, Technical Note 2 for a more detailed explanation.  #@NEW_LINE#@#  )We need instead a sample-based measure that can discern adding noisy versus influential variables and identify variable sets with large prediction rates for a given moderate sample size.  #@NEW_LINE#@#  

Alternative_Measure__I-Score  #@NEW_LINE#@#  
We consider this obstacle and suggest an alternative measure, a lower bound to c, which we estimate using the I-score of the PR method (4) in sample data.  #@NEW_LINE#@#  The I-score converges asymptotically to a constant multiple ofI()=j[P(j|d)P(j|u)]2.  #@NEW_LINE#@#  [3]  #@NEW_LINE#@#  
To relate I to c defined in Eq.  #@NEW_LINE#@#  2, we first examine the following Lemma 1, which is derived in Technical Notes, Technical Note 3.  #@NEW_LINE#@#  
Lemma 1.  #@NEW_LINE#@#  For K real values {zj;1iK}, j=1Kzj=a and j=1K|zj|=b, we havej=1Kzj2a2+b22.  #@NEW_LINE#@#  [4]  #@NEW_LINE#@#  
In the case of zj=(P(j|d)P(j|u)) for j, we have a=0.  #@NEW_LINE#@#  It then follows that2i=1k[P(j|d)P(j|u)]2i=1k|P(j|d)P(j|u)|.  #@NEW_LINE#@#  
This suggests that a strategy seeking variable sets with larger values of I can have the parallel effect of encouraging selection of variable sets with larger values of c, yielding better predictors.  #@NEW_LINE#@#  In the following, we present Theorem 1 and Corollary 2 (see Technical Notes, Technical Note 5 and Technical Note 6 for proofs).  #@NEW_LINE#@#  
Theorem 1.  #@NEW_LINE#@#  Under the assumptions that ndn, a value strictly between 0 and 1, and (d)=(u)=1/2, thenlimnsn2In=2(1)2j[P(j|d)P(j|u)]2[5]where = indicates that the left-hand side converges in probability to the right-hand side and sn2=ndnu/n2 (see Technical Notes, Technical Note 5 for more detail).  #@NEW_LINE#@#  
We now show that I defined in Eq.  #@NEW_LINE#@#  3 is a parameter relevant to c().  #@NEW_LINE#@#  Together with Lemma 1, we can use the I-score to derive a useful asymptotic lower bound to the prediction rate of a variable set , c(), as presented in Corollary 2.  #@NEW_LINE#@#  
Corollary 2.  #@NEW_LINE#@#  Under the assumptions in Theorem 1, the following is an asymptotic lower bound for the correct prediction rate:c()12+142limnIn(1).  #@NEW_LINE#@#  [6]  #@NEW_LINE#@#  
Using sample data, the estimated lower bound for c is then12+142In(1).  #@NEW_LINE#@#  [7]  #@NEW_LINE#@#  
The lower bounds presented in the toy example were obtained using the above Eq.  #@NEW_LINE#@#  7.  #@NEW_LINE#@#  
We extend to an arbitrary prior in Corollary 3 (see Generalization to Arbitrary Priors for discussion and proof).  #@NEW_LINE#@#  
Corollary 3.  #@NEW_LINE#@#  Under the assumptions of an arbitrary prior (d) and ndn as n, the correct prediction rate isc[pd,pu]=12+12j|P(j|d)(d)P(j|u)(u)|.  #@NEW_LINE#@#  [8]  #@NEW_LINE#@#  
The last generalization of the proposed framework accounts for incurring different costs (or losses) when making incorrect predictions (see Generalization to Different Loss and Cost Functions for discussion).  #@NEW_LINE#@#  Note that searching for  with larger I-scores is asymptotically equivalent to searching for larger values of the lower bound in Eq.  #@NEW_LINE#@#  6 which is closely related to the correct predictivity of a given variable set , c().  #@NEW_LINE#@#  For example, if a variable set  has a large I-score (substantially larger than 1; see ref.  #@NEW_LINE#@#  4), it is a strong indication that  itself could be a variable set with high predictivity.  #@NEW_LINE#@#  This stands in contrast to many current approaches to prediction [e.g., random forest and least absolute shrinkage and selection operator (LASSO)] that are evaluated for predictivity via cross-validation, which is computer-intensive.  #@NEW_LINE#@#  

Desirable_Properties_of_the_I-Score  #@NEW_LINE#@#  
We note that the I-score is one possible approach to approximating the prediction rate in the sample analog form, and that the search for other potential scores is desirable and needed.  #@NEW_LINE#@#  Nevertheless, several properties of I are particularly appealing.  #@NEW_LINE#@#  
First, I requires no specification of a model for the joint effect of {X1,X2,,Xm} on Y because it is designed to capture the discrepancy between the conditional means of Y on {X1,X2,,Xm} and the mean of Y.  #@NEW_LINE#@#  Second, as mentioned earlier, the I-score does not monotonically increase with the addition of any and all variables as would the sample analog form of c. Rather, given a variable set of size m with m1 truly influential variables, the I-score is typically higher under the influential m1 variables than under all m variables.  #@NEW_LINE#@#  If m1 variables are influential in the sense that any smaller subset of variables is less influential, then removal of a variable to size m2 will decrease the I-score in expectation.  #@NEW_LINE#@#  This natural tendency of the I-score to peak at variable set(s) that lead to high predictivity in the face of noisy variables under the current sample size is crucial.  #@NEW_LINE#@#  
Most important to note, we showed that the I-score can help find variables with high c by identifying variables that have high values of I (recall I=j[P(j|d)P(j|u)]2), which is related to the lower bound of c. An important step to finding these highly predictive variable sets and discarding noisy ones through finding high I-scores is using the backward dropping algorithm (BDA) developed in ref.  #@NEW_LINE#@#  4.  #@NEW_LINE#@#  The algorithm requires drawing many starting sets of variables and recursively dropping random variables and calculating I-scores.  #@NEW_LINE#@#  For more information, see ref.  #@NEW_LINE#@#  4 or BDA.  #@NEW_LINE#@#  


Generalization_to_Arbitrary_Priors  #@NEW_LINE#@#  
A problem that emerges when dealing with case-control data such as GWAS is that prior information on observing the next person as a disease case is unknown and not easily estimated from empirical data.  #@NEW_LINE#@#  Priors are defined by circumstances and contexts within which the case-control data are sampledeach dataset requires its own unique and unknown prior at that point in time.  #@NEW_LINE#@#  
Corollary 3.  #@NEW_LINE#@#  Under the assumptions of an arbitrary prior (d) and ndn as n, the correct prediction rate can be easily seen as  #@NEW_LINE#@#  
Let the modified score In be defined asnsn2In=14jXnj2[y¯j((d))(1y¯j)((u)1)]2.  #@NEW_LINE#@#  
Then we havelimnsn2Inn=14jX[P(j|d)(d)P(j|u)(u)]2.  #@NEW_LINE#@#  [S5]  #@NEW_LINE#@#  
Similar lower bounds to Corollary 2 can then be derived asc[pXd,pXu]=12+12jX|P(j|d)(d)P(j|u)(u)|12+12limn(1)In2na2[S6]where a=j(P(j|d)(d)P(j|u)(u))=(d)(u).  #@NEW_LINE#@#  
Similar to Corollary 1, Eq.  #@NEW_LINE#@#  S5 is a direct consequence of Eq.  #@NEW_LINE#@#  S6 and Lemma 1 (but with zj replaced by |P(j|d)(d)P(j|u)(u)|).  #@NEW_LINE#@#  

Generalization_to_Different_Loss_and_Cost_Functions  #@NEW_LINE#@#  
Thus far we have used a 01 loss on the binary classification problem.  #@NEW_LINE#@#  The 01 loss treats false negatives and false positives equally.  #@NEW_LINE#@#  In real applications, the scientist may wish to weigh the costs of different incorrect predictions differently.  #@NEW_LINE#@#  For instance, failing to detect a cancer patient may be deemed a more costly mistake to make than that of misclassifying a healthy patient because ameliorating the former mistake later on can be more difficult.  #@NEW_LINE#@#  The different cost amounts in making a loan decision is another example.  #@NEW_LINE#@#  The cost of lending to a defaulter may be seen as greater than that of the loss-of-business cost of declining a loan to a nondefaulter due to some positive level of risk aversion.  #@NEW_LINE#@#  Let loss function L be defined asL(d,u)=ld,L(u,d)=lu[S7]andL(d,d)=L(u,u)=0[S8]where ld and lu are the prices paid (or losses incurred) for misclassifying a diseased individual to the healthy class or a healthy person to a diseased class, respectively.  #@NEW_LINE#@#  We can derive the optimum Bayes solution by minimizing the expected predicted loss, that is, to assign future observations to the class with less loss, given its j value.  #@NEW_LINE#@#  We simply assign a test sample with partition (predictor) j to d ifP(j|d)(d)L(d,u)less_thanP(j|u)(u)L(u,d)otherwise, assign to u. Equivalently, choose d ifP(j|d)(d)ldless_thanP(j|u)(u)luotherwise u.  #@NEW_LINE#@#  In this way, the expected loss of adopting this rule is thus:el=12jmin{aj,bj},where aj=P(j|d)(d)ld and bj=P(j|u)(u)lu.  #@NEW_LINE#@#  The random rule of classifying an individual to the healthy class or disease class has an expected loss of=12(aj+bj)=12((d)ld+(u)lu),a constant independent of the partition x.  #@NEW_LINE#@#  The gain in cl (interpreted as less the expected loss of Bayes rule) can be defined ascl=12jmax{aj,bj}=12j(aj+bj)el=el.  #@NEW_LINE#@#  
Because  is independent of X and X, it is desirable to search for X with larger cl to achieve better gains.  #@NEW_LINE#@#  Again we havecl=2+clel2=2+14j|ajbj|  #@NEW_LINE#@#  
After standardizing by , we obtain the improved prediction rate asc=cl=12+14j|ajbj|  #@NEW_LINE#@#  
Collecting the above discussion together, let the cost-based I-score Ic be defined asnsn2Ic=14jnj2[y¯j((d))ld(1y¯j)((u)1)lu]2n24j[P(j|d)(d)ldP(j|u)(u)lu]2.  #@NEW_LINE#@#  [S9]  #@NEW_LINE#@#  
We present the following lower bound in Corollary 4.  #@NEW_LINE#@#  Letj(P(j|d)(d)l2P(j|u)(u)l1)=(d)l2(u)l1=a.  #@NEW_LINE#@#  
Corollary 4.  #@NEW_LINE#@#  Under the assumptions of Corollary 2 and using the loss function L described in Eqs.  #@NEW_LINE#@#  S7 and S8, thenlimnsn2Icn=14j[P(j|d)(d)ldP(j|u)(u)lu]2.  #@NEW_LINE#@#  [S10]  #@NEW_LINE#@#  
Furthermore, one can derive a similar lower bound for the correct prediction rate c asc=12+14j|ajbj|limn(12+14(1)Icna2)=12+14limn(1)Icna2[S11]  #@NEW_LINE#@#  
The proofs for Eqs.  #@NEW_LINE#@#  S10 and S11 are quite similar to that for Corollary 3 given above; we shall omit them.  #@NEW_LINE#@#  

Technical_Notes  #@NEW_LINE#@#  
Technical_Note_1__Alternative_Formulation_of_the_Theoretical_Prediction_Rate  #@NEW_LINE#@#  
Recall that the expected error of adopting the above Bayes decision rule (under a 0/1 loss) ise[pd,pu]=12min{pd(),pu()}.  #@NEW_LINE#@#  
The correct prediction rate c on  is defined asc()=c[pd,pu]=1e[pd,pu]=12max{pd(),pu()}where e is the error rate.  #@NEW_LINE#@#  For simplicity of presentation, we can represent the above asc=12jmax{P(j|d),P(j|u)}where j is short for j, a cell in the partition  formed by the variables .  #@NEW_LINE#@#  
It is easy to show that12{c[pd,pu]e[pd,pu]}=c[pd,pu]12=14j|P(j|d)P(j|u)|.  #@NEW_LINE#@#  
Therefore,c[pd,pu]=12+14j|P(j|d)P(j|u)|.  #@NEW_LINE#@#  

Technical_Note_2__Issue_with_Sample_Analog_of_c  #@NEW_LINE#@#  
Suppose m={X1,,Xm} and m+1={X1,,Xm,Xm+1}.  #@NEW_LINE#@#  The partition formed by m ism={A1,,Am1},whereas the partition formed by m+1 ism+1={A1B,,Am1B,A1Bc,,Am1Bc}={mB,mBc}where B={m+1=1}.  #@NEW_LINE#@#  Letm1=m{Xm+1=1}andm0=m{m+1=0},where m1 and m0 form two subpartitions of m+1, i.e., m+1=m0m1.  #@NEW_LINE#@#  Then|p^m(d)p^m(u)||p^m0(d)p^m0(u)|+|p^m1(d)p^m1(u)|,where p^() is the sample estimator.  #@NEW_LINE#@#  We see that the sample analog inherently favors an increase in number of partition cells (i.e., adding more variables).  #@NEW_LINE#@#  

Technical_Note_3__Proof_of_Lemma_1  #@NEW_LINE#@#  
It is obvious that |a|b.  #@NEW_LINE#@#  Let S1 be the sum of the positive values of zj and S2 the sum of the negative values.  #@NEW_LINE#@#  Let T1 be the sum of the squares of the positive values and T2 the sum of the squares of the negative values.  #@NEW_LINE#@#  It follows that S1+S2=a and S1S2=b and thus S1=(a+b)/2 and S2=(ab)/2.  #@NEW_LINE#@#  Then clearly T1S12 and T2S22.  #@NEW_LINE#@#  Consequently,j=1Kzj2=T1+T2S12+S22=a2+b22[S1]which is equivalent to the inequality in Eq.  #@NEW_LINE#@#  4 and equality is attained when there are at most one positive and one negative component if |a|less_thanb.  #@NEW_LINE#@#  

Technical_Note_4__Technical_Details_on_I-Score  #@NEW_LINE#@#  
The influential score (I-score) is a statistic derived from the PR method.  #@NEW_LINE#@#  Several forms and variations were associated with the PR method before it was finally coined with this name in 2009 (4).  #@NEW_LINE#@#  We introduce the PR method and the I-score briefly here.  #@NEW_LINE#@#  
Consider a set of n observations of a disease phenotype Y (dichotomous or continuous) and a large number S of SNPs, X1,X2,,XS.  #@NEW_LINE#@#  Randomly select a small group, m, of the SNPs.  #@NEW_LINE#@#  Following the same notation as in previous sections, we call this small group ={Xk,k=1,,m}.  #@NEW_LINE#@#  Recall that Xk takes values 0,1, and 2 (corresponding to three genotypes for a SNP locus: AA, A/B, and B/B).  #@NEW_LINE#@#  There are then m1=3m possible values for s. The n observations are partitioned into m1 cells according to the values of the m SNPs (Xks in ), with nj observations in the jth cell.  #@NEW_LINE#@#  We refer to this partition as .  #@NEW_LINE#@#  The proposed I-score (denoted by I) is designed to place greater weight on cells that hold more observations:I=j=1m1njn(Y¯jY¯)2sn2/nj=j=1m1nj2(Y¯jY¯)2i=1n(YiY¯)2[S2]where sn2=1ni=1n(YiY¯)2.  #@NEW_LINE#@#  We note that the I-score is designed to capture the discrepancy between the conditional means of Y on {X1,X2,,Xm} and the mean of Y.  #@NEW_LINE#@#  
In this paper, we consider the special problem of a case-control experiment where there are nd cases and nu controls and the variable Y is 1 for a case and 0 for a control.  #@NEW_LINE#@#  Then sn2=(ndnu)/n2 where n=nd+nu.  #@NEW_LINE#@#  

Technical_Note_5__Proof_of_Theorem_1  #@NEW_LINE#@#  
We prove that the I-score approaches a constant multiple of I asymptotically.  #@NEW_LINE#@#  
Under the null hypothesis of no association between ={Xk,k=1,,m} and Y, I can be asymptotically expressed as j=1m1jj2 (a weighted average), where j is between 0 and 1 and j=1m1j is equal to 1j=1m1pj2, where pj is the cell js probability.  #@NEW_LINE#@#  {j2} are m1 chi-squares, each with degree of freedom, df=1 (see ref.  #@NEW_LINE#@#  4).  #@NEW_LINE#@#  
Furthermore, the above formulation and properties of I apply to the specified Y model with case-control study (where Y=1 designates case and Y=0 designates control) as demonstrated in ref.  #@NEW_LINE#@#  4.  #@NEW_LINE#@#  More specifically, in a case-control study with nd cases and nu controls (letting n=nd+nu), nsn2I can be expressed as the following:nsn2I=jnj2(Y¯jY¯)2=j(nd,jm+nu,jm)2(nd,jmnd,jm+nu,jmndnd+nu)2=(ndnund+nu)2j(nd,jmndnu,jmnu)2where nd,jm and nu,jm denote the numbers of cases and controls falling in jth cell, and  stands for the partition formed by m variables in .  #@NEW_LINE#@#  Since the PR method seeks the partition that yields larger I-scores, one can decompose the following:nsn2I=jnj2(Y¯jY¯)2=An+Bn+Cnwhere An=jnj2(Y¯jj)2, Bn=jnj2(Y¯j)2, and Cn=j2nj2(Y¯jj)(Y¯j).  #@NEW_LINE#@#  Here, j and  are the local and grand means of Y, that is, E(Y¯j)=j;Y¯==ndnd+nu for fixed n. It is easy to see that both terms An and Cn, when divided by n2 converge to 0 in probability as n. We turn to the final term, Bn.  #@NEW_LINE#@#  Note thatlimnBnn2=limnj(nj2n2)(j)2  #@NEW_LINE#@#  
In a case-control study, we havej=ndP(j|d)ndP(j|d)+nuP(j|u)and=ndnd+nu  #@NEW_LINE#@#  
Because for every j, njn converges (in probability) to pj=P(j|d)+(1)P(j|u) as n, if limnndn=, a fixed constant between 0 and 1, it follows thatBnn2=j(nj2n2)(j)2jpj2(P(j|d)P(j|d)+(1)P(j|u))2asn=j{P(j|d)[P(j|d)+(1)P(j|u)]}2=j{(1)P(j|d)[(1)P(j|u)]}2=2(1)2j[P(j|d)P(j|u)]2  #@NEW_LINE#@#  
Thus, ignoring the constant term in the above equation, the I-score can guide a search for X partitions, which will lead to finding larger values of the summation term j[P(j|d)P(j|u)]2.  #@NEW_LINE#@#  We have proven Theorem 1.  #@NEW_LINE#@#  

Technical_Note_6__Proof_of_Corollary_2  #@NEW_LINE#@#  
Under the assumptions in Theorem 1, the following is an asymptotic lower bound for the correct predictive rate:c()12+142limnIn(1).  #@NEW_LINE#@#  [S3]  #@NEW_LINE#@#  
Proof: From Eq.  #@NEW_LINE#@#  2,c()=12+14j|P(j|d)P(j|u)|(Lemma 1)12+142j(P(j|d)P(j|u))2=12+142I()(Theorem 1)=12+142limnsn2In2(1)2=12+142limnIn(1).  #@NEW_LINE#@#  [S4]  #@NEW_LINE#@#  
The asymptotic lower bound of Eq.  #@NEW_LINE#@#  2 is a simple consequence of Lemma 1 and Theorem 1.  #@NEW_LINE#@#  In theory, the above corollary allows us to apply a useful lower bound for identifying good variable sets with large I-scores.  #@NEW_LINE#@#  In practice, however, once the variable sets are found (through their large I-scores), the true prediction rates can be greater than the identified lower bounds.  #@NEW_LINE#@#  Theorem 1 provides a simple asymptotic behavior of the I-score under some strict assumptions.  #@NEW_LINE#@#  We offer similar derivations below following two levels of relaxations of the constraints.  #@NEW_LINE#@#  
We remark that with additional work one can show that the convergence given in Eq.  #@NEW_LINE#@#  2 can be extended to be uniformly over all partitions {} with bounded number of cells and for all  that stay away from 0 to 1.  #@NEW_LINE#@#  


BDA  #@NEW_LINE#@#  
The BDA§ is a greedy algorithm to search for the variable subset that maximizes the I-score through stepwise elimination of variables from an initial subset sampled in some way from the variable space.  #@NEW_LINE#@#  The details are as follows.  #@NEW_LINE#@#  
If no variable in the initial subset has influence on Y, then the values of I will not change much in the dropping process.  #@NEW_LINE#@#  However, when influential variables are included in the subset then the I-score will increase (decrease) rapidly before (after) reaching the maximum.  #@NEW_LINE#@#  

Using_the_I-Score_in_Sample-Constrained_Settings  #@NEW_LINE#@#  
We have shown that I/n asymptotically approaches a constant multiple of I (which is related to a lower bound of c) and has several desirable properties.  #@NEW_LINE#@#  We take this opportunity to explore and illustrate an application of the I-score measuring predictivity with sample data.  #@NEW_LINE#@#  To provide additional evidence of the I-scores ability to measure true predictivity, we consider a set of simulations for which we know the true levels of predictivity for all variable sets.  #@NEW_LINE#@#  We also provide a real data application on breast cancer for which the I-score approach has done very well in predicting.  #@NEW_LINE#@#  
We take a moment to comment that evaluating a variable set for predictivity, what we have called here VSA, is different from evaluating a given classifier, which is the prediction stage, usually following or in conjunction with VS.  #@NEW_LINE#@#  The latter considers evaluating f(), a special function f() applied to a particular set of explanatory variables , for a given outcome variable y, whereas the former considers the potential predictivity of the set of explanatory variables  for that outcome y for all possible f().  #@NEW_LINE#@#  Our work here focuses simply on VSA.  #@NEW_LINE#@#  Variable sets assessed as highly predictive in our framework can then be flexibly used in various models for prediction purposes as pleases the researcher.  #@NEW_LINE#@#  
We are now in an odd situation where we have identified variable sets that could not have been found using conventional approaches and yet we wish to evaluate the predictivity of our identified variable sets against these conventional approaches.  #@NEW_LINE#@#  Nevertheless, we endeavor to do so.  #@NEW_LINE#@#  A couple options arise for approaches to compare against: training prediction rate and out-of-sample testing prediction rate.  #@NEW_LINE#@#  We will show that the I-score-based measure provides a useful and meaningful estimated lower bound to the correct prediction rate and correlates well with the out-of-sample test rate, whereas the training rate statistic, the sample analog of c, does not.  #@NEW_LINE#@#  As such, our approach has an important benefit to prediction research: Compared with methods such as cross-validation of error rates, the I-score is efficient in the use of sample data, in the sense that it uses all observations instead of separating data into testing and training.  #@NEW_LINE#@#  
Simulations  #@NEW_LINE#@#  
We offer simulations to illustrate how (i) the I-score can serve as a lower bound to the true predictivity of a given variable set even as noisy variables are adjoined, (ii) thereby serving as a screening mechanism, and (iii) finding the maximum I-score when conducting a BDA leads to finding the variable set with the highest corresponding level of predictivity.  #@NEW_LINE#@#  BDA reduces a variable set one variable at a time, by eliminating the weakest element until I reaches a peak.  #@NEW_LINE#@#  
We consider a module of three important variables {X1,X2,X3} (see Fig 2 for the disease model used) among six unimportant variables {X7,,X12} using sample sizes of 250 cases/250 controls, 500 cases/500 controls, and 1,000 cases/1,000 controls.  #@NEW_LINE#@#  (See Simulation Details for more detailed model setting and simulation details.)  #@NEW_LINE#@#  We demonstrated that the In(1) estimates* I, which is related to an asymptotic lower bound (Eq.  #@NEW_LINE#@#  6) for c, as n. It would be helpful to see how I performs at fixed, reasonable sample sizes.  #@NEW_LINE#@#  We compare the I-score derived predictivity lower bound against the Bayes theoretical prediction rate in our simulations to illustrate this.  #@NEW_LINE#@#  The out-of-sample correct prediction rate is presented in the simulations here as a further benchmark against which the I-score can be compared when data are limited, as is the case in real-world applications.  #@NEW_LINE#@#  The out-of-sample correct prediction rate is derived from the most optimistic context achievable in the real world, whereby future testing data are infinite.  #@NEW_LINE#@#  In all of the simulations, the I-score of a set of influential variables drops when a noisy variable is added.  #@NEW_LINE#@#  This drop is subsequently seen in the I-score derived bound for the correct prediction rate.  #@NEW_LINE#@#  The I-score can screen out noisy variables, which makes it useful in practical data applications.  #@NEW_LINE#@#  
To illustrate how these statistics fare in accurately capturing the level of predictivity of each variable set under consideration, we consider their performance given already having found X2 and X3 as important.  #@NEW_LINE#@#  We then add X1, which should ideally correspond with an increase in the statistic.  #@NEW_LINE#@#  We continue adding the remaining noisy variables one at a time to this good set of variables and observe how the statistics evaluate the new, larger set of variables for predictivity.  #@NEW_LINE#@#  In Fig 3, violin plots show distributions of training rate, the I-score lower bound, and the ideal out-of-sample prediction rate under each setting across the simulations.  #@NEW_LINE#@#  Theoretical Bayes rate is also plotted as a reference, which remains flat when noisy variables are added.  #@NEW_LINE#@#  This is because the Bayes rate is defined purely by the partition formed from the informative variables and does not change when adjoining noisy variables (X7,,X12) and creating finer partitions.  #@NEW_LINE#@#  
Several patterns emerge in these simulations.  #@NEW_LINE#@#  First, and most importantly, the I-score-derived prediction rate seems to be a reasonable lower bound to the Bayes rate.  #@NEW_LINE#@#  This holds even in moderate sample sizes.  #@NEW_LINE#@#  
The second pattern is that the estimated I-score lower bound peaks at the variable set that is inclusive of all influential variables (X1, X2, and X3) and no additional noisy variables.  #@NEW_LINE#@#  This is a characteristic of the out-of-sample correct prediction rate as well.  #@NEW_LINE#@#  For instance, if we consider the top row of Fig 3 and start from the right of the x axes in each of the three plots with the largest set of variables inclusive of both influential and noisy variables (X1,X2,X3,X7,,X12), continual removal of the noisy variables (sliding to the left of the x axis) until we reach the variable set (X1,X2, X3) results in higher predictivity as measured by the I-score lower bound.  #@NEW_LINE#@#  We can note that the I-score lower bounds drop upon further removing the influential X1 variable from the set (X1,X2,X3).  #@NEW_LINE#@#  Thus, the variable set that appears with the maximum I-score derived lower bound here both identifies the largest possible variable set of influential variables with no noisy variables and is also reflective of a conservative lower bound of the correct prediction rate for that variable set.  #@NEW_LINE#@#  We note that once we have found the variable sets with the highest I-scores and calculated the corresponding lower bound of the correct prediction rate, we can adjust this lower bound rate for its bias to derive an improved estimate of the correct prediction rate.  #@NEW_LINE#@#  
A third pattern is that the training rate suffers from overfitting when adjoining noisy variables even when the variable set includes a true influential subset of variables.  #@NEW_LINE#@#  If the variable set is irreducible, however, the training rate estimator reflects the Bayes correct prediction rate well; thus, the training rate estimator can perform reasonably well conditional on already identifying (X1,X2,X3).  #@NEW_LINE#@#  The training rate estimator cannot be used to screen to that variable set first, however.  #@NEW_LINE#@#  
Finally, and as we might expect, the training set rate explodes due to overfitting in high dimensions as noisy variables are adjoined to the partition formed by the informative variables (X1, X2, X3).  #@NEW_LINE#@#  Although the training set prediction rate seems to improve as the sample size increases, it cannot be used to screen out noisy variables, and is therefore difficult to use as a statistic to select highly predictive variable sets.  #@NEW_LINE#@#  The predictivity rates found through this statistic also dramatically depart from the out-of-sample testing rate.  #@NEW_LINE#@#  It tends to ever-optimistically evaluate variable sets for their future predictions even when noisy variables are added.  #@NEW_LINE#@#  This stands in stark contrast to the out-of-sample prediction rate because it lowers in prediction rate with the addition of useless variables.  #@NEW_LINE#@#  We notice that there is a trend that the I-score prediction rate does not remain flat.  #@NEW_LINE#@#  The score increases when removing a noisy variable and reducing to a variable set of only influential variables, indicating the additional advantage of the I-score as a lower bound; the I-score prefers a simpler model even when the Bayes rate remains the same, selecting for more parsimonious partitions that attain the Bayes rate, simultaneously a closer reflection of the out-of-sample prediction rate.  #@NEW_LINE#@#  
Recall the correct prediction rate is based on an absolute difference of probabilities summed over all Xs.  #@NEW_LINE#@#  Suppose we start with influential variables only, with c correct prediction rate, the highest we can attain out of all possible variable sets.  #@NEW_LINE#@#  Adding noisy variables to this set, variables that add no signal but simply create a finer partition, still returns c. When estimating the correct prediction rate using sample data, though, the training estimate of c value generally keeps increasing if noisy variables are added; the researcher does not know when to stop the search for influential variables, making selecting for highly predictive variables difficult.  #@NEW_LINE#@#  Ideally, we would like to punish adding such noisy variables to our variable set, so having a measure that balances between favoring coarser partitions but still recognizing actual new variables with strong enough signals (non noisy variables) is important.  #@NEW_LINE#@#  The I-score seems to support such an effectpreferring coarser partitions unless an additional variable (and therefore finer partition) provides enough signal in the data to justify keeping it.  #@NEW_LINE#@#  
Noisy variables in sample data may be indicative of actually noisy variables or influential variables with weak signals due to the sample size.  #@NEW_LINE#@#  Thus, we note there are cases where the I-score might not recognize these variables when their signals would require unrealistic sample sizes to be found through the measure.  #@NEW_LINE#@#  An example of this would be if a good predictor is highly complex (perhaps a combination of very many variables) and the observations are sparse in the partition.  #@NEW_LINE#@#  Because the I-score places greater weight on where the data tend to appear (note the ni2 term in the score), when most of the partition cells contain no observations or at most one observation, this can often look like noise.  #@NEW_LINE#@#  
The main draw of the I-score is its ability to screen for influential variable sets.  #@NEW_LINE#@#  The variable sets inclusive of the three influential variables (X1, X2, and X3) alone display the highest I-scores.  #@NEW_LINE#@#  Searching for variable sets with the highest I-scores thus tends to return highly influential variables only.  #@NEW_LINE#@#  Using the training prediction rate as a guiding measure for screening, however, would continually seek for ever-larger variable sets, regardless of whether they include noisy variables or not.  #@NEW_LINE#@#  

Real_Data_Application__van_t_Veer_Breast_Cancer_Data  #@NEW_LINE#@#  
To reinforce the previous sections, we briefly analyze real disease data.  #@NEW_LINE#@#  As noted before, part of this research team has discovered that applying the PR approach to real disease data has not only been quite successful in finding variable sets (thus encompassing higher-order interactions, traditionally rather tricky in big data), but has also resulted in finding variable sets that are very predictive that do not necessarily show up as significant through traditional significance testing.  #@NEW_LINE#@#  We present one discovered variable set (a total of 18 variable sets were found in ref.  #@NEW_LINE#@#  5) found to be highly predictive for a breast cancer dataset that is not highly significant using a chi-square test.  #@NEW_LINE#@#  In Table 1 we investigate the top, five-variable set (in this case five genes) found to be predictive through both top I-score and performance in prediction in cross-validation and an independent testing set in ref.  #@NEW_LINE#@#  5.  #@NEW_LINE#@#  To find how significant these variables are, we calculate the individual, marginal association of each variable in the marginal P value.  #@NEW_LINE#@#  Given the familywise P value threshold of 6.98 × 105, none of these variables seems statistically significant.  #@NEW_LINE#@#  Measuring the joint influence of all five variables is not significant either.  #@NEW_LINE#@#  Using the variable sets (all 18 in ref.  #@NEW_LINE#@#  5) that seemed to have the highest I-scores to predict on this dataset resulted in an out-of-sample testing error rate of 8%, in direct comparison with the literatures best error rates of 30%.  #@NEW_LINE#@#  Using only the variable set displayed in Table 1 and the lower bound in Eq.  #@NEW_LINE#@#  6 we can calculate the asymptotic lower bound of the correct prediction rate for this variable set as 59%.  #@NEW_LINE#@#  Thus, using only this variable set alone, we can achieve at least a 59% correct classification rate at minimum.  #@NEW_LINE#@#  For details on the final predictors, see ref.  #@NEW_LINE#@#  5.  #@NEW_LINE#@#  


Simulation_Details  #@NEW_LINE#@#  
The simulation is based on a six-SNP disease model.  #@NEW_LINE#@#  The six SNPs are organized into two three-SNP modules (X1, X2, X3), and (X4, X5, X6).  #@NEW_LINE#@#  Six additional variables (X7, , X12) are simulated to be noisy and unrelated to the disease.  #@NEW_LINE#@#  The frequencies for the minor allele of each SNP are all 0.5.  #@NEW_LINE#@#  The risk of the disease for an individual depends on the two three-SNP genotypes of these two modules.  #@NEW_LINE#@#  Each module defines two sets of genotypes, high risk genotypes and low risk genotypes, identically depicted in Fig 2.  #@NEW_LINE#@#  If an individual has two low risk genotypes, he has odds of 1/60 for having the disease.  #@NEW_LINE#@#  Here, odds is the ratio of the probability of an event occurring (disease) over the probability of the event not occurring (no disease).  #@NEW_LINE#@#  For an individual with one of the low-risk genotypes and one of the high-risk genotypes, the odds are increased to 1/10.  #@NEW_LINE#@#  If an individual has high-risk genotypes for both modules, the odds become 1.  #@NEW_LINE#@#  In this section, we present results for the first module (X1, X2 and X3).  #@NEW_LINE#@#  In Fig S1 we present results for both modules, or all six SNPs, together.  #@NEW_LINE#@#  
The data can take on three sample size levels: 250 cases/250 controls, 500 cases/500 controls, and 1,000 cases/1,000 controls.  #@NEW_LINE#@#  For each possible variable set we create a partition  and calculate the p^id and p^iu (the estimated probability that an individual in cell j is a case or a control), respectively: nidnd and niunu where i=1...m and m=|| where || is the size of the partition .  #@NEW_LINE#@#  We conducted 300 simulations and evaluated a set of statistics on each of the variable sets for each simulation: the training prediction rate, Bayes prediction rate, out-of-sample prediction rate, and the I-score-derived lower bound estimate of the predictivity rate; see Fig 3.  #@NEW_LINE#@#  Throughout, we assume prior probability of (0.5, 0.5) for case and control.  #@NEW_LINE#@#  The statistics are detailed below:  #@NEW_LINE#@#  

Concluding_Remarks  #@NEW_LINE#@#  
Prediction has become more important in recent decades and, with it, the need for tools appropriate for good prediction.  #@NEW_LINE#@#  A first step can be to assess variable sets for predictivity, which we call VSA.  #@NEW_LINE#@#  We show in other work that assessment of variables from a statistical significance criterion to predict is not ideal (3).  #@NEW_LINE#@#  A currently popular alternative solution is to select variables via sample-based, out-of-sample testing error rates.  #@NEW_LINE#@#  This approach is ad hoc in nature, sample-based, and is not measuring some theoretical underlying level of predictivity for a given variable set.  #@NEW_LINE#@#  Often validation of selected candidate variable sets requiressetting aside valuable sample data in out-of-sample testing or cross-validation.  #@NEW_LINE#@#  Sometimes the sample size may not suffice for validating variable set sizes larger than one or two variables, as is often the case in big data like GWAS.  #@NEW_LINE#@#  Cross-validation avoids setting aside sample data as independent test sets but is computationally difficult in big data without using independent samples.  #@NEW_LINE#@#  As such, prediction research would benefit from a theoretical framework that directly defines a variable sets predictivity as a parameter of interest to estimate.  #@NEW_LINE#@#  We believe our work here is a preliminary and important effort in that direction, by considering what theoretically highly predictive variable sets are, and how we might try to find them.  #@NEW_LINE#@#  In fact, using measures such as the I-score could be an important new direction in the prediction literature because it neither uses the training sample prediction rate nor does it require an artificial or ad hoc regularization choice.  #@NEW_LINE#@#  
We identify the equation for the theoretical correct predictivity of variable sets (c) in Eq.  #@NEW_LINE#@#  2 and then demonstrate that, unfortunately, the training estimate for it is quite useless.  #@NEW_LINE#@#  As such, we offer an alternative measure.  #@NEW_LINE#@#  We show that the In asymptotically approaches a lower bound to I of Eq.  #@NEW_LINE#@#  2 and is thus correlated with the correct predictivity rate of a given variable set.  #@NEW_LINE#@#  Importantly, we show that the I-score has a natural tendency to discard noisy variables, keep influential ones, and asymptotically approach this lower bound to c. The I-score does well in identifying predictive variable sets in both our complex simulations as well as real data application.  #@NEW_LINE#@#  
We note that other measures with such desirable properties may also exist, and we encourage rigorous research in this direction.  #@NEW_LINE#@#  As a new field of inquiry, the search for measures that maximize predictivity may do much in the way of living up to the hopes of advancing predicting outcomes of interest, such as disease status.  #@NEW_LINE#@#  In some ways, this work is motivated by a practical consideration of finite samples.  #@NEW_LINE#@#  As noted in the setup of our framework, in a theoretical world of limitless data we can in fact find the variable sets with highest values of c. However, our real world of finite sample sizes requires other sample-appropriate measures that may approximate but not achieve the c. In other words, based on available sample size, the I-score, and any other such measure, detects not necessarily the maximum c but some c,nH, the largest c correct prediction rate for which the corresponding X variables can be selected given n. Consider a situation where the true set of variables  that provide the theoretical maximum c is very large.  #@NEW_LINE#@#  Suppose we have a sample of data that is quite modest.  #@NEW_LINE#@#  Selecting all variables  is not possible given the sample size n (too many of the cell frequencies are small or zero) and so a measure such as the I-score retrieves a set  that provides potentially the largest c achievable given the sample constraint.  #@NEW_LINE#@#  This in some ways mirrors the common issue of not detecting true effects when the sample size is too small in statistical significance testing.  #@NEW_LINE#@#  
We leave the important discussion of how to combine identified predictive variable sets in different final prediction models outside the scope of this paper.  #@NEW_LINE#@#  

Simulation_Results_for_Important_Variable_Set_of_Size_6  #@NEW_LINE#@#  
Here we present simulation results in Fig S1 for the six SNPs according to the model described in the main text.  #@NEW_LINE#@#  All other simulation parameters were the same as the three-SNP example.  #@NEW_LINE#@#  

Acknowledgments  #@NEW_LINE#@#  
This research is supported by National Science Foundation Grant DMS-1513408.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  


