article id="http://dx.doi.org/10.1073/pnas.1712966114"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Nature and origins of the lexicon in 6-mo-olds  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Infants start understanding words at 6 mo, when they also excel at subtle speechsound distinctions and simple multimodal associations, but dont yet talk, walk, or point.  #@NEW_LINE#@#  However, true word learning requires integrating the speech stream with the world and learning how words interrelate.  #@NEW_LINE#@#  Using eye tracking, we show that neophyte word learners already represent the semantic relations between words.  #@NEW_LINE#@#  We further show that these same infants word learning has ties to their environment: The more they hear labels for what theyre looking at and attending to, the stronger their overall comprehension.  #@NEW_LINE#@#  These results provide an integrative approach for investigating home environment effects on early language and suggest that language delays could be detected in early infancy for possible remediation.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Recent research reported the surprising finding that even 6-mo-olds understand common nouns [Bergelson E, Swingley D (2012) Proc Natl Acad Sci USA 109:32533258].  #@NEW_LINE#@#  However, is their early lexicon structured and acquired like older learners?  #@NEW_LINE#@#  We test 6-mo-olds for a hallmark of the mature lexicon: cross-word relations.  #@NEW_LINE#@#  We also examine whether properties of the home environment that have been linked with lexical knowledge in older children are detectable in the initial stage of comprehension.  #@NEW_LINE#@#  We use a new dataset, which includes in-lab comprehension and home measures from the same infants.  #@NEW_LINE#@#  We find evidence for cross-word structure: On seeing two images of common nouns, infants looked significantly more at named target images when the competitor images were semantically unrelated (e.g., milk and foot) than when they were related (e.g., milk and juice), just as older learners do.  #@NEW_LINE#@#  We further find initial evidence for home-lab links: common noun copresence (i.e., whether words referents were present and attended to in home recordings) correlated with in-lab comprehension.  #@NEW_LINE#@#  These findings suggest that, even in neophyte word learners, cross-word relations are formed early and the home learning environment measurably helps shape the lexicon from the outset.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Data processing and annotation information, and in-house scripts are available on our OSF lab wiki and github repository (https://github.com/SeedlingsBabylab/).  #@NEW_LINE#@#  Raw home-recording data (audio and video) are available through HomeBank and Databrary; see details in Materials and Methods; clips are available in Supporting Information.  #@NEW_LINE#@#  
Eye-Tracking_Results  #@NEW_LINE#@#  
Eye-tracking data were processed in R 3.3.1 to determine where the child was looking for each 20-ms bin during the test trials: the target or distractor interest areas (an invisible 620 × 620 pixel rectangle around each image), or neither.  #@NEW_LINE#@#  Eye movement data were time-aligned to parents target word utterances (noted by experimenter key press).  #@NEW_LINE#@#  
These data were then aggregated across two time windows: a pretarget baseline from trial start to target word onset and a posttarget window from 367 ms to trial end, i.e., 5,000 ms after target onset.  #@NEW_LINE#@#  Given the longitudinal home-and-lab design of this data collection, we exclude at the trial level rather than the infant level, where possible.  #@NEW_LINE#@#  Trials were excluded if infants did not look at either image for at least 1/3 of the target window (367 ms to 5,000 ms), if no looking was recording in the pretarget baseline window, or if the trial was never displayed due to experiment termination for infant fussiness; see Supporting Information.  #@NEW_LINE#@#  
We used the standard baseline-corrected target looking metric, which calculates the proportion of target looking [target/(target+distractor)] in the posttarget window, and subtracts this same proportion from the baseline window.  #@NEW_LINE#@#  Visual inspection of subject means revealed one outlier (5 SD above the mean).  #@NEW_LINE#@#  Once this outlier was removed, this outcome measure did not differ from a normal distribution (ShapiroWilk test, P = 0.93).  #@NEW_LINE#@#  
We predicted that, if infants comprehension is affected by semantic relatedness, performance on related trials would be worse than on unrelated trials.  #@NEW_LINE#@#  Indeed, performance was significantly above chance on unrelated trials [t(50)=2.91, P=0.005, by two-tailed one-sample t test], at chance on related trials [t(50)=0.64, P=0.524, by two-tailed one-sample t test], and significantly different between the two [t(50)=2.22, P=0.031, by two-tailed paired t test].  #@NEW_LINE#@#  That is, infants looked more at the labeled image on unrelated trials than on related trials.  #@NEW_LINE#@#  Over subjects, 36/51 infants attained positive subject means for the unrelated trial type (M=0.044, SD=0.108, P = 0.005 by binomial test), while only 26/51 did so for related trials (M=0.013, SD = 0.15, P=1 by binomial test).  #@NEW_LINE#@#  Over items, infants showed the same numeric pattern as over subjects, i.e., the item mean was higher for unrelated items than related items (M=0.035, SD=0.081 and M=0.003, SD=0.067, respectively), and item means were positive for most items in the unrelated condition, but not the related condition; with only 16 items, item-level effects were not different from chance (P0.05).  #@NEW_LINE#@#  Summarily, in the related condition but not in the unrelated condition, performance was positive over most infants and most items; see Fig 1 and Figs.  #@NEW_LINE#@#  S1 and S2.  #@NEW_LINE#@#  

Home-Recording_Results  #@NEW_LINE#@#  
After preprocessing (see Materials and Methods), annotators marked each object word (i.e., concrete noun) in the recordings along with three properties: utterance type, object copresence, and speaker.  #@NEW_LINE#@#  Each words utterance type was classified by its syntactic and prosodic features into seven categories: declaratives, questions, imperatives, short phrases, reading, singing, and unclear.  #@NEW_LINE#@#  Object copresence was coded yes, no, and unclear based on whether the annotator felt that the object corresponding to the word being annotated was present and attended to by the child.  #@NEW_LINE#@#  For videos, this was generally visually appreciable; for audio files, annotators used their impression from the context (e.g., generally, in heres your spoon!  #@NEW_LINE#@#  spoon was coded yes for object copresence, while in twinkle little star, star was coded no).  #@NEW_LINE#@#  Interrater reliability was high [computed for 10% of annotations for utterance type: 89% agreement, Cohens (K) = 0.8; object copresence: 85% agreement, Cohens (K) = 0.7].  #@NEW_LINE#@#  We then derived token counts for the various forms a word occurred in (e.g., tooth,,tootheroo, teeth), and type counts for the lemma (e.g., tooth).  #@NEW_LINE#@#  
While input data varied by child, there was relatively high consistency in our proportional measures (Table 1).  #@NEW_LINE#@#  We operationalized input quantity as the number of types and tokens of each lemma.  #@NEW_LINE#@#  Given that the video recordings were all 1 h, while the audio recordings varied based on the nap time of the child, we report daily audio rates and hourly video rates.  #@NEW_LINE#@#  
In our daylong audio recordings, infants heard 710 object word tokens of 180 word types, from seven speakers, on average.  #@NEW_LINE#@#  Sixty percent of this input came from infants mothers, and 50% of the time that infants heard an object word, the corresponding referent was visible and attended to (i.e., 50% object copresence).  #@NEW_LINE#@#  In our hour-long video recordings, infants heard 170 tokens, from 60 types, from three speakers.  #@NEW_LINE#@#  In the videos, 70% of the input came from infants mothers, with 60% object copresence.  #@NEW_LINE#@#  For both audio and video recordings, the average entropy across utterance types (i.e., the variability proportions of each utterance type in bits) was 1.9; short phrases were 10% of the input (Fig 2).  #@NEW_LINE#@#  
Comparing the daylong audio and hour-long video recordings, we found that, although infants heard more object word input in the audio recordings in an absolute sense, they heard relatively more input in the videos.  #@NEW_LINE#@#  That is, infants heard only 25 to 50% fewer word tokens, word types, and speakers in the hour-long video than in the daylong audio recording (from a different day), even though the latter was 1011× longer than the former; we explore this in ongoing work.  #@NEW_LINE#@#  

Questionnaires  #@NEW_LINE#@#  
Vocabulary questionnaires [MacArthur-Bates Communicative Development Inventory (MCDI)] showed that parents felt their infants understood few of our 16 tested words [M = 1.96 (3.98); R: 0 to 15; mode: 0], whereas, on Word Exposure Surveys, parents indicated that their child heard these words daily on average (4 on a 1 to 5 scale from never to several times a day).  #@NEW_LINE#@#  According to parental report, 71% of infants were not yet babbling, all but one were not yet hands-and-knees crawling, and 29% were exclusively breast-fed; see Supporting Information.  #@NEW_LINE#@#  

Home_and_Lab_Linkages  #@NEW_LINE#@#  
We next examined data from the infants who provided both home and in-lab data (video and eye tracking: n = 40; audio and eye tracking: n = 41).  #@NEW_LINE#@#  We modeled infants subject means from our eye-tracking experiment as a function of the properties we had annotated (noun input, talker, utterance types, and object copresence).  #@NEW_LINE#@#  Given the relatively small sample size, the large number of ways one might aggregate the home data, and both predicted and unanticipated collinearity among these four preidentified properties of interest, we opted for a simple analysis approach.  #@NEW_LINE#@#  Namely, we tested directionally specified correlations between home environment measures found to predict lexical knowledge in previous research and infants in-lab comprehension.  #@NEW_LINE#@#  That is, if measures that predict lexical knowledge in older infants hold at 6 mo at levels our home measures can detect, we would see positive correlations between in-lab comprehension and noun input, talker variability, utterance type diversity, and object copresence.  #@NEW_LINE#@#  When analyzing counts, we examined audio and video data separately, since length varied substantially by recording type.  #@NEW_LINE#@#  When examining proportions, we averaged audio and video data.  #@NEW_LINE#@#  We first conducted ShapiroWilk Normality tests; if both variables were normally distributed, we used Pearson correlations; if one or both were not, we used Kendall correlations.  #@NEW_LINE#@#  
As described above, the eye-tracking results suggest that neophyte word learners are sensitive to semantic similarity.  #@NEW_LINE#@#  However, given that the home environment is not split into experiences relevant for our two experimental trial types, we examine how overall lab performance correlates with home measures.  #@NEW_LINE#@#  Notably, aggregating across trial types for each infant, performance overall was not above chance, given the relatively weak performance on related trials [t(40) = 1.59, P = 0.12].  #@NEW_LINE#@#  [This t test was run on the subset of infants reported above for whom there is also home-recording data (n = 41); the same pattern holds over all infants included in the initial analysis.]  #@NEW_LINE#@#  
We find a significant correlation between the proportion of object copresence and infants overall in-lab comprehension (r = 0.39, P = 0.013) (Fig 3).  #@NEW_LINE#@#  This result is consistent with a broader role for referential transparency in word learning, even among (early-learned) nouns; we return to this in Discussion.  #@NEW_LINE#@#  
We next considered four measures of object word quantity from the literature: number of types, tokens, and words read, along with typetoken ratio.  #@NEW_LINE#@#  These variables tend to be highly correlated with each other, and, indeed, we find each of them significantly correlated with the others in both audio and video data (|r| between 0.23 and 0.82; P less_than 0.05).  #@NEW_LINE#@#  (As expected, typetoken ratio negatively correlated with quantity; all other rs were positive.)  #@NEW_LINE#@#  
Given predicted correlations among quantity measures, we asked how these measures correlated with lab performance.  #@NEW_LINE#@#  No video quantity measures correlated significantly with in-lab comprehension.  #@NEW_LINE#@#  However, several quantity metrics from the audio recordings (number of types, tokens, and object wordcontaining reading utterances) were close to the Pless_than0.05 significance threshold [r = 0.32, P = 0.047; r = 0.29, P = 0.074; and () = 0.19, P = 0.088, respectively]; this leaves open the possibility that an expanded home and lab sample, or other measures of word knowledge and/or input quantity, would render clearer results.  #@NEW_LINE#@#  
Looking at talker variability, we again did not find significant correlations with in-lab comprehension in audio or video recordings, or in the proportion of input from infants mothers (all P  0.05).  #@NEW_LINE#@#  Finally, looking at utterance type variability (calculated as each infants average entropy across utterance types), we found no significant correlations with in-lab comprehension (P = 0.58), likely due to highly similar utterance type distributions across participants (Fig 2).  #@NEW_LINE#@#  We replicated previous work reporting that infants hear 10% of words in isolation (31), but this property too was not associated with in-lab comprehension (P = 0.43).  #@NEW_LINE#@#  
Thus, across the four aspects of home environment we predicted would positively correlate with in-lab comprehension, only object copresence clearly did so, although it is premature to conclude that this variable is a differentially better predictor than the others.  #@NEW_LINE#@#  Given that our analyses were predicated upon directional predictions, but served as initial exploratory steps, we evaluated this correlation further by calculating bootstrapped CIs with 1,000 iterations.  #@NEW_LINE#@#  The 95% CI did not include 0 (0.15 to 0.62).  #@NEW_LINE#@#  (Audio and video object copresence were marginally correlated with each other, r = 0.29 and P = 0.064.)  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
Consistent with research on adults and children, we find that 6-mo-olds understand words more readily when shown two semantically unrelated referents than when shown two related ones.  #@NEW_LINE#@#  We further find initial evidence that, in these same infants, in-lab word comprehension is linked with referential transparency in the home, but not with measures of talker or utterance-type, and only marginally with input quantity.  #@NEW_LINE#@#  These findings enrich our understanding of infants real-time word comprehension, and the longer-scale learning environment that fuels it.  #@NEW_LINE#@#  
Our eye-tracking results suggest that even first words are not unconnected islands of meaning; they already contain semantic structure.  #@NEW_LINE#@#  Still, there are various interpretations of infants relatively strong performance on unrelated trials (e.g., carjuice), versus their poor performance on related trials (e.g., carstroller).  #@NEW_LINE#@#  One possibility is that infants know (something about) the tested words, but cannot overcome semantic competition on related trials, i.e., hearing car leads to car looking, but also activates related words, e.g., stroller, to a similar (or indistinguishable) degree.  #@NEW_LINE#@#  
Alternatively, infants word knowledge may be underspecified: They may know enough about a words meaning to tell it apart from the unrelated referent but not the related one (which, by design, has distributional, conceptual, and/or visual overlap).  #@NEW_LINE#@#  That is, perhaps infants know car cannot refer to juice, but not whether stroller is in the car category.  #@NEW_LINE#@#  
Furthermore, these options may intertwine, and, indeed, our time course data are compatible with both (Fig S2): Infants consistently looked at the labeled target image in related trials, but shifted between the images in unrelated trials, consistent with underspecification or competition.  #@NEW_LINE#@#  
In studies of children and adults, these possibilities are disambiguated by both cleaner (i.e., less noisy, more accurate) eye movements, which reveal transient looks at the semantically related distractor, and overt (touch or click) target selection, which 6-mo-olds (who do not even point yet) cannot do.  #@NEW_LINE#@#  Additional infant measures (e.g., neural recordings or reaching tasks) may be promising future directions.  #@NEW_LINE#@#  
These results also complement previous early word comprehension research (1, 2, 4, 5).  #@NEW_LINE#@#  Here too, further work may elucidate how presentation method influences infants looking behavior (e.g., video vs. photo; two-image displays vs. scenes).  #@NEW_LINE#@#  
Turning to the corpus results, homelab links were relatively limited.  #@NEW_LINE#@#  Specifically, we did not find a relation between how infants object word input was distributed across speakers and utterance types, and infants comprehension of common nouns.  #@NEW_LINE#@#  Especially for utterance type, this may reflect the limits of analyzing only object word utterances.  #@NEW_LINE#@#  Similarly, input quantity measures (which are tied to toddlers vocabulary, e.g., ref.  #@NEW_LINE#@#  24) were only marginally correlated with comprehension.  #@NEW_LINE#@#  While these variables may matter for subsequent lexical knowledge, they did not unequivocally do so here.  #@NEW_LINE#@#  Given our young and relatively homogenous sample, we may have had limited variability therein.  #@NEW_LINE#@#  
Intriguingly, object copresence significantly correlated with in-lab comprehension.  #@NEW_LINE#@#  Of course, infants do not learn words and referents they have not experienced.  #@NEW_LINE#@#  Furthermore, parents focusing in on infants attention likely provides higher-quality learning instances (35).  #@NEW_LINE#@#  Our results also suggest an expansion of Bergelson and Swingley (36), who found that common nouns are more referentially transparent than common nonnouns.  #@NEW_LINE#@#  Here we suggest that referential transparency for a given infant may map onto that same childs comprehension, within the already referentially transparent noun class.  #@NEW_LINE#@#  
These results are first steps toward understanding how the initial lexicon is organized and acquired from experience by 6 mo.  #@NEW_LINE#@#  We find that real-time comprehension is influenced by links among words.  #@NEW_LINE#@#  We further find promising results tying infants experiences with referential transparency to early word knowledge.  #@NEW_LINE#@#  Finally, these results suggest a combined labhome (LH) approach can begin to reveal the range and dynamics of learner-by-environment interaction, as infants initial understanding of words starts to give way to robust knowledge of their language.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
Participants  #@NEW_LINE#@#  
The final eye-tracking experiment sample was 51 6-mo-olds (M = 6.1 mo, r = 5.6 mo to 6.7 mo, 23 female).  #@NEW_LINE#@#  Families elected to participate in a one-time lab-only (LO) study (n = 12; LO group), or additionally enroll in a larger yearlong study with home visits (n = 44; LH group).  #@NEW_LINE#@#  Four further infants who participated in the eye-tracking study were excluded for fussiness or calibration failure resulting in 0 trials with sufficient data for analysis, in one or both trial types.  #@NEW_LINE#@#  One additional excluded infant performed 5 SD above the mean.  #@NEW_LINE#@#  See Supporting Information for further details on per-trial exclusion criteria.  #@NEW_LINE#@#  
Infants were recruited from a database in Rochester, NY.  #@NEW_LINE#@#  All children were healthy, had no hearing or vision problems, were carried full term (40 ± 3 wk), and heard 75% English at home.  #@NEW_LINE#@#  Families received $10 and a small gift for participating in the lab study, and a further $5 if they also completed the home visits (audio and video).  #@NEW_LINE#@#  Families who completed our optional demographics questionnaire (98%) reported that infants were largely white and middle class (LH: 95% white, 75% of mothers received a B.A.  #@NEW_LINE#@#  or higher; LO: 79% and 57%, respectively).  #@NEW_LINE#@#  
Given unknown effect sizes for homelab links in young infants, the target sample size was 48, i.e., 3× the standard minimum sample size (n = 16).  #@NEW_LINE#@#  We enrolled and retained 44 infants over an 8-mo enrollment window for the LH group.  #@NEW_LINE#@#  Once LH enrollment ended, LO did as well.  #@NEW_LINE#@#  

Lab_Visit_Procedure  #@NEW_LINE#@#  
First, staff explained the study to families and obtained consent for either the yearlong study (including home recordings) or one-time eye tracking, as relevant (approved by the University of Rochester IRB).  #@NEW_LINE#@#  Parents then completed surveys about their child; see Supporting Information.  #@NEW_LINE#@#  
Next, the parent sat with the infant in their lap in a dimly lit testing room, in front of an Eyelink 1000+ Eyetracker (SR Research), which was in head-free mode, and sampled monocularly at 500 Hz (less_than0.5° average accuracy).  #@NEW_LINE#@#  A small sticker on the infants forehead tracked head movements.  #@NEW_LINE#@#  The experiment was run from a computer that was back-to-back with the testing monitor, allowing for adjustment if the child moved out of eye-tracking range.  #@NEW_LINE#@#  
The experiment began with four warm-ups, in which a single image was labeled by a sentence played over speakers (e.g., Look at the apple!).  #@NEW_LINE#@#  Parents were then given a visor that blocked the screen (or closed their eyes, n = 3), and over-ear headphones.  #@NEW_LINE#@#  The experiment was also recorded by camcorder to ensure compliance and monitor the childs state.  #@NEW_LINE#@#  
Next came 32 test trials, in which infants saw two images on a gray background, and heard a sentence labeling one image (Fig 4).  #@NEW_LINE#@#  An attention getter was shown as needed.  #@NEW_LINE#@#  On each test trial, parents spoke a single sentence aloud to their child, which labeled one of the images on the screen; they first heard a prerecorded sentence over headphones that they then repeated aloud (1).  #@NEW_LINE#@#  Images were shown for 5 s after target word onset; the length of time before the parent said the target word after the images appeared varied across trials, averaging 3 s to 4 s.  #@NEW_LINE#@#  
Each infant saw both trial types (16 related and 16 unrelated trials, interspersed pseudorandomly).  #@NEW_LINE#@#  Infants were alternately assigned to two trial orders, which counterbalanced side and ordering of images, target items, and trial type.  #@NEW_LINE#@#  

Stimuli  #@NEW_LINE#@#  
Sixteen common concrete nouns were chosen as target words, based on corpora and prior research; see Supporting Information and Fig 4.  #@NEW_LINE#@#  Each word was part of two item pairs, one in each trial type (related and unrelated, e.g., dogbaby and spoonbaby; n = 16 item pairs).  #@NEW_LINE#@#  Pairings maximized semantic overlap within related pairs, and minimized it in unrelated pairs.  #@NEW_LINE#@#  Semantic network analyses confirmed that related pairs were more similar than unrelated pairs; see Supporting Information.  #@NEW_LINE#@#  Within trial type, items were paired to minimize phonetic overlap (one related pair unavoidably had the same initial consonant: bookball).  #@NEW_LINE#@#  
Audio stimuli were sentences recorded by a female using infant-directed speech prosody in a sound booth; they were 1.1 s to 1.8 s, normalized to 72 db (i.e., a volume that allowed only parents to hear them when they were played over the parent headphones).  #@NEW_LINE#@#  Infants only heard the sentences from their parents (see Lab Visit Procedure).  #@NEW_LINE#@#  Each sentence occurred in one of four carrier phrases: Can you find the X?  #@NEW_LINE#@#  Wheres the X?  #@NEW_LINE#@#  Do you see the X?  #@NEW_LINE#@#  and Look at the X!  #@NEW_LINE#@#  where X is the target word (only one sentence frame was used per item pair).  #@NEW_LINE#@#  
Visual stimuli were photos of each target and warm-up word, edited onto a gray background, displayed at 500 × 500 pixels on a 27.4- by 34-cm LCD 96 pixels per inch screen, at a viewing distance of 55 cm to 60 cm.  #@NEW_LINE#@#  Warm-up images (n = 4) were displayed centrally.  #@NEW_LINE#@#  For test trials (n = 32), each image was centered within the left and right half of the screen (counterbalanced across trials).  #@NEW_LINE#@#  Each of 16 test photos occurred four times: once as target and once as distractor in each trial type (related and unrelated; see Fig 4).  #@NEW_LINE#@#  

Home-Recording_Procedure  #@NEW_LINE#@#  
Home recordings captured infants typical environment through an hour-long video recording and, on a separate day, a daylong audio recording.  #@NEW_LINE#@#  Before recording, parents were given a release form, which allowed up to three levels of sharing; see Supporting Information.  #@NEW_LINE#@#  Recordings that parents opted to share with authorized researchers can be found on Databrary and Homebank (n = 43); access is available to researchers who complete ethics certification and membership agreements through these repositories.  #@NEW_LINE#@#  See sample clips in Supporting Information.  #@NEW_LINE#@#  
Video recordings took place at infants homes.  #@NEW_LINE#@#  Research staff put a specialized hat on the child, and a small Looxcie camera (8.4 × 1.7 × 1.3 cm, 22 g) was affixed above each ear with Velcro (one pointed slightly up and one slightly down, to better capture the childs visual field).  #@NEW_LINE#@#  Staff also put a camcorder on a tripod in the corner.  #@NEW_LINE#@#  Parents were given an information sheet, asked to move the tripod if they changed rooms, and given staff contacts.  #@NEW_LINE#@#  Staff departed and returned after 1 h.  #@NEW_LINE#@#  
At either the lab or home visit, parents were given a LENA (Language Environment Analysis) audio recorder (8.6 × 5.6 × 1.3 cm, 57 g) and clothing with a LENA pocket.  #@NEW_LINE#@#  (LENA Foundation).  #@NEW_LINE#@#  Parents were asked to turn the recorder on when the child awoke, and off at the end of the day or if they wanted to stop recording.  #@NEW_LINE#@#  Recording time ranged from 10.66 h to 16.00 h (M = 14.37).  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
We thank SEEDLingS staff: Amatuni, Dailey, Koorathota, Schneider, Tor; research assistants at University of Rochester and Duke University; and National Institutes of Health Grants T32 DC000035 and DP5-OD019812 (to E.B.)  #@NEW_LINE#@#  and HD-037082 (to R.N.A.  #@NEW_LINE#@#  ).  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  

Published under the PNAS license.  #@NEW_LINE#@#  

