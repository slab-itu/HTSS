article id="http://dx.doi.org/10.1073/pnas.1721355115"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
This study measures face identification accuracy for an international group of professional forensic facial examiners working under circumstances that apply in real world casework.  #@NEW_LINE#@#  Examiners and other human face specialists, including forensically trained facial reviewers and untrained superrecognizers, were more accurate than the control groups on a challenging test of face identification.  #@NEW_LINE#@#  Therefore, specialists are the best available human solution to the problem of face identification.  #@NEW_LINE#@#  We present data comparing state-of-the-art face recognition technology with the best human face identifiers.  #@NEW_LINE#@#  The best machine performed in the range of the best humans: professional facial examiners.  #@NEW_LINE#@#  However, optimal face identification was achieved only when humans and machines worked in collaboration.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Achieving the upper limits of face identification accuracy in forensic applications can minimize errors that have profound social and personal consequences.  #@NEW_LINE#@#  Although forensic examiners identify faces in these applications, systematic tests of their accuracy are rare.  #@NEW_LINE#@#  How can we achieve the most accurate face identification: using people and/or machines working alone or in collaboration?  #@NEW_LINE#@#  In a comprehensive comparison of face identification by humans and computers, we found that forensic facial examiners, facial reviewers, and superrecognizers were more accurate than fingerprint examiners and students on a challenging face identification test.  #@NEW_LINE#@#  Individual performance on the test varied widely.  #@NEW_LINE#@#  On the same test, four deep convolutional neural networks (DCNNs), developed between 2015 and 2017, identified faces within the range of human accuracy.  #@NEW_LINE#@#  Accuracy of the algorithms increased steadily over time, with the most recent DCNN scoring above the median of the forensic facial examiners.  #@NEW_LINE#@#  Using crowd-sourcing methods, we fused the judgments of multiple forensic facial examiners by averaging their rating-based identity judgments.  #@NEW_LINE#@#  Accuracy was substantially better for fused judgments than for individuals working alone.  #@NEW_LINE#@#  Fusion also served to stabilize performance, boosting the scores of lower-performing individuals and decreasing variability.  #@NEW_LINE#@#  Single forensic facial examiners fused with the best algorithm were more accurate than the combination of two examiners.  #@NEW_LINE#@#  Therefore, collaboration among humans and between humans and machines offers tangible benefits to face identification accuracy in important applications.  #@NEW_LINE#@#  These results offer an evidence-based roadmap for achieving the most accurate face identification possible.  #@NEW_LINE#@#  

Examples highlighting the face region in the images used in this study (all image pairs are shown in SI Appendix, Figs.  #@NEW_LINE#@#  S8S14).  #@NEW_LINE#@#  (Left) This pair is a same identity pair, and (Right) this pair shows a different identity pair.  #@NEW_LINE#@#  
Results  #@NEW_LINE#@#  
Accuracy  #@NEW_LINE#@#  
Fig 2 shows performance of the subject groups and algorithms using the area under the receiver operating characteristic curve (AUC) as a measure of accuracy.  #@NEW_LINE#@#  The groups are ordered by AUC median from the most to least accurate: facial examiners (0.93), facial reviewers (0.87), superrecognizers (0.83), fingerprint examiners (0.76), and students (0.68).  #@NEW_LINE#@#  Algorithm performance increased monotonically from the oldest algorithm (A2015) to the newest algorithm (A2017b).  #@NEW_LINE#@#  Comparing the algorithms with the human groups, the publicly available algorithm (A2015) performed at a level similar to the students (0.68).  #@NEW_LINE#@#  Algorithm A2016 performed at the level of fingerprint examiners (0.76).  #@NEW_LINE#@#  Algorithm A2017a performed at a level (0.85) comparable with the superrecognizers (0.83) and reviewers (0.87).  #@NEW_LINE#@#  The performance of A2017b (0.96) was slightly higher than the median of the facial examiners (0.93).  #@NEW_LINE#@#  
More formally, all face specialist groups surpassed fingerprint examiners (facial examiners, P = 2.14×106; facial reviewers, P = 0.004; superrecognizers, P = 0.017).  #@NEW_LINE#@#  The face specialist groups also surpassed students (facial examiners, P = 2.53×108; facial reviewers, P = 4.01×106; superrecognizers, P = 0.0005) (SI Appendix, SI Text).  #@NEW_LINE#@#  Performance across the face specialist groups did not differ statistically.  #@NEW_LINE#@#  Summary statistics for accuracy, however, should be interpreted in the context of the full performance distributions within each group.  #@NEW_LINE#@#  

Performance_Distributions  #@NEW_LINE#@#  
Individual accuracy varied widely in all groups.  #@NEW_LINE#@#  All face specialist groups (facial examiners, reviewers, and superrecognizers) had at least one participant with an AUC below the median of the students.  #@NEW_LINE#@#  At the top of the distribution, all but the student group had at least one participant with no errors.  #@NEW_LINE#@#  To examine specialist groups in the context of the general population (students), we fit a Gaussian distribution to the student AUCs (SI Appendix, SI Text).  #@NEW_LINE#@#  Next, we computed the fraction of participants in each group who scored above the 95th percentile (Fig 2, dashed line).  #@NEW_LINE#@#  For the facial examiner group, 53% were above the 95th percentile of students; for the facial reviewers, this proportion was 36%.  #@NEW_LINE#@#  For superrecognizers, it was 46%, and for fingerprint examiners, it was 17%.  #@NEW_LINE#@#  For the algorithms, the accuracy of A2017b was higher than the majority (73%) of participants in the face specialist groups.  #@NEW_LINE#@#  Conversely, 35% of examiners, 13% of reviewers, and 23% of superrecognizers were more accurate than A2017b.  #@NEW_LINE#@#  Compared with students, the accuracy of A2017b was equivalent to a student at the 98th percentile (z score = 2.090), A2017a was at the 91st percentile (z score = 1.346), A2016 was at the 76th percentile (z score = 0.676), and A2015 was at the 53rd percentile (z score = 0.082).  #@NEW_LINE#@#  These results show a steady increase in algorithm accuracy from a level comparable with students in 2015 to a level comparable with the forensic facial examiners in 2017.  #@NEW_LINE#@#  

Fusing_Human_Judgments  #@NEW_LINE#@#  
In forensic practice, it is common for multiple examiners to review an identity comparison to assure consistency and consensus (3, 5).  #@NEW_LINE#@#  To examine the effects of fusion on accuracy, we combined individual participants judgments in each group.  #@NEW_LINE#@#  We began with one participant and increased the number of participants judgments fused from 2 to 10.  #@NEW_LINE#@#  To fuse n participants, we selected n participants randomly and averaged their rating-based judgments for each image pair.  #@NEW_LINE#@#  For fusing judgments, averaging is generally the most effective fusion strategy (21).  #@NEW_LINE#@#  An AUC was then computed from these average judgments.  #@NEW_LINE#@#  The sampling procedure was repeated 100 times for each value of n.  #@NEW_LINE#@#  
Median accuracy peaked at 1.0 (no errors) with the fusion of four examiners or three superrecognizers (Fig 3).  #@NEW_LINE#@#  The performance of all of the groups increased with fusion (SI Appendix, SI Text).  #@NEW_LINE#@#  For reviewers, the median peaked at 0.98 with 10 participants fused.  #@NEW_LINE#@#  Fingerprint examiners peaked at a median of 0.97 for 10 participants.  #@NEW_LINE#@#  For superrecognizers, the median increased from 0.83 to 0.98 when two superrecognizers were fused and to 1.0 when three or more superrecognizers were fused.  #@NEW_LINE#@#  Using a fusion perspective in comparing accuracy across participant groups, the data indicate that the median examiner (0.93) performs at a level roughly equal to two facial reviewers (median=0.93) and seven fingerprint examiners (median=0.94).  #@NEW_LINE#@#  Notably, the median of individual judgments by examiners is superior to the combination of 10 students (median=0.88).  #@NEW_LINE#@#  

Fusing_Humans_and_Machines  #@NEW_LINE#@#  
We examined the effectiveness of combining examiners, reviewers, and superrecognizers with algorithms.  #@NEW_LINE#@#  Human judgments were fused with each of the four algorithms as follows.  #@NEW_LINE#@#  For each face image pair, an algorithm returned a similarity score that is an estimate of how likely it is that the images show the same person.  #@NEW_LINE#@#  Because the similarity score scales differ across algorithms, we rescaled the scores to the range of human ratings (SI Appendix, SI Text).  #@NEW_LINE#@#  For each face pair, the human rating and scaled algorithm score were averaged, and the AUC was computed for each participantalgorithm fusion.  #@NEW_LINE#@#  
Fig 4 shows the results of fusing humans and algorithms.  #@NEW_LINE#@#  The most effective fusion was the fusion of individual facial examiners with algorithm A2017b, which yielded a median AUC score of 1.0.  #@NEW_LINE#@#  This score was superior to the combination of two facial examiners (MannWhitney U test = 2.82×104, n1=1,596, n2=57, P=8.37×107).  #@NEW_LINE#@#  Fusing individual examiners with A2017a and A2016 yielded performance equivalent to the fusion of two examiners (MannWhitney U test = 4.53×104, n1=1,596, n2=57, P=0.956; MannWhitney U test = 4.33×104, n1=1,596, n2=57, P=0.526, respectively).  #@NEW_LINE#@#  Fusing one examiner with A2015 did not improve accuracy over a single examiner (MannWhitney U test = 1,592, n1=57, n2=57, P=0.86).  #@NEW_LINE#@#  Fusing one examiner with A2017b proved more accurate than fusing one examiner with either A2017a or A2016 (MannWhitney U test = 1,054, n1=57, n2=57, P=7.92×104; MannWhitney U test = 942, n1=57, n2=57, P=7.28×105, respectively).  #@NEW_LINE#@#  Finally, fusing one examiner with both A2017b and A2017a did not improved accuracy over fusing one examiner with A2017b (MannWhitney U test = 1,414, n1=57, n2=57, P=0.21).  #@NEW_LINE#@#  This analysis was repeated for fusing algorithms and facial reviewers and for fusing algorithms and superrecognizers.  #@NEW_LINE#@#  Similar results were found for both groups (SI Appendix, SI Text).  #@NEW_LINE#@#  


Error_Rates_for_Highly_Confident_Decisions  #@NEW_LINE#@#  
In legal proceedings, the conclusions of greatest impact are identification errors made with high confidence.  #@NEW_LINE#@#  These can lead to miscarriages of justice with profound societal implications.  #@NEW_LINE#@#  In this study, the two responses that expressed high confidence were the observations strongly support that it is the same person (+3) and the observations strongly support that it is not the same person (3).  #@NEW_LINE#@#  To examine the error rates associated with judgments of +3 and 3, we computed the fraction of high-confidence same-person (+3) ratings made to different identity face pairs and estimated the error rate as a Bernoulli distribution.  #@NEW_LINE#@#  The Bernoulli parameter q^ is the fraction of different identity pairs that were given a rating of +3.  #@NEW_LINE#@#  Fig 5 shows the estimated parameter q^ with 95% confidence intervals by participant group.  #@NEW_LINE#@#  (SI Appendix, Table S2 shows estimated Bernoulli parameters and the confidence intervals.)  #@NEW_LINE#@#  The analysis was also conducted on the probability of same identity pairs being assigned a 3 rating.  #@NEW_LINE#@#  
For facial examiners, the error rate for judging with high confidence that two different faces were the same was 0.009 (upper limit of the confidence interval, 0.022).  #@NEW_LINE#@#  The corresponding error rate on judging the same person as two different people was 0.018 (upper limit of confidence interval, 0.030).  #@NEW_LINE#@#  For facial reviewers, the corresponding error rates and confidence intervals were similar to those for the facial examiners (SI Appendix, SI Text).  #@NEW_LINE#@#  For superrecognizers, although their error rate for the rating of +3 on two different faces was comparable with that of examiners and reviewers, their error rate for 3 ratings assigned to same face image pairs was higher.  #@NEW_LINE#@#  Student error rates for high-confidence decisions were substantially higher than those of the facial examiners, reviewers, and superrecognizers.  #@NEW_LINE#@#  Notably, we found that fusion reduced high-confidence errors for facial examiners, facial reviewers, and superrecognizers (SI Appendix, SI Text).  #@NEW_LINE#@#  Specifically, fusing one individual and A2017b was superior to fusing two individuals, and fusing two individuals was superior to one individual.  #@NEW_LINE#@#  
One possible explanation for these results is that forensic professionals avoid extreme ratings at both ends of the scale.  #@NEW_LINE#@#  To test this, we examined whether forensic professionals (facial examiners, facial reviewers, fingerprint examiners) overall made fewer high-confidence responses than nonprofessionals (superrecognizers, students).  #@NEW_LINE#@#  For each participant, the number of high-confidence responses was computed.  #@NEW_LINE#@#  Analysis showed that forensic professionals made fewer high-confidence decisions than nonforensic professionals (MannWhitney U test = 1,966.5, n1=140, n2=44, P = 2.83×104).  #@NEW_LINE#@#  This is consistent with a result obtained in a previous study by Norell et al.  #@NEW_LINE#@#  (22), which tested police detectives and students on face identity matching experiments.  #@NEW_LINE#@#  The result suggests that forensic training of any kind may affect the use of the response scale to avoid errors made with high confidence.  #@NEW_LINE#@#  

Discussion  #@NEW_LINE#@#  
The results of the study point to tangible ways to maximize face identification accuracy by exploiting the strengths of humans and machines working collaboratively.  #@NEW_LINE#@#  First, to optimize the accuracy of face identification, the best approach is to combine human and machine expertise.  #@NEW_LINE#@#  Fusing the most accurate machine with individual forensic facial examiners produced decisions that were more accurate than those arrived at by any pair of human and/or machine judges.  #@NEW_LINE#@#  This humanmachine combination yielded higher accuracy than the fusion of two individual forensic facial examiners.  #@NEW_LINE#@#  Computational theory indicates that fusing systems works best when their decision strategies differ (21, 23).  #@NEW_LINE#@#  Therefore, the superiority of humanmachine fusion over humanhuman fusion suggests that humans and machines have different strengths and weaknesses that can be exploited/mitigated by cross-fusion.  #@NEW_LINE#@#  
Second, for human decisions, the highest possible accuracy is obtained when human judgments are combined by simple averaging.  #@NEW_LINE#@#  The power of fusing human decisions to improve accuracy is well-known in the face recognition literature (3, 4).  #@NEW_LINE#@#  Our results speak to the tangible benefits of putting fusion formally into the process of a forensic decision-making process.  #@NEW_LINE#@#  Collaborative peer review of decisions is a common strategy in facial forensics.  #@NEW_LINE#@#  This study suggests that, in addition to social collaboration, computationally combining multiple independent decisions made in isolation also produces solid gains in accuracy (24).  #@NEW_LINE#@#  Although fusing student judgments improves accuracy, we show that there are limits to the gains possible from fusion.  #@NEW_LINE#@#  A fusion of student judgments will not approach the accuracy of fusing facial examiners or reviewers.  #@NEW_LINE#@#  This suggests that a strategy for achieving optimal accuracy is to fuse people in the most accurate group of humans.  #@NEW_LINE#@#  
Third, systematic differences were found for the performance of the human groups on average.  #@NEW_LINE#@#  Professional forensic facial examiners, professional facial reviewers, and superrecognizers were the most accurate groups.  #@NEW_LINE#@#  Fingerprint examiners were less accurate than the face specialists but more accurate than students.  #@NEW_LINE#@#  Notably, the group medians ranged from highly accurate for facial examiners (AUC = 0.93) to moderately above chance for students (AUC = 0.68).  #@NEW_LINE#@#  This suggests that our face matching test tapped into the entire operating range of normal human accuracy.  #@NEW_LINE#@#  
Fourth, the distribution of individual performance in this test was perhaps as informative as the summary data on central tendency.  #@NEW_LINE#@#  In particular, although the median accuracy measures strongly prescribe the use of professional facial examiners for cases where face identification accuracy is important, some individuals in this group performed poorly.  #@NEW_LINE#@#  Mitigating this concern to some extent, confident incorrect judgments by facial examiners were extremely rare.  #@NEW_LINE#@#  At the other end of the spectrum, some individuals in other groups performed with high accuracy that was well within the range of the best face specialists.  #@NEW_LINE#@#  Remarkably, in all but the student group, at least one individual performed the test with no errors.  #@NEW_LINE#@#  The range of accuracy of individuals in each group suggests the possibility of prescreening the general population for people with natural ability at face identification.  #@NEW_LINE#@#  The superrecognizers in our study were not trained formally in face recognition, yet they performed at levels comparable with those of the facial professionals.  #@NEW_LINE#@#  This suggests that both talent and training may underlie the high accuracy seen in the two groups of facial professionals.  #@NEW_LINE#@#  
Turning to the performance of the algorithms, the results indicate the potential for machines to contribute beneficially to the forensic process.  #@NEW_LINE#@#  Accuracy of the publicly available algorithm that we tested (A2015) was at the level of median accuracy of the studentsmodestly above chance.  #@NEW_LINE#@#  The other algorithms follow a rapid upward performance trajectory: from parity with a median fingerprint examiner (A2016) to parity with a median superrecognizer (A2017a) and finally, to parity with median forensic facial examiners (A2017b).  #@NEW_LINE#@#  There is now a decade-long effort to compare the accuracy of face recognition algorithms with humans (6).  #@NEW_LINE#@#  In the earliest tests (25), the face matching tasks presented relatively controlled images.  #@NEW_LINE#@#  As these tests progressed, algorithms and humans were compared on progressively more challenging image pairs.  #@NEW_LINE#@#  In this study, image pairs were selected to be extremely challenging based on both human and algorithm performance.  #@NEW_LINE#@#  The difficulty of these items for humans was supported by the accuracy of students, who represent a general population of untrained humans.  #@NEW_LINE#@#  Students performed poorly on these challenging image pairs.  #@NEW_LINE#@#  All four of the algorithms performed at or above median student performance.  #@NEW_LINE#@#  Two algorithms performed in the range of the facial specialists, and one algorithm matched the performance of forensic facial examiners.  #@NEW_LINE#@#  
In summary, this is the most comprehensive examination to date of face identification performance across groups of humans with variable levels of training, experience, talent, and motivation.  #@NEW_LINE#@#  We compared the accuracy of state-of-the-art face recognition algorithms with humans and show the benefits of a collaborative effort that combines the judgments of humans and machines.  #@NEW_LINE#@#  The work draws on previous cornerstone findings on human expertise and talent with faces, strategies for fusing human judgments, and computational advances in face recognition.  #@NEW_LINE#@#  The study provides an evidence-based roadmap for achieving highly accurate face identification.  #@NEW_LINE#@#  These methods should be extended in future work to test humans and machines on a wider range of face recognition tasks, including recognition across viewpoint and with low-quality images and video as well as recognition of faces from diverse demographic categories.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
Test_Protocol_for_Human_Participants  #@NEW_LINE#@#  
To allow examiners access to their tools and methods while comparing face images, participants in all conditions, except the untrained student control group, downloaded the pairs of face images and were allowed 3 mo to complete the comparisons.  #@NEW_LINE#@#  For facial examiners and reviewers, comparisons were completed in their laboratory using their tools and methods.  #@NEW_LINE#@#  For superrecognizers and fingerprint examiners, the comparisons were done on a computer using tools available on the computer (e.g., image software tools).  #@NEW_LINE#@#  Students viewed the face pairs presented on a computer monitor one at a time.  #@NEW_LINE#@#  The size of the images was preset, and it was the same for all images.  #@NEW_LINE#@#  Pairs remained visible until a response was entered on the keyboard.  #@NEW_LINE#@#  
For each pair of face images, the participants in all subject groups were required to respond on a 7-point scale: +3, the observations strongly support that it is the same person; +2, the observations support that it is the same person; +1, the observations support to some extent that it is the same person; 0, the observations support neither that it is the same person nor that it is different persons; 1, the observations support to some extent that it is not the same person; 2, the observations support that it is not the same person; 3, the observations strongly support that it is not the same person.  #@NEW_LINE#@#  The wording was chosen to reflect scales used by forensic examiners in their daily work.  #@NEW_LINE#@#  A receiver operating characteristic curve and the AUC were computed from the ratings for each subject.  #@NEW_LINE#@#  
The experimental design was approved by the National Institute of Standards and Technology (NIST) IRB.  #@NEW_LINE#@#  Data collection procedures for students were approved by the IRB at the University of Texas at Dallas, and all subjects provided consent.  #@NEW_LINE#@#  

Test_Protocol_for_Algorithms  #@NEW_LINE#@#  
Algorithms first encoded each face as a compact vector of feature values by processing the image with the trained DCNN.  #@NEW_LINE#@#  DCNNs consist of multiple layers of simulated neurons that convolute and pool input (face images), feeding the data forward to one or more fully connected layers at the top of the network.  #@NEW_LINE#@#  The output is a compressed feature vector that represents a face (algorithm A2015 uses 4,096 features, A2016 uses 320 features, and A2017a and A2017b use 512 features).  #@NEW_LINE#@#  For each image pair in the test, a similarity score was computed between the representations of the two faces.  #@NEW_LINE#@#  The similarity score is the algorithms estimate of whether the images show the same person.  #@NEW_LINE#@#  To avoid response bias, performance was measured by computing an AUC directly from the similarity score distributions for same and different identity pairs, eliminating the need for a threshold.  #@NEW_LINE#@#  SI Appendix, SI Text has details on the algorithms.  #@NEW_LINE#@#  

Stimuli  #@NEW_LINE#@#  
Image pairs were chosen carefully in three screening steps.  #@NEW_LINE#@#  These steps were based on human and algorithm performance (details follow).  #@NEW_LINE#@#  The goal of the screening process was to select highly challenging image pairs that would test the upper limits of the participants skills, while avoiding floor effects for the students.  #@NEW_LINE#@#  The starting point for pair selection was a set of 9,307 images of 507 individuals taken with a Nikon D70 6 megapixel single-lens reflex camera.  #@NEW_LINE#@#  Images were acquired during a single academic year in indoor and outdoor settings at the University of Notre Dame.  #@NEW_LINE#@#  Faces were in approximately frontal pose (Fig 1 shows example pairs).  #@NEW_LINE#@#  
We screened for identity matching difficulty with a fusion of three top-performing algorithms from an international competition of algorithms [Face Recognition Vendor Test 2006 (FRVT 2006)] (26).  #@NEW_LINE#@#  Based on the results of the fusion algorithm, the images were stratified into three difficulty levels (27).  #@NEW_LINE#@#  Image pairs were further pruned using human experimental data.  #@NEW_LINE#@#  We began with the accuracy of undergraduate students on the two most difficult levels for the algorithm (28, 29).  #@NEW_LINE#@#  We selected the highest performing 25% of participants and chose the 84 same identity and 84 different identity image pairs that elicited the highest proportion of errors in this group.  #@NEW_LINE#@#  These pairs formed a stimulus pool of image pairs that were challenging for humans and previous generation face recognition algorithms.  #@NEW_LINE#@#  A second stimulus pool was created in a similar way but with the goal of finding image pairs on which previous generation algorithms failed systematically.  #@NEW_LINE#@#  We sampled the stimuli from those used in a recent study that compared human and computer algorithm performance on a special set of image pairs for which machine performance in the FRVT 2006 (26) was 100% incorrect (29).  #@NEW_LINE#@#  Specifically, similarity scores computed between same identity faces were uniformly lower than those computed for the different identity image pairs.  #@NEW_LINE#@#  Finally, we implemented a third level of stimulus screening for both stimulus pools.  #@NEW_LINE#@#  We used performance on an identity matching task with very short (30 s) stimulus presentation times (3) and sorted these stimuli according to difficulty for the forensic examiners from that test.  #@NEW_LINE#@#  
Discussions with facial examiners before the study indicated that they were willing to compare 20 pairs of images over a 3-mo period.  #@NEW_LINE#@#  This allowed them to spend the time that they would normally spend for a forensic comparison.  #@NEW_LINE#@#  Using the screening described, we chose 12 image pairs from the first stimulus pool and 8 pairs from the second.  #@NEW_LINE#@#  There were same (n=12) and different identity (n=8) pairs.  #@NEW_LINE#@#  The slight imbalance eliminated the use of a process of elimination strategy (SI Appendix, SI Text).  #@NEW_LINE#@#  

Data_Availability  #@NEW_LINE#@#  
Deidentified data for facial examiners and reviewers, superrecognizers, and fingerprint examiners can be obtained by signing a data transfer agreement with the NIST.  #@NEW_LINE#@#  The images are available by license from the University of Notre Dame.  #@NEW_LINE#@#  Data for the students and algorithms are in Datasets S1 and S2.  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
Work was funded in part by the Federal Bureau of Investigation (FBI) to the NIST; the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) via IARPA R&D Contract 2014-14071600012 (to R.C.  #@NEW_LINE#@#  ); Australian Research Council Linkage Projects LP160101523 (to D.W.) and LP130100702 (to D.W.); and National Institute of Justice Grant 2015-IJ-CX-K014 (to A.J.O.).  #@NEW_LINE#@#  The views and conclusions contained herein should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, the IARPA, or the FBI.  #@NEW_LINE#@#  The US Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon.  #@NEW_LINE#@#  The identification of any commercial product or trade name does not imply endorsement or recommendation by the NIST.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  

This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).  #@NEW_LINE#@#  

