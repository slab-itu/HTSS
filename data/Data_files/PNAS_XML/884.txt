article id="http://dx.doi.org/10.1073/pnas.1711125114"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Identification of individuals by trait prediction using whole-genome sequencing data  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
By associating deidentified genomic data with phenotypic measurements of the contributor, this work challenges current conceptions of genomic privacy.  #@NEW_LINE#@#  It has significant ethical and legal implications on personal privacy, the adequacy of informed consent, the viability and value of deidentification of data, the potential for police profiling, and more.  #@NEW_LINE#@#  We invite commentary and deliberation on the implications of these findings for research in genomics, investigatory practices, and the broader legal and ethical implications for society.  #@NEW_LINE#@#  Although some scholars and commentators have addressed the implications of DNA phenotyping, this work suggests that a deeper analysis is warranted.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Prediction of human physical traits and demographic information from genomic data challenges privacy and data deidentification in personalized medicine.  #@NEW_LINE#@#  To explore the current capabilities of phenotype-based genomic identification, we applied whole-genome sequencing, detailed phenotyping, and statistical modeling to predict biometric traits in a cohort of 1,061 participants of diverse ancestry.  #@NEW_LINE#@#  Individually, for a large fraction of the traits, their predictive accuracy beyond ancestry and demographic information is limited.  #@NEW_LINE#@#  However, we have developed a maximum entropy algorithm that integrates multiple predictions to determine which genomic samples and phenotype measurements originate from the same person.  #@NEW_LINE#@#  Using this algorithm, we have reidentified an average of 8 of 10 held-out individuals in an ethnically mixed cohort and an average of 5 of either 10 African Americans or 10 Europeans.  #@NEW_LINE#@#  This work challenges current conceptions of personal privacy and may have far-reaching ethical and legal implications.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
First, we used 10-fold cross-validation (CV) to evaluate held-out predictions of each phenotype from the genome, images, and voice samples.  #@NEW_LINE#@#  For each of 10 random subsets of the data, we have trained models on the 9 remaining subsets.  #@NEW_LINE#@#  Accuracy was measured by the fraction of trait variance explained by the predictive model (RCV2), averaged over 10 CV sets (SI Appendix).  #@NEW_LINE#@#  Second, we consolidated all predictions into a single machine learning model for reidentifying genomes based on phenotypic prediction.  #@NEW_LINE#@#  This application establishes current limits on the deidentification of genomic data.  #@NEW_LINE#@#  
Study_Population  #@NEW_LINE#@#  
We collected a convenience sample of 1,061 individuals from the San Diego, CA, area.  #@NEW_LINE#@#  Their genomes were sequenced at an average depth of 30Ã— (17).  #@NEW_LINE#@#  The cohort was ethnically diverse, with 569, 273, 63, 63, and 18 individuals who identified themselves as of African, European, Latino, East Asian, and South Asian ethnicity, respectively, and 75 as others (Fig 1A).  #@NEW_LINE#@#  The genetic diversity in the San Diego area was reflected in continuous differences in admixture proportions (18) (Fig 1B).  #@NEW_LINE#@#  It also included a diverse age range from 18 to 82 y old, with an average of 36 y old (Fig 1C).  #@NEW_LINE#@#  Each individual underwent standardized collection of phenotypes, including high-resolution 3D facial images, voice samples, quantitative eye and skin colors, age, height, and weight (Fig 1).  #@NEW_LINE#@#  The study was approved by the Western Institutional Review Board, Puyallup, WA.  #@NEW_LINE#@#  All study participants provided informed consent, allowing research use of their data (see SI Appendix).  #@NEW_LINE#@#  

Predicting_Face_and_Voice  #@NEW_LINE#@#  
Modern facial- and voice-recognition systems reach human-level identification performance (19, 20).  #@NEW_LINE#@#  Although still in its infancy, genomic prediction of the face may enable identification of a person.  #@NEW_LINE#@#  We first represented face shape and texture variation using principal components (PC) analysis to define a low-dimensional representation of the face (14, 2125).  #@NEW_LINE#@#  Next, we predicted each face PC separately using ridge regression with ancestry information from 1,000 genomic PCs [also equivalent to genomic best linear unbiased prediction from common variation (26)], with sex, BMI, and age as covariates.  #@NEW_LINE#@#  We undertook a similar procedure using distances between 3D landmarks.  #@NEW_LINE#@#  A sample of predicted faces is presented in Fig 2.  #@NEW_LINE#@#  Predictions for 24 consented individuals are presented in SI Appendix, Fig S11.  #@NEW_LINE#@#  We observed that facial predictions reflected the sex and ancestry proportions of the individual.  #@NEW_LINE#@#  
To assess the influence of each covariate on predictive accuracy, we measured the per-pixel RCV2 between observed and predicted faces.  #@NEW_LINE#@#  Because errors were anisotropic, we separated residuals for horizontal, vertical, and depth dimensions.  #@NEW_LINE#@#  Fig 3 shows the distribution of RCV2 along each axis as a function of the model covariates.  #@NEW_LINE#@#  We observed from this plot that sex and genomic PCs alone explained large fractions of the predictive accuracy of the model.  #@NEW_LINE#@#  Previously reported single nucleotide polymorphisms (SNPs) related to facial structure (5, 14, 27) did not improve the sex and PC model.  #@NEW_LINE#@#  In contrast, we found that accounting for age and BMI improved the accuracy of facial structure along the horizontal and vertical dimensions (Fig 3).  #@NEW_LINE#@#  To further understand predictive accuracy for the full model, we mapped per-pixel accuracy onto the average facial scaffold (Fig 4), finding that most of the predictive accuracy was in facial regions that differed the most between African and European individuals (SI Appendix, Fig S13): Much of the predictive accuracy along the horizontal dimension came from estimating the width of the nose and lips.  #@NEW_LINE#@#  Along the vertical dimension, we obtained the highest precision in the placement of the cheekbones and the upper and lower regions of the face.  #@NEW_LINE#@#  For the depth axis, the most predictable features were the protrusions of the brow, nose, and lips.  #@NEW_LINE#@#  A genome-wide association study (GWAS) on distances between 36 landmarks (SI Appendix, Tables S1 and S2) found no significant associations after correcting for the number of phenotypes tested (SI Appendix and Dataset S1).  #@NEW_LINE#@#  Because the predictive analysis used the same cohort, we did not use any results from our GWAS to improve (i.e., overfit) predictive models.  #@NEW_LINE#@#  
For prediction of voice, we extracted and predicted a 100-dimensional identity-vector and voice pitch embedding (28) from voice samples collected from our cohort.  #@NEW_LINE#@#  Similar to face prediction, we fitted ridge regression models to each dimension of the embedding.  #@NEW_LINE#@#  As covariates, we used 1,000 genomic PCs and sex.  #@NEW_LINE#@#  We were able to predict voice pitch with an RCV2 of 0.70.  #@NEW_LINE#@#  However, predictions for only 3 of the 100 identity-vector dimensions exceeded an RCV2 of 0.10.  #@NEW_LINE#@#  
Besides genomic prediction, our method for reidentification used predictions from image and voice embeddings.  #@NEW_LINE#@#  Face shape, face color, and voice were reasonably predictive of age, sex, and ancestry (Table 1).  #@NEW_LINE#@#  In summary, we are able to predict variation in face and voice from WGS data and to predict age, sex, and ancestry from face and voice embeddings.  #@NEW_LINE#@#  

Predicting_Age_from_WGS_Data  #@NEW_LINE#@#  
Age is a soft biometric that narrows down identity (15).  #@NEW_LINE#@#  We predicted age from WGS data based on somatic changes that are biologically associated with aging (e.g., telomere shortening).  #@NEW_LINE#@#  Telomere length can be estimated from WGS data based on the proportion of reads containing telomere repeats (29).  #@NEW_LINE#@#  We predicted age from estimated telomere length with RCV2=0.29 (Fig 5A).  #@NEW_LINE#@#  A similar method had been reported to predict age from telomeres with an R2 of 0.05 (29), consistent with our result on 1,960 females from the same cohort that had been sequenced by using the same pipeline as our study cohort (SI Appendix) (30).  #@NEW_LINE#@#  In addition to telomere length, we were able to detect mosaic loss of the X chromosome with age in women from WGS data.  #@NEW_LINE#@#  This effect has been reported using in situ hybridization (31).  #@NEW_LINE#@#  In men, no such effect has been observed, presumably because at least one functioning copy of the X chromosome is required per cell.  #@NEW_LINE#@#  Additionally, we were able to replicate previous results (32, 33) and detect mosaic loss of the Y chromosome with age in men.  #@NEW_LINE#@#  Together, telomere shortening and sex chromosome loss, quantified by using sex chromosome copy numbers, were predictive of age, with an RCV2 of 0.44 (mean absolute error (MAE) = 8.0 y).  #@NEW_LINE#@#  

Height__Weight__and_BMI_Prediction  #@NEW_LINE#@#  
To predict height, weight, and BMI, we applied joint shrinkage to previously reported effect sizes (3436).  #@NEW_LINE#@#  For height, where we observed the largest predictive power among these traits, a model using reported SNP effects alone yielded RCV2=0.06 in males (m) and RCV2=0.08 in females (f).  #@NEW_LINE#@#  Simulations indicated that such predictive performance would result in marginal improvements in discriminative power over random (SI Appendix, Fig S34).  #@NEW_LINE#@#  Consequently, models added genomic PCs and sex.  #@NEW_LINE#@#  As shown in Fig 5B, we observed a strong performance for the prediction of height (RCV2=0.53, MAE=4.9cm) and weaker performance for the prediction of weight (RCV2=0.14, MAE=15.6kg) and BMI (RCV2=0.17, MAE=5.3kg/m2).  #@NEW_LINE#@#  

Eye_Color_and_Skin_Color_Prediction  #@NEW_LINE#@#  
Whereas weight and BMI have complex genetic architecture and have mid to high heritability estimates from 50 to 93% (34, 37), eye color has an estimated heritability of 98% (38), with eight SNPs determining most of the variability (39).  #@NEW_LINE#@#  Similarly, skin color has an estimated heritability of 81% (40), with 11 genes predominantly contributing to pigmentation (41).  #@NEW_LINE#@#  
For both eye and skin color, previous models predicted color categories rather than continuous values (10, 13, 42), often by using ad hoc decision rules.  #@NEW_LINE#@#  To our knowledge, none have used genome-wide variation to predict color.  #@NEW_LINE#@#  Here, we modeled eye and skin color as 3D continuous RGB values, maintaining the full color variation (see Fig 5 C and D for eye and skin color, respectively).  #@NEW_LINE#@#  For both, we calculated per-channel RCV2 of 0.770.82.  #@NEW_LINE#@#  

Linking_Genomes_to_Phenotypic_Profiles  #@NEW_LINE#@#  
In the previous sections, we presented predictive models for face, voice, age, height, weight, BMI, eye color, and skin color.  #@NEW_LINE#@#  We integrated each of the predictions as outlined in Fig 6.  #@NEW_LINE#@#  In brief, we used predictive models to embed each phenotype and each genome and ranked individuals by their similarity computed from the embeddings listed in SI Appendix, Table S14.  #@NEW_LINE#@#  Face and voice prediction were modified to use genomic predictions of sex, BMI, and age rather than observed values.  #@NEW_LINE#@#  We predicted sex, age and ancestry proportions from face and voice as additional variables that could be compared with corresponding genomic predictions (RCV2 in SI Appendix, Tables S3 and S4).  #@NEW_LINE#@#  Finally, to account for variations in accuracy, we learned an optimal similarity for matching observed and predicted values for each feature set, leading to consistent improvements over naive combination of predictors (SI Appendix, Figs.  #@NEW_LINE#@#  S26 and S28).  #@NEW_LINE#@#  To assess the matching performance, we considered the following tasks.  #@NEW_LINE#@#  Given an individuals WGS data, we sought to identify that individual out of N suspects whose phenotypes were observed, a problem that we refer to as select at N (sN).  #@NEW_LINE#@#  In a second scenario, we evaluated whether deidentified WGS samples of N individuals could be matched to their N phenotypic sets (i.e., images and demographic information).  #@NEW_LINE#@#  This scenario corresponds to the reidentification of genomic databases.  #@NEW_LINE#@#  We refer to this challenge as match at N (mN).  #@NEW_LINE#@#  Fig 7A presents a schematic of sN and mN.  #@NEW_LINE#@#  In contrast to sN, where a genome is paired to the most similar phenotypic profile, for mN, each genome was paired to one and only one phenotypic set in a globally optimal manner.  #@NEW_LINE#@#  That is, we treated mN as a bipartite graph matching problem and maximized the expected number of correct pairs (6, 43).  #@NEW_LINE#@#  Table 2 shows sN and mN accuracy across feature sets and pool sizes averaged over all possible lineups per CV fold.  #@NEW_LINE#@#  To further assess the reidentification performance beyond basic demographic information, we include results stratified by gender (SI Appendix, Fig S29); the largest ethnicity groups, AFR and EUR (SI Appendix, Fig S30); and gender/ethnicity (SI Appendix, Fig S31).  #@NEW_LINE#@#  Corresponding receiver operating characteristic curves are provided in SI Appendix, Figs.  #@NEW_LINE#@#  S26 and S27.  #@NEW_LINE#@#  We considered three sets of information: (i) 3D face; (ii) demographic variables such as age, self-reported gender, and ethnicity; and (iii) additional traits like voice, height, weight, and BMI.  #@NEW_LINE#@#  We found that 3D face alone is most informative, with an s10 of 58% (m, 42%; f, 43%; AFR, 32%; EUR, 35%).  #@NEW_LINE#@#  Ethnicity was second, achieving an s10 of 50% (m, 48%; f, 52%).  #@NEW_LINE#@#  Voice had an s10 of 42% (m, 27%; f, 31%; AFR, 29%; EUR, 25%), whereas age, gender, and height/weight/BMI yielded sN of 20% (m, 19%; f, 20%; AFR, 20%; EUR, 20%), 21% (AFR, 20%; EUR, 20%), and 27% (m, 17%; f, 18%; AFR, 23%; EUR, 24%), respectively.  #@NEW_LINE#@#  Finally, we integrated these variables to obtain an s10 of 74% (m, 65%; f, 65%; AFR, 44%; EUR, 50%).  #@NEW_LINE#@#  For the full model, m10 was 83% (m, 72%; f, 70%; AFR, 47%; EUR, 57%), compared with 64% (m, 44%; f, 46%; AFR, 33%; EUR, 34%) for 3D face alone.  #@NEW_LINE#@#  
We evaluated the scenario that tests the probability of including the true individual in a 10-person subset of a random 100-person pool chosen from our cohort.  #@NEW_LINE#@#  Fig 7B presents our ability to ensure that an individual is in the top M from a pool of size NM.  #@NEW_LINE#@#  We ranked the correct individual in the top M=10 of N=100 88% of the time, showing the ability to enrich for persons of interest.  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
We have presented predictive models for facial structure, voice, eye color, skin color, height, weight, and BMI from common genetic variation and have developed a model for estimating age from WGS data.  #@NEW_LINE#@#  Despite limitations in statistical power due to the small sample size of 1,061 individuals, predictions are sound.  #@NEW_LINE#@#  Although individually, each predictive model provided limited information about an individuals identity, we have derived an optimal similarity measure from multiple prediction models that enabled matching between genomes and phenotypic profiles with good accuracy.  #@NEW_LINE#@#  Over time, predictions will get more precise, and, thus, the results of this work will be of greater consideration in the current discussion on genome privacy protection.  #@NEW_LINE#@#  Although precision will be gained from larger GWAS contributing common variants, our simulation results indicate that high values of R2 are required to significantly improve identification (SI Appendix, Figs.  #@NEW_LINE#@#  S33 and S34).  #@NEW_LINE#@#  These values will likely be obtained by improved phenotyping (e.g., imaging) or from sequencing studies contributing low-frequency variants that have larger effects (44) and discriminate interregional admixture on a finer level (45).  #@NEW_LINE#@#  Precision will also improve from integration of other experimental sources.  #@NEW_LINE#@#  For example, age prediction from DNA methylation (46) would be expected to improve performance over a purely genome-based approach.  #@NEW_LINE#@#  
Today, HIPAA does not consider genome sequences as identifying information that has to be removed under the Safe Harbor Method for deidentification.  #@NEW_LINE#@#  Based on an assessment of current risks, the latest revision of the Common Rule (01/19/2017; https://www.hhs.gov/ohrp/regulations-and-policy/regulations/finalized-revisions-common-rule) excludes proposed restrictions on the sharing of genomics data.  #@NEW_LINE#@#  Here, we show that phenotypic prediction from WGS data can enable reidentification without any further information being shared.  #@NEW_LINE#@#  If conducted for unethical purposes, this approach could compromise the privacy of individuals who contributed their genomes into a database.  #@NEW_LINE#@#  In stratified analyses, we see that risk of reidentification correlates with variability of the cohort.  #@NEW_LINE#@#  Although sharing of genomic data is invaluable for research, our results suggest that genomes cannot be considered fully deidentifiable and should be shared by using appropriate levels of security and due diligence.  #@NEW_LINE#@#  
Our results may also be discussed in the context of genomic forensic sciences.  #@NEW_LINE#@#  Forensic applications include postmortem identification (47) and the association and identification of DNA from biological evidence (15, 48) for intelligence and law enforcement agencies.  #@NEW_LINE#@#  In the United States, an average of 35% of homicides remain unsolved (49).  #@NEW_LINE#@#  For crimes such as these, DNA evidence (e.g., a spot of blood at a crime scene) may be available (50).  #@NEW_LINE#@#  In many cases, the perpetrators DNA is not included in a database such as the Combined DNA Index System (51).  #@NEW_LINE#@#  As the field of genomics matures, forensics may adopt approaches similar to this work to complement other types of evidence.  #@NEW_LINE#@#  Matching DNA evidence to a more commonly available phenotypic set, such as facial images and basic demographic information, would serve to aid cases where conventional DNA testing, database search, and familial testing (52) fails.  #@NEW_LINE#@#  Today, forensic genomics relies heavily on PCR analysesin particular, the study of short tandem repeats and characterization of the Y chromosome and mitochondrial DNA haplotypes.  #@NEW_LINE#@#  The current WGS workflow requires 100ng of DNA.  #@NEW_LINE#@#  However, materials for forensic analyses may be extremely limited, thus confining a broader application of WGS.  #@NEW_LINE#@#  In these cases, the protocol would need additional cycles of amplification or even whole-genome amplification to achieve sufficient DNA for analysis.  #@NEW_LINE#@#  In addition, the forensics field is subject to regulations that differ between states and countries.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
We use the following two-step approach to measure similarity between a deidentified genome gG and a set of identified phenotypic measurements derived from an image and demographic information pP (Fig 6) (see SI Appendix for details).  #@NEW_LINE#@#  First, we find a mapping of phenotypes, P:PEP, and a mapping of genomes, P:GEP, into a common D-dimensional embedding-space EPRD.  #@NEW_LINE#@#  As mappings, we use a combination of PC analysis and predictive modeling.  #@NEW_LINE#@#  Second, we learn an optimal similarity P:EPÃ—EP that allows comparison of mapped phenotypes P(p) and genomes P(g).  #@NEW_LINE#@#  
Learning_Embeddings  #@NEW_LINE#@#  
For any given phenotype, we have defined suitable embeddings.  #@NEW_LINE#@#  Phenotypes that are a single number, such as height, weight, or age, are simply represented by their phenotype value.  #@NEW_LINE#@#  For high-dimensional phenotypes, such as images or voice samples, we have defined embeddings to capture a maximum amount of information relevant for matching.  #@NEW_LINE#@#  For example, facial images provide information on the shape and the color of the face.  #@NEW_LINE#@#  Additionally, a facial image may provide information about sex, ancestry, and the age of the person.  #@NEW_LINE#@#  Consequently, we embedded images into a set of PC dimensions that capture shape and color information, and additional dimensions for sex, ancestry, and age.  #@NEW_LINE#@#  Having defined an embedding, we learned P:PEP and P:GEP to map phenotypes and genomes into this embedding.  #@NEW_LINE#@#  In the case of facial images, P is given by face shape and color PC projection of the image and regression models that had been trained to predict sex, age, and ancestry from the image.  #@NEW_LINE#@#  P is given by extracting sex and ancestry from the genome, as well as regression models for facial PCs and age.  #@NEW_LINE#@#  For a list of the embeddings used for different phenotypes, see SI Appendix, Table S14.  #@NEW_LINE#@#  

Learning_a_Similarity_Function  #@NEW_LINE#@#  
Having obtained the embedding functions, we learn an optimal similarity, P, that takes embedded phenotype P(p) and genotype P(g) and outputs a similarity.  #@NEW_LINE#@#  As a naive similarity Pcosine, we took the cosine between the vector valued P(p) and P(g).  #@NEW_LINE#@#  However, because not all dimensions of EP can be expected to yield equal amounts of information for judging similarity between phenotypes and genomes, we learned optimally weighted similarity functions P to improve reidentification.P(P(p),P(g))=d=1Dwd|P(p)dP(g)d|,[1]where the weights wd, which reflect the importance of d-th dimension of EP, have been trained using a maximum entropy model (53).  #@NEW_LINE#@#  


Footnotes  #@NEW_LINE#@#  



