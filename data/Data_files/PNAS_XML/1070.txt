article id="http://dx.doi.org/10.1073/pnas.1714936115"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Active learning machine learns to create new quantum experiments  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Quantum experiments push the envelope of our understanding of fundamental concepts in quantum physics.  #@NEW_LINE#@#  Modern experiments have exhaustively probed the basic notions of quantum theory.  #@NEW_LINE#@#  Arguably, further breakthroughs require the tackling of complex quantum phenomena and consequently require complex experiments and involved techniques.  #@NEW_LINE#@#  The designing of such complex experiments is difficult and often clashes with human intuition.  #@NEW_LINE#@#  We present an autonomous learning model which learns to design such complex experiments, without relying on previous knowledge or often flawed intuition.  #@NEW_LINE#@#  Our system not only learns how to design desired experiments more efficiently than the best previous approaches, but in the process also discovers nontrivial experimental techniques.  #@NEW_LINE#@#  Our work demonstrates that learning machines can offer dramatic advances in how experiments are generated.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
How useful can machine learning be in a quantum laboratory?  #@NEW_LINE#@#  Here we raise the question of the potential of intelligent machines in the context of scientific research.  #@NEW_LINE#@#  A major motivation for the present work is the unknown reachability of various entanglement classes in quantum experiments.  #@NEW_LINE#@#  We investigate this question by using the projective simulation model, a physics-oriented approach to artificial intelligence.  #@NEW_LINE#@#  In our approach, the projective simulation system is challenged to design complex photonic quantum experiments that produce high-dimensional entangled multiphoton states, which are of high interest in modern quantum experiments.  #@NEW_LINE#@#  The artificial intelligence system learns to create a variety of entangled states and improves the efficiency of their realization.  #@NEW_LINE#@#  In the process, the system autonomously (re)discovers experimental techniques which are only now becoming standard in modern quantum optical experimentsa trait which was not explicitly demanded from the system but emerged through the process of learning.  #@NEW_LINE#@#  Such features highlight the possibility that machines could have a significantly more creative role in future research.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Our learning setup can be put in terms of a scheme visualized in Fig 1A.  #@NEW_LINE#@#  The agent (our learning algorithm) interacts with a virtual environmentthe simulation of an optical table.  #@NEW_LINE#@#  The agent has access to a set of optical elements (its toolbox), with which it generates experiments: It sequentially places the chosen elements on the (simulated) table; following the placement of an element, the quantum state generated by the corresponding setup, i.e., configuration of optical elements, is analyzed.  #@NEW_LINE#@#  Depending on the state, and the chosen task, the agent either receives a reward or not.  #@NEW_LINE#@#  The agent then observes the current table configuration and places another element; thereafter, the procedure is iterated.  #@NEW_LINE#@#  We are interested in finite experiments, so the maximal number of optical elements in one experiment is limited.  #@NEW_LINE#@#  This goes along with practical restrictions: Due to accumulation of imperfections, e.g., through misalignment and interferometric instability, for longer experiments we expect a decreasing overall fidelity of the resulting state or signal.  #@NEW_LINE#@#  The experiment ends (successfully) when the reward is given or (unsuccessfully) when the maximal number of elements on the table is reached without obtaining the reward.  #@NEW_LINE#@#  Over time, the agent learns which element to place next, given a table.  #@NEW_LINE#@#  
Initially, we fix the analyzer in Fig 1A to reward the successful generation of a state from a certain class of high-dimensional multipartite entangled states.  #@NEW_LINE#@#  Conceptually, this choice of the reward function explicitly specifies our criterion for an experimental setup (and the states that are created) to be interesting, but we will come back to a more general scenario momentarily.  #@NEW_LINE#@#  We characterize interesting states by a SchmidtRank vector (SRV) (21, 22)the numerical vector containing the rank of the reduced-density matrix of each of the subsystemstogether with the requirement that the states are maximally entangled in the OAM basis (12, 13).  #@NEW_LINE#@#  
The initial state of the simulated experiment is generated by a double spontaneous parametric down-conversion (SPDC) process in two nonlinear crystals.  #@NEW_LINE#@#  Similar to refs.  #@NEW_LINE#@#  12 and 13, we ignore higher-order terms with OAM |m|1 from down-conversion as their amplitudes are significantly smaller than those of the low-order terms.  #@NEW_LINE#@#  Neglecting these higher-order terms in the down-conversion, the initial state |(0) can be written as a tensor product of two pairs of OAM-entangled photons (13),|(0)=13(m=11|ma|mb)(m=11|mc|md),[1]where the indexes a,b,c, and d specify four arms in the optical setup.  #@NEW_LINE#@#  The toolbox contains a basic set of elements (12) including beam splitters (BS), mirrors (Refl), shift-parameterized holograms (Holo), and Dove prisms (DP).  #@NEW_LINE#@#  Taking into account that each distinct element can be placed in any one (or two in the case of BS) of the four arms a,b,c,d, we allow in total 30 different choices of elements.  #@NEW_LINE#@#  Since none of the optical elements in our toolbox creates entanglement in OAM degrees of freedom, we use a measurement in arm a and postselection to trigger a tripartite state in the other three arms.  #@NEW_LINE#@#  
Our learning agent is based on the PS (19) model for AI.  #@NEW_LINE#@#  PS is a physics-motivated framework which can be used to construct RL agents.  #@NEW_LINE#@#  PS was shown to perform well in standard RL problems (4043) and in advanced robotics applications (44), and it is also amenable for quantum enhancements (4547).  #@NEW_LINE#@#  The main component of the PS agent is its memory network (shown in Fig 1B) comprising units of episodic memory called clips.  #@NEW_LINE#@#  Here, clips include remembered percepts (in our case, the observed optical tables) and actions (corresponding to the placing of an optical element).  #@NEW_LINE#@#  Each percept (clip) si, i[1,,N] is connected to every action (clip) aj, j[1,,30] via a weighted directed edge (i,j), which represents the possibility of taking an action aj in a situation si with probability pi,j (Fig 1B).  #@NEW_LINE#@#  The process of learning is manifested in the creation of new clips and in the adjustment of the probabilities.  #@NEW_LINE#@#  Intuitively, the probabilities of perceptaction transitions which eventually lead to a reward will be enhanced, leading to a higher likelihood of rewarding behavior in the future (see Projective Simulation for details).  #@NEW_LINE#@#  
Designing_Short_Experiments  #@NEW_LINE#@#  
As mentioned, a maximum number of elements placed in one experiment is introduced to limit the influence of experimental imperfections.  #@NEW_LINE#@#  For the same reason, it is valuable to identify the shortest experiments that produce high-dimensional tripartite entanglement characterized by a desired SRV.  #@NEW_LINE#@#  To relate our work to existing experiments (13), we task the agent with designing a setup that creates a state with SRV (3,3,2).  #@NEW_LINE#@#  To further emphasize the benefits of learning, we then investigate whether the agent can use its knowledge, attained through the learning setting described previously, to discover a more complex quantum state.  #@NEW_LINE#@#  Thus, the task of finding a state with SRV (3,3,2) is followed by the task of finding a state with SRV (3,3,3), after 5×104 simulated experiments.  #@NEW_LINE#@#  As always, a reward is issued whenever a target state is found, and the table is reset.  #@NEW_LINE#@#  Fig 2A shows the success probability throughout the learning process: The PS agent first learns to construct a (3,3,2) state and then, with probability 0.5, very quickly [compared to the by itself simpler task of finding a (3, 3, 2) state] learns to design a setup that corresponds to a (3,3,3) state.  #@NEW_LINE#@#  Our results suggest that either the knowledge of constructing a (3,3,2) state is highly beneficial in the second phase or the more complicated (3,3,3) state is easier to generate.  #@NEW_LINE#@#  To resolve this dichotomy, we simulated a learning process where the PS agent is required to construct a (3,3,3) state within 6×104 experiments, without having learned to build a (3,3,2) state during the first 5×104 experiments.  #@NEW_LINE#@#  The results of the simulations are shown in Fig 2A as dashed lines (lower and upper edges of the frame).  #@NEW_LINE#@#  It is apparent that the agent without previous training on (3,3,2) states does not show any significant progress in constructing a (3,3,3) experiment.  #@NEW_LINE#@#  Furthermore, the PS agent constantly and autonomously improves by constructing shorter and shorter experiments (Fig S1A).  #@NEW_LINE#@#  By the end of the first phase, PS almost always constructs experiments of length 4the shortest length of an experiment producing a state from the (3,3,2) SRV class.  #@NEW_LINE#@#  During the (3,3,3) learning phase, the PS agent produces a (3,3,3) state of the shortest length in half of the cases.  #@NEW_LINE#@#  Experimental setups useful for the second phase are almost exclusively so-called parity sorters (Fig 3).  #@NEW_LINE#@#  Other setups that are not equivalent to parity sorters seem not to be beneficial in finding a (3,3,3) state.  #@NEW_LINE#@#  As we show later, the PS agent tends to use parity sorters more frequently while exploring the space of different SRV classes.  #@NEW_LINE#@#  This is particularly surprising since the parity sorter itself was originally designed for a different task.  #@NEW_LINE#@#  

Designing_New_Experiments  #@NEW_LINE#@#  
The connection between high-dimensional entangled states and the structure of optical tables which generate them is not well understood (12).  #@NEW_LINE#@#  Having a database of such experiments would allow us to deepen our understanding of the structure of the set of entangled states that can be accessed by optical tables.  #@NEW_LINE#@#  In particular, such a database could then be further analyzed to identify useful subsetups or gadgets (12)certain few-element combinations that appear frequently in larger setupswhich are useful for generating complex states.  #@NEW_LINE#@#  With our second task, we have challenged the agent to generate such a database by finding high-dimensional entangled states.  #@NEW_LINE#@#  As a bonus, we found that the agent does the postprocessing above for us implicitly and in runtime.  #@NEW_LINE#@#  The outcome of this postprocessing is encoded in the structure of the memory network of the PS agent.  #@NEW_LINE#@#  Specifically, the subsetups which were particularly useful in solving this task are clearly visible, or embodied, in the agents memory.  #@NEW_LINE#@#  
To find as many different high-dimensional three-photon entangled states as possible, we reward the agent for every new implementation of an interesting experiment.  #@NEW_LINE#@#  To avoid trivial extensions of such implementations, a reward is given only if the obtained SRV was not reached before within the same experiment.  #@NEW_LINE#@#  Fig 2B displays the total number of new, interesting experiments designed by the basic PS agent (solid blue curve) and the PS agent with action composition (19) (dashed blue curve).  #@NEW_LINE#@#  Action composition allows the agent to construct new composite actions from useful optical setups (i.e., placing multiple elements in a fixed configuration), thereby autonomously enhancing the toolbox (see Projective Simulation for details).  #@NEW_LINE#@#  It is a central ingredient for an AI to exhibit even a primitive notion of creativity (50) and was also used in ref.  #@NEW_LINE#@#  12 to augment automated random search.  #@NEW_LINE#@#  For comparison, we provide the total number of interesting experiments obtained by automated random search with and without action composition (Fig 2B, solid and dashed red curves).  #@NEW_LINE#@#  As we will see later, action composition will allow for additional insight into the agents behavior and helps provide useful information about quantum optical setups in general.  #@NEW_LINE#@#  We found that the PS model discovers significantly more interesting experiments than both automated random search and automated random search with action composition (Fig 2B).  #@NEW_LINE#@#  

Ingredients_for_Successful_Learning  #@NEW_LINE#@#  
In general, successful learning relies on a structure hidden in the task environment (or dataset).  #@NEW_LINE#@#  The results presented thus far show that PS is highly successful in the task of designing new interesting experiments, and here we elucidate why this should be the case.  #@NEW_LINE#@#  The following analysis also sheds light on other settings where we can be confident that RL techniques can be applied as well.  #@NEW_LINE#@#  
First, the space of optical setups can be illustrated using a graph as given in Fig 4C, where the building of an optical experiment corresponds to a walk on the directed graph.  #@NEW_LINE#@#  Note that optical setups that create a certain state are not unique: Two or more different setups can generate the same quantum state.  #@NEW_LINE#@#  Due to this fact, this graph does not have a tree structure but rather resembles a maze.  #@NEW_LINE#@#  Navigating in a maze, in turn, constitutes one of the classic textbook RL problems (10, 41, 48, 49).  #@NEW_LINE#@#  Second, our empirical analysis suggests that experiments generating high-dimensional multipartite entanglement tend to have some structural similarities (12) (Fig 4 A and B partially displays the exploration space).  #@NEW_LINE#@#  Fig 4 shows regions where the density of interesting experiments (large colored nodes) is high and others where it is lowinteresting experiments seem to be clustered (Fig S2).  #@NEW_LINE#@#  In turn, RL is particularly useful when one needs to handle situations which are similar to those previously encounteredonce one maze (optical experiment) is learned, similar mazes (experiments) are tackled more easily, as we have seen before.  #@NEW_LINE#@#  In other words, whenever the experimental task has a maze-type underlying structure, which is often the case, PS can likely helpand critically, without having any a priori information about the structure itself (41, 51).  #@NEW_LINE#@#  In fact, PS gathers information about the underlying structure throughout the learning process.  #@NEW_LINE#@#  This information can then be extracted by an external user or potentially be used further by the agent itself.  #@NEW_LINE#@#  

The_Potential_of_Learning_from_Experiments  #@NEW_LINE#@#  
Thus far, we have established that a machine can indeed design new quantum experiments in the setting where the task is precisely specified (via the rewarding rule).  #@NEW_LINE#@#  Intuitively, this could be considered the limit of what a machine can do for us, as machines are specified by our programs.  #@NEW_LINE#@#  However, this falls short from what, for instance, a human researcher can achieve.  #@NEW_LINE#@#  How could we, even in principle, design a machine to do something (interesting) we have not specified it to do?  #@NEW_LINE#@#  To develop an intuition for the type of behavior we could hope for, consider, for the moment, what we may expect a human, say a good PhD student, would do in situations similar to those studied thus far.  #@NEW_LINE#@#  
To begin with, a human cannot go through all conceivable optical setups to find those that are interesting.  #@NEW_LINE#@#  Arguably, she would try to identify prominent subsetups and techniques that are helpful in the solving of the problem.  #@NEW_LINE#@#  Furthermore, she would learn that such techniques are probably useful beyond the specified tasks and may provide new insights in other contexts.  #@NEW_LINE#@#  Could a machine, even in principle, have such insight?  #@NEW_LINE#@#  Arguably, traces of this can be found already in our, comparatively simple, learning agent.  #@NEW_LINE#@#  By analyzing the memory network of the agent (ranking clips according to the sum of the weights of their incident edges), specifically the composite actions it learned to generate, we can extract subsetups that have been particularly useful in the endeavor of finding many different interesting experiments in Fig 2B for L=6.  #@NEW_LINE#@#  
For example, PS composes and then extensively uses a combination corresponding to an optical interferometer as displayed in Fig 3A which is usually used to sort OAM modes with different parities (52)in essence, the agent (re)discovered a parity sorter.  #@NEW_LINE#@#  This interferometer has already been identified as an essential part of many quantum experiments that create high-dimensional multipartite entanglement (12), especially those involving more than two photons (Fig 2A and ref.  #@NEW_LINE#@#  13).  #@NEW_LINE#@#  
One of the PS agents assignments was to discover as many different interesting experiments as possible.  #@NEW_LINE#@#  In the process of learning, even if a parity sorter was often (implicitly) rewarded, over time it will no longer be, as in this scenario only novel experiments are rewarded.  #@NEW_LINE#@#  This, again implicitly, drives the agent to invent new, different-looking configurations, which are similarly useful.  #@NEW_LINE#@#  
Indeed, in many instances the most rewarded action is no longer the original parity sorter in the form of a MachZehnder interferometer (Fig 3A) but a nonlocal version thereof (Fig 3B).  #@NEW_LINE#@#  As it turns out, the two setups are equivalent in the Klyshko wave front picture (53, 54), where the time of two photons in arms b and c in Fig 3B is inverted, and these photons are considered to be reflected at their origin (represented by Reflx)the nonlinear crystals.  #@NEW_LINE#@#  This transformation results in a reduction of the four-photon experiment in Fig 3B to the two-photon experiment shown in Fig 3C.  #@NEW_LINE#@#  
Such a nonlocal interferometer has only recently been analyzed and motivated the work in ref.  #@NEW_LINE#@#  17.  #@NEW_LINE#@#  Furthermore, by slightly changing the analyzer to reward only interesting experiments that produce states with SRV other than the most common SRV (3,3,2), subsetups aside from the OAM parity sorter become apparent.  #@NEW_LINE#@#  For example, the PS discovered and exploited a technique to increase the dimensionality of the OAM of the photons by shifting the OAM mode number in one path and subsequently mixing it with an additional path as displayed in Fig 3D.  #@NEW_LINE#@#  Moreover, one can observe that this technique is frequently combined with a local OAM parity sorter.  #@NEW_LINE#@#  This setup allows the creation of high-dimensional entangled states beyond the initial state dimension of 3.  #@NEW_LINE#@#  
All of the observed setups are, in fact, modern quantum optical gadgets/devices (designed by humans) that either already found applications in state-of-the-art quantum experiments (13) or could be used (as individual tools) in future experiments which create high-dimensional entanglement starting from lower-dimensional entanglement.  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
One of the fastest-growing trends in recent times is the development of smart technologies.  #@NEW_LINE#@#  Such technologies are not only permeating our everyday lives in the forms of smart phones, smart watches, and in some places even smart self-driving cars, but are expected to induce the next industrial revolution (55).  #@NEW_LINE#@#  Hence, it should not come as a surprise when smart laboratories emerge.  #@NEW_LINE#@#  Already now, modern laboratories are to a large extent automated (56) and are removing the need for human involvement in tedious (or hazardous) tasks.  #@NEW_LINE#@#  
In this work we broach the question of the potential of automated laboratories, trying to understand to what extent machines could not only help in research, but perhaps also even genuinely perform it.  #@NEW_LINE#@#  Our approach highlights two aspects of learning machines, both of which will be assets in quantum experiments of the future.  #@NEW_LINE#@#  First, we have improved upon the original observation that search algorithms can aid in the context of finding special optical setups (12) by using more sophisticated learning agents.  #@NEW_LINE#@#  This yields confidence that even more involved techniques from AI research [e.g., generalization (43), meta-learning (51), etc., in the context of the PS framework] may yield ever-improving methods for the autonomous design of experiments.  #@NEW_LINE#@#  In a complementary direction, we have shown that the structure of learning models commonly applied in the context of AI research [even the modest basic PS reinforcement learning machinery augmented with action-clip composition (19, 40)] possibly allows machines to tackle problems they were not directly instructed or trained to solve.  #@NEW_LINE#@#  This supports the expectation that AI methodologies will genuinely contribute to research and, very optimistically, the expectation that they will contribute to the discovery of new physics.  #@NEW_LINE#@#  

Acknowledgments  #@NEW_LINE#@#  
A.A.M., H.P.N., V.D., M.T., and H.J.B.  #@NEW_LINE#@#  were supported by the Austrian Science Fund (FWF) through Grants SFB FoQuS F4012 and DK-ALM: W1259-N27, by the Templeton World Charity Foundation through Grant TWCF0078/AB46, and by the Ministerium für Wissenschaft, Forschung, und Kunst Baden-Württemberg (AZ: 33-7533.-30-10/41/1).  #@NEW_LINE#@#  M.K.  #@NEW_LINE#@#  and A.Z.  #@NEW_LINE#@#  were supported by the Austrian Academy of Sciences (ÖAW), by the European Research Council (Simulators and Interfaces with Quantum Systems Grant 600645 EU-FP7-ICT), and the Austrian Science Fund (FWF) with SFB FoQuS F40 and FWF project CoQuS W1210-N16.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  

Published under the PNAS license.  #@NEW_LINE#@#  

