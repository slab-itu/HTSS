article id="http://dx.doi.org/10.1073/pnas.1612862114"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Development of visual category selectivity in ventral visual cortex does not require visual experience  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
The brains ability to recognize visual categories is guided by category-selective ventral-temporal cortex (VTC).  #@NEW_LINE#@#  Whether visual experience is required for the functional organization of VTC into distinct functional subregions remains unknown, hampering our understanding of the mechanisms that drive category recognition.  #@NEW_LINE#@#  Here, we demonstrate that VTC in individuals who were blind since birth shows robust discriminatory responses to natural sounds representing different categories (faces, scenes, body parts, and objects).  #@NEW_LINE#@#  These activity patterns in the blind also could predict successfully which category was visually perceived by controls.  #@NEW_LINE#@#  The functional cortical layout in blind individuals showed remarkable similarity to the well-documented layout observed in sighted controls, suggesting that visual functional brain organization does not rely on visual input.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
To what extent does functional brain organization rely on sensory input?  #@NEW_LINE#@#  Here, we show that for the penultimate visual-processing region, ventral-temporal cortex (VTC), visual experience is not the origin of its fundamental organizational property, category selectivity.  #@NEW_LINE#@#  In the fMRI study reported here, we presented 14 congenitally blind participants with face-, body-, scene-, and object-related natural sounds and presented 20 healthy controls with both auditory and visual stimuli from these categories.  #@NEW_LINE#@#  Using macroanatomical alignment, response mapping, and surface-based multivoxel pattern analysis, we demonstrated that VTC in blind individuals shows robust discriminatory responses elicited by the four categories and that these patterns of activity in blind subjects could successfully predict the visual categories in sighted controls.  #@NEW_LINE#@#  These findings were confirmed in a subset of blind participants born without eyes and thus deprived from all light perception since conception.  #@NEW_LINE#@#  The sounds also could be decoded in primary visual and primary auditory cortex, but these regions did not sustain generalization across modalities.  #@NEW_LINE#@#  Surprisingly, although not as strong as visual responses, selectivity for auditory stimulation in visual cortex was stronger in blind individuals than in controls.  #@NEW_LINE#@#  The opposite was observed in primary auditory cortex.  #@NEW_LINE#@#  Overall, we demonstrated a striking similarity in the cortical response layout of VTC in blind individuals and sighted controls, demonstrating that the overall category-selective map in extrastriate cortex develops independently from visual experience.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Univariate_Analysis__Contrasts_and_Selectivity  #@NEW_LINE#@#  
We first tested the fMRI contrasts between each category individually against the average of the other three categories across subjects.  #@NEW_LINE#@#  The results are mapped on the average cortex reconstruction in Fig 1, Left for auditory and visual stimuli in the sighted controls and auditory stimuli in blind participants separately.  #@NEW_LINE#@#  The visually presented trials resulted in solid responses in VTC.  #@NEW_LINE#@#  On the left and right hemispheres, face-selective vertices are consistently located laterally to the midfusiform sulcus (MFS) (27).  #@NEW_LINE#@#  More medially, scene-selective vertices are found surrounding the collateral sulcus.  #@NEW_LINE#@#  The scene- and face-selective patches border an object-selective patch lying in the MFS.  #@NEW_LINE#@#  On the left hemisphere another object-sensitive region was found on the occipitotemporal sulcus bordering the face-sensitive vertices, whereas the right hemisphere shows a body region ventrally to the face-selective patch.  #@NEW_LINE#@#  Compared with the visual trials, the auditory trials in the controls yielded little to no significant vertices that survived the statistical threshold.  #@NEW_LINE#@#  However, the blind participants showed a solid response to face-related sounds on the fusiform gyrus bilaterally.  #@NEW_LINE#@#  In addition, a few object-sensitive clusters appear, most prominently in the left occipitotemporal sulcus.  #@NEW_LINE#@#  
The selectivity maps that are depicted in Fig 1, Right show the unthresholded preferred stimulus condition after normalization for global response amplitude.  #@NEW_LINE#@#  In the visual modality, the selectivity maps closely resemble the statistical contrast maps.  #@NEW_LINE#@#  However, the auditory trials in the sighted controls yield a less clear outline of functional boundaries.  #@NEW_LINE#@#  The correlation between the auditory and visual maps was found to be reasonable (r = 0.1775, P less_than 0.001), probably in part because of the face- and object-selective patches in the left hemisphere.  #@NEW_LINE#@#  Interestingly, the auditory selectivity maps of the blind individuals show a clear resemblance to the visual maps in controls (thresholded as well as unthresholded).  #@NEW_LINE#@#  Most pronounced are the face- and scene-selective patches bordering the MFS and the object-selective region near the occipitotemporal sulcus, especially in the left hemisphere.  #@NEW_LINE#@#  The correlation between the blind map and the visual control map is substantial (r = 0.3415, P less_than 0.001).  #@NEW_LINE#@#  We also computed the correlation between the visual control map and the blind map when the latter is based on only the first four runs, resulting in a correlation of r = 0.2671, P less_than 0.001.  #@NEW_LINE#@#  

Classification_Results__VTC  #@NEW_LINE#@#  
The average accuracies of MVPA predictions across subjects that resulted from the classification procedure in VTC are shown in Fig 2.  #@NEW_LINE#@#  As can be seen, all category pairs can be decoded from neural responses in VTC significantly better than empirical chance level in blind subjects.  #@NEW_LINE#@#  The classification performances of the auditory and visual trials measured in the sighted controls allow direct comparisons between the two groups of subjects.  #@NEW_LINE#@#  Unsurprisingly, the visual presentation of the categories yielded nearly perfect results in the control group.  #@NEW_LINE#@#  The auditory runs resulted in a lower performance, but the classifier still achieved a significant decoding in all but one (body vs. object) category pair.  #@NEW_LINE#@#  
We also compared the classification performances of the auditory trials in the blind individuals and the controls, averaging across all pairwise comparisons.  #@NEW_LINE#@#  The prediction accuracies in the blind subjects were substantially higher when all eight runs were included in the classification, yielding a significant difference between the blind and control individuals (permutation test; P less_than 0.001).  #@NEW_LINE#@#  When we reduced the data to four runs in both the blind participants and the controls, this difference became smaller but was still significant with a two-tailed test (P less_than 0.05).  #@NEW_LINE#@#  
We isolated the three category 1 (anophthalmic) blind subjects (for categories of blind subjects, see Experimental Procedures) and assessed the accuracy with which the classification algorithm could decode the categories from their neural responses (Fig 3).  #@NEW_LINE#@#  Because these subjects are the only participants in our study whom we are certain never had any visual experience, even during a brief period early in life, these results serve as a benchmark for our full pool of blind subjects.  #@NEW_LINE#@#  Interestingly, the analysis resulted in a robust decoding accuracy in all three anophthalmic subjects.  #@NEW_LINE#@#  

Classification_Results__Primary_Visual_Cortex_and_Primary_Auditory_Cortex  #@NEW_LINE#@#  
Because primary visual regions serve basic visual processing in sighted individuals, we wondered whether primary visual cortex (V1) shows a discriminatory response to the four auditory conditions in the blind group.  #@NEW_LINE#@#  Fig 4 shows the classification results, indicating that V1 indeed discriminates between the face, body, object, and scene categories.  #@NEW_LINE#@#  Comparing these results with those in the sighted controls, we found that the auditory trials in the control group yield no statistically significant decoding in any category pair in V1.  #@NEW_LINE#@#  The direct comparison between the participant groups averaged across categories approaches significance (P = 0.07, two-tailed) when only the first four runs of the blind subjects are considered.  #@NEW_LINE#@#  However, the visual trials in the controls result in a robust prediction in all category pairs, indicating that the four visual conditions most likely differ in low-level visual features.  #@NEW_LINE#@#  
In primary auditory cortex (A1), the auditory conditions could be discriminated in both subject groups.  #@NEW_LINE#@#  Here there was a significantly stronger discrimination in the control group than in the blind group across category pairs (P less_than 0.001).  #@NEW_LINE#@#  In addition, we observed significant decoding of the visual categories in A1 in the control group (P less_than 0.01).  #@NEW_LINE#@#  When looking at the individual category pairs, we find all but one pair (scene vs. object) to be statistically decodable from response patterns in A1.  #@NEW_LINE#@#  
We also examined how the neural responses in VTC of the three anophthalmic blind participants differed across categories.  #@NEW_LINE#@#  Again, we demonstrate that early visual cortex shows a discriminatory response in almost all category pairs in these individuals, see Fig 5.  #@NEW_LINE#@#  

Between-Subject_Classification_Results__VTC  #@NEW_LINE#@#  
Because a successful decoding in VTC of blind participants does not show whether the same neural populations are at work as in sighted controls, we attempted to cross-decode between blind and sighted individuals.  #@NEW_LINE#@#  More specifically, we asked if training a classifier on the auditory trials of the blind individuals would allow generalization to the visual and auditory trials in the sighted controls.  #@NEW_LINE#@#  As can be seen in Fig 6, the visual-response patterns could be predicted successfully from the auditory responses in the blind participants (P less_than 0.01).  #@NEW_LINE#@#  When inspecting the category pairs, we found that all pairs except body vs. object could be decoded using this approach on the visual data.  #@NEW_LINE#@#  When we tried to predict the auditory trials in the control group, we found the same global outcome, albeit with lower prediction accuracies.  #@NEW_LINE#@#  Despite the relatively low performance accuracies, the results still are significantly better than our permutation results.  #@NEW_LINE#@#  
When we invert the generalization direction (Fig 6) to training on the controls and predicting the classes in the blind subjects, we qualitatively find the same results as when we generalized to the controls, albeit with only marginal prediction accuracies.  #@NEW_LINE#@#  The asymmetry in decoding direction is significant (P less_than 0.001).  #@NEW_LINE#@#  This phenomenon of asymmetric generalization has been reported in other studies (2831), and the exact source remains unclear (32).  #@NEW_LINE#@#  One possible interpretation that has been put forward is that it reflects the underlying nature of the representations; for instance, the auditory trials might activate only a subset of voxels that contain categorical information, whereas the visual trials activate the majority of VTC voxels.  #@NEW_LINE#@#  Therefore, training the classifier on the sparse auditory data ensures that the algorithm captures the relevant information that generalizes well to the well-defined visual responses, but not vice versa (33).  #@NEW_LINE#@#  Alternatively, the asymmetry may be based on inherent properties of the data themselves, such as differences in signal-to-noise ratio between the auditory and visual runs.  #@NEW_LINE#@#  Ultimately, the data themselves do not provide definitive insights into the mechanism that underlies asymmetry.  #@NEW_LINE#@#  

Between-Subject_Classification_Results__V1_and_A1  #@NEW_LINE#@#  
To see whether the neural selectivity maps in V1 and A1 observed in the blind group correspond to the maps that process auditory and visual stimuli in the controls, we attempted the cross-subject decoding approach on these brain regions.  #@NEW_LINE#@#  In both generalization directions, however, the classifier failed to decode the categories from the neural responses in V1 and A1 above empirically estimated chance levels (training on blind, testing on controls: average prediction accuracy V1: 51.3 ±: 2.7%, A1: 53.0 ± 3.4%; training on controls, testing on blind: average prediction accuracy V1: 50.9 ± 1.7%, A1: 52.3 ± 2.2%).  #@NEW_LINE#@#  
We tested whether the between-group classification in V1 and A1 is significantly lower than the aforementioned between-group classification in VTC.  #@NEW_LINE#@#  The results of such between-region comparisons in classifier performance should be interpreted with care, but to make the conditions at least a bit comparable, we reduced all regions to a standard size.  #@NEW_LINE#@#  More specifically, we selected the 100 vertices with the best generalized linear model (GLM) fits across subjects for VTC, A1, and V1 and ran the blind-to-sighted (auditory-to-visual) classification analysis on these data points.  #@NEW_LINE#@#  When statistically quantifying the differences in performance across subjects and condition pairs in VTC and the other two regions of interest (ROIs), we find that the between-group classification in VTC is higher than in V1 (P = 0.0017) and A1 (P = 0.0203).  #@NEW_LINE#@#  

Consistency_of_the_Functional_Map_Across_Subjects  #@NEW_LINE#@#  
Fig 7 provides a detailed picture of the overall degree of similarity in the cortical layout of the functional responses in VTC among subjects, between subject groups, and between modalities.  #@NEW_LINE#@#  The figure format also provides a complete view of the interindividual variability.  #@NEW_LINE#@#  As can be seen, the visual maps show a high internal consistency (r = 0.49), whereas the average correlation of the auditory maps in the controls is much lower (r = 0.05).  #@NEW_LINE#@#  Interestingly, from this matrix it becomes clear that the internal consistency of the auditory maps of the blind group is higher than that of the controls (r = 0.15 vs. r = 0.03, P less_than 0.001).  #@NEW_LINE#@#  Moreover, the correlation of the auditory maps of the blind individuals with the visual maps of the controls is significantly stronger than the correlation of the auditory maps of the controls with their visual maps (r = 0.10 vs. r = 0.05, P less_than 0.01).  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
Studying the visual system in blind individuals offers a rare window into the development of functional specificity in the brain.  #@NEW_LINE#@#  In this paper, we investigated the contention that the categorical selectivity and functional organization of VTC can develop independently of visual experience.  #@NEW_LINE#@#  We tested two hypotheses: (i) that face-, body-, object-, and scene-related natural sounds elicit distinct responses in VTC of congenitally blind individuals, and (ii) that the functional architecture of VTC in these blind participants shows significant overlap with the layout of this brain region in sighted controls.  #@NEW_LINE#@#  We found that VTC in the blind is indeed sensitive to all four categories, showing distinct responses that were decodable within and to a large extent across subjects.  #@NEW_LINE#@#  However, it is important to note that the category selectivity in the blind subjects for the auditory stimuli is not as strong as the selectivity in the controls for visual stimuli.  #@NEW_LINE#@#  Still, the auditory maps in VTC are stronger in the blind group than in controls.  #@NEW_LINE#@#  Finally, we demonstrated a striking similarity in the cortical response layout of VTC in blind individuals and sighted controls, demonstrating that categorical sensitivity in the extrastriate cortex does not require visual input.  #@NEW_LINE#@#  
Layout_of_VTC_in_Blind_Individuals_Corresponds_to_the_Layout_in_Sighted_Controls  #@NEW_LINE#@#  
Through statistical contrasts and the spatial mapping of selectivity, we have demonstrated that the global functional layout of VTC is remarkably similar to the well-known functional architecture described in the literature (34, 35).  #@NEW_LINE#@#  Directly correlating the functional map of the blind individuals with the visual responses in the sighted controls yields a substantial similarity.  #@NEW_LINE#@#  

VTC_in_Blind_Persons_Functionally_Discriminates_Among_Categories  #@NEW_LINE#@#  
Using MVPA, we demonstrated that VTC yields robust and distinct responses to the four categories used in this study.  #@NEW_LINE#@#  Moreover, we were able to predict visual responses in sighted control subjects by training a classifier on the neural responses in the blind individuals, revealing an important functional overlap between the cortical layouts in the two groups.  #@NEW_LINE#@#  These findings, in combination with the functional topography presented in the congenitally blind subjects, provide reliable evidence that the category-selective ventral cortex does not require visual experience to develop its functional architecture.  #@NEW_LINE#@#  This notion is strengthened by strong discriminatory response in VTC to the four categories shown by even our anophthalmic participants.  #@NEW_LINE#@#  This result counters the alternative explanation that a brief period of low-quality visual experience in the early youth of category 2 and 3 blind participants might have functionally shaped their visual brain regions.  #@NEW_LINE#@#  
A partially intact functional architecture of VTC in individuals without visual experience can have two main interpretations.  #@NEW_LINE#@#  The first is that the categorical selectivity is innate; this proposal would be at odds with the hypothesis that a retinotopic protomap becomes fine-tuned over time via correlations between retinotopic and categorical input (911).  #@NEW_LINE#@#  In this scenario, the eccentricity bias of higher-order visual regions is a purely hard-wired principle, strictly following the retinotopic organization of the earlier stages of visual processing.  #@NEW_LINE#@#  In this case, the functional specificity of VTC is merely an extension of this principle, which would make testable predictions about the computational mechanisms that underlie visual categorical processing (36, 37).  #@NEW_LINE#@#  Alternatively, the development of categorical selectivity in VTC could be driven in part by multimodal or supramodal principles (38).  #@NEW_LINE#@#  In this view, a particular categorical sensitivity is hard-wired in the extrastriate cortex from birth.  #@NEW_LINE#@#  Through early development and exploration of the outer world, this part of the brain is fine-tuned via sensory input.  #@NEW_LINE#@#  Visual input is the primary modality for shaping category sensitivity, but when this sensory input is lacking from birth, other modalities can take over.  #@NEW_LINE#@#  This interpretation in terms of intermodality interactions might also extend to our finding that our control subjects show a less pronounced functional topography in the auditory modality than do blind individuals.  #@NEW_LINE#@#  If the organization of VTC were fully innate and multimodal from birth, one could reasonably expect a solid response to auditory stimulation in sighted individuals as well.  #@NEW_LINE#@#  
Indeed, our findings across all tests suggest stronger, not weaker, responses in VTC to auditory stimuli in blind than in sighted participants: stronger functional selectivity in the analyses shown in Fig 1, a tendency for stronger decoding in the analyses of Fig 2, and stronger map consistency in the matrices of Fig 7.  #@NEW_LINE#@#  This strong auditory activation in VTC of blind participants is all the more striking because visual imagery, which is an important possible explanation for auditory activation in visual cortex in sighted individuals (39), cannot explain the auditory activation in congenitally blind participants.  #@NEW_LINE#@#  
Even though the category-selective maps uncovered in the blind through auditory stimuli tended to be stronger in blind than in sighted individuals, these auditory maps are still much weaker than the category-selective maps found with visual stimulation.  #@NEW_LINE#@#  Thus, when we conclude that the category-selective functional organization in VTC develops independently from visual experience, we refer solely to the basic map that is similar across modalities and is present in the blind and to the auditory activation of this map that is even stronger in the blind.  #@NEW_LINE#@#  Still, it is striking that the activity pattern elicited by an object-related sound in visual cortex of a congenitally blind individual nicely predicts the activity pattern elicited by a visual movie clip of that same object in a sighted participant.  #@NEW_LINE#@#  

What_Does_VTC_Represent_in_Blind_Individuals?  #@NEW_LINE#@#  
The activation of categorical representations in VTC of blind individuals raises some fundamental questions.  #@NEW_LINE#@#  First, what neural pathways mediate the categorical representations activated in VTC through auditory stimulation?  #@NEW_LINE#@#  Earlier work has demonstrated that nonvisual sensory input can be conveyed into the visual cortex in both blind and sighted (albeit to a smaller extent) individuals (38, 40).  #@NEW_LINE#@#  These nonvisual signals might reach visual cortex through thalamocortical projections or corticocortical connections, transferring information directly from primary sensory areas or indirectly from higher-order (multisensory) areas (e.g., the superior temporal sulcus).  #@NEW_LINE#@#  Moreover, in blind individuals these existing pathways will be altered through cross-modal plasticity via rewiring and/or unmasking or strengthening of existing connections.  #@NEW_LINE#@#  It has been shown that the process of synaptic pruning, which typically removes about 40% of the synaptic connections of visual cortex in early life (41), is limited in the case of early sensory deprivation.  #@NEW_LINE#@#  A decreased level of pruning might, at least in part, underlie the auditory-to-visual cross-modal plasticity in the early blind (4245): the absence of visual input during early development allows the synaptic connections to strengthen in blind individuals (46).  #@NEW_LINE#@#  This cross-modal plasticity predicts a stronger response in visual cortex of blind persons when auditory stimuli are processed and perhaps could explain why we observed a stronger selectivity for auditory stimuli in VTC of the congenitally blind participants.  #@NEW_LINE#@#  However, in ventral visual cortex we see not only a stronger overall auditory response but also a regional distribution of category preference that mimics the preference in the sighted brain.  #@NEW_LINE#@#  To explain this distribution as the result of pruning, we need to assume that the synapses that run from the auditory stream to visual cortex early in development already have a rudimentary category preference.  #@NEW_LINE#@#  Recent findings regarding the visual word form area illustrate the importance of early connectivity patterns for the functional development of visual cortex.  #@NEW_LINE#@#  In a longitudinal study, structural connectivity patterns (but not functional responses) measured in children at a prereading age could predict the cortical location of the visual word form area measured 3 y later when children had learned to read (47).  #@NEW_LINE#@#  
Another fundamental question regarding categorical representations in VTC concerns the information that is represented: What does VTC of the blind person encode when it is presented with auditory stimuli, and why is the selectivity of VTC responses stronger in congenitally blind individuals?  #@NEW_LINE#@#  Although our data allow cross-decoding between the blind and sighted participants, this cross-decoding does not give direct insights into the information that is represented in VTC in blind individuals.  #@NEW_LINE#@#  Several non-mutually exclusive interpretations are possible.  #@NEW_LINE#@#  For instance, VTC could be encoding the auditory properties of the sound stimuli.  #@NEW_LINE#@#  Alternatively, the sounds trigger might more abstract category-related representations such as form, semantics, or perhaps even linguistics.  #@NEW_LINE#@#  We could argue that the cross-decoding with the sighted individuals makes some of these scenarios implausible, because there is no evidence for them in the literature on sighted individuals (e.g., for the representation of auditory dimensions).  #@NEW_LINE#@#  However, it is possible that VTC in blind individuals encodes one of these dimensions but VTC in sighted controls does not.  #@NEW_LINE#@#  Thus, the informational content could be very different in the two subject groups.  #@NEW_LINE#@#  We know only that the content gives rise to the same preferences at the category level and that even such a mere overlap in functional topography between the two groups leads to the significant cross-decoding.  #@NEW_LINE#@#  Note that such a situation with multiple correlated biases also seems to be a general property of visual category representations in sighted individuals.  #@NEW_LINE#@#  For example, in sighted individuals the category preference for scenes is correlated with a feature preference for higher spatial frequencies, straight lines, and eccentric stimulation (for more discussion, see ref.  #@NEW_LINE#@#  48).  #@NEW_LINE#@#  
Finally, even though we find significant cross-decoding, our results do not rule out the possibility that part of the category preferences in the blind individuals is idiosyncratic to this population.  #@NEW_LINE#@#  Indeed, in Fig 7, the auditory maps of blind subjects seem to be more similar to the auditory maps of other blind participants than to the visual maps of the sighted participants.  #@NEW_LINE#@#  This finding is surprising, because the visual maps of the sighted are more robust and more consistent among themselves, and might suggest that the maps of the blind participants have peculiarities that are not present in the maps of the sighted.  #@NEW_LINE#@#  

V1_and_A1  #@NEW_LINE#@#  
The finding that V1 discriminates among the four sound categories only in the blind illustrates the brains ability to reorganize itself when a brain region is deprived of input (49).  #@NEW_LINE#@#  Previous work in the blind has demonstrated that when the brain is deprived of visual input, early visual cortex can show involvement in complex cognitive tasks, such as listening to spoken language (50) and spatially informative sounds (51).  #@NEW_LINE#@#  Earlier work in sighted individuals also has demonstrated that auditory scenes elicit discriminable responses in early visual cortex (52), although our sighted controls did not show this effect.  #@NEW_LINE#@#  The exact functional mechanisms behind this selectivity for auditory stimuli in V1 are still to be investigated.  #@NEW_LINE#@#  A possible explanation might be that, because we focused primarily on higher-level categorical boundaries, we did not control for all low-level auditory properties of our stimuli.  #@NEW_LINE#@#  Features such as spectral properties and modulation over time may differ among categories and could have activated V1 differently in our blind participants.  #@NEW_LINE#@#  
In A1, we found that selectivity for the auditory conditions was stronger in the sighted controls than in the blind participants.  #@NEW_LINE#@#  The direction of this difference was unexpected, given that blind participants rely upon auditory perception much more than do the controls and do not have any visual input to compete for attentional resources.  #@NEW_LINE#@#  This finding is all the more striking because the strength of the selectivity for auditory stimuli tended to go in the opposite direction in ventral visual cortex, with stronger selectivity for auditory stimuli.  #@NEW_LINE#@#  Still, this seemingly reduced functionality of nondeprived brain regions has been reported before in studies on early blind individuals (53, 54).  #@NEW_LINE#@#  It seems as if some of the processing in nondeprived cortex has been replaced by the responses in deprived visual areas (55).  #@NEW_LINE#@#  The finding is also helpful for the interpretation of our findings in visual cortex, because the weaker decoding for the sounds in the auditory cortex of the blind rules out the possibility that the stronger decoding in VTC is related to an enhanced processing of the sounds.  #@NEW_LINE#@#  
Interestingly, the activation patterns in V1 and A1 did not allow cross-decoding between visual and auditory stimuli.  #@NEW_LINE#@#  Note that we should be careful not to attach too many conclusions to this outcome, given that it is possible that the between-subject classification fails for more technical reasons, such as the activation patterns being organized differently in different subjects or are organized at a finer scale than in VTC.  #@NEW_LINE#@#  

Natural_Sounds_Related_to_Visual_Categories  #@NEW_LINE#@#  
The use of natural sounds to investigate category-selectivity in ventral visual cortex has proven to stimulate categorical regions in visual cortex of blind individuals successfully, providing an interesting perspective on the domain × modality interaction that was recently proposed (56).  #@NEW_LINE#@#  Note that our study does not allow us to compare the response to natural sounds directly with the response to spoken words in blind individuals.  #@NEW_LINE#@#  In addition, we emphasize that our finding of shared category preferences between blind and sighted individuals does not pinpoint the exact informational content that underlies this between-group similarity; the informational representation is an important question for future research.  #@NEW_LINE#@#  


Experimental_Procedures  #@NEW_LINE#@#  
Participants  #@NEW_LINE#@#  
Fourteen congenitally blind (five females, mean age 37.1 y) and 20 sighted adults with normal or corrected-to-normal vision (11 females, mean age 34.5 y) took part in this study.  #@NEW_LINE#@#  Because of technical problems, we could not record auditory runs for two of the sighted subjects, resulting in 18 subjects with both visual and auditory data and two subjects with only visual data.  #@NEW_LINE#@#  Participants were screened for fMRI compatibility, signed informed consent, and were financially compensated for their participation.  #@NEW_LINE#@#  The study was approved by the medical ethical committee of Katholieke Universiteit Leuven.  #@NEW_LINE#@#  

Gradation_of_Blindness  #@NEW_LINE#@#  
The blind participants had to meet two inclusion criteria that are typical in the literature of congenital blindness: (i) that the onset of blindness was congenital, and (ii) that the participant had no (memory of) object or shape perception.  #@NEW_LINE#@#  We assigned the blind volunteers to three categories of blindness: category 1: anophthalmia, i.e., no eyeballs developed in utero; category 2: no (memory of) lightdark perception; and category 3: lightdark perception in one or both eyes.  #@NEW_LINE#@#  See Table 1 for more details.  #@NEW_LINE#@#  Objectively, category 1 blind participants are the only participants whom one can be certain had no visual perception even in early postnatal weeks or months.  #@NEW_LINE#@#  

Stimuli  #@NEW_LINE#@#  
We used four visual categories for both visual and auditory stimulation: faces, body parts, artificial objects, and scenes.  #@NEW_LINE#@#  Visual stimuli consisted of 64 short (1,800 ms) movie clips with 16 unique clips per category.  #@NEW_LINE#@#  In the face category, clips portrayed actions such as laughing, chewing, blowing a kiss, and whistling.  #@NEW_LINE#@#  In the body parts category, clips showed scratching, hand clapping, finger snapping, bare foot steps, and knuckle cracking.  #@NEW_LINE#@#  In the objects category we created clips of a car, a washing machine, a clock, tools, a mug, a glass, a ball, and a fan.  #@NEW_LINE#@#  In the scenes category, the clips depicted waves crashing on a beach, a restaurant overview, a train station, traffic, a forest, a lake, and a grass field.  #@NEW_LINE#@#  Auditory stimuli consisted of 64 short (1,800 ms) audio clips with 16 unique clips per category.  #@NEW_LINE#@#  Stimuli were matched in overall sound intensity by normalizing the rms of the sound pressure levels.  #@NEW_LINE#@#  In the face category, clips reflected facial sounds such as laughing, chewing, blowing a kiss, and whistling.  #@NEW_LINE#@#  In the body category, sounds illustrated scratching, hand clapping, finger snapping, and foot steps.  #@NEW_LINE#@#  In the objects category, sounds depicted various artificial man-made objects and tools, such as a car starting, a washing machine, a bouncing ball, fluid poured into a glass, and the ripping of paper.  #@NEW_LINE#@#  In the scenes category, sounds reflected waves crashing on a beach, a calm lake, a forest with birds, a crowded restaurant, a train station, and a busy road.  #@NEW_LINE#@#  We attempted to match each visual stimulus with a sound stimulus, that is, the visual movie clip of clapping hands and the sound of clapping hands.  #@NEW_LINE#@#  

Design  #@NEW_LINE#@#  
With the sighted participants we scanned four runs each in the auditory and visual modalities.  #@NEW_LINE#@#  The auditory runs were presented before the visual runs.  #@NEW_LINE#@#  The blind volunteers were presented with eight auditory runs.  #@NEW_LINE#@#  Within each run, the stimuli were presented in a block design, with four blocks per condition for each run.  #@NEW_LINE#@#  In each block, eight stimuli were presented with a 200-ms interstimulus interval.  #@NEW_LINE#@#  The conditions were presented in an ABCD order, in which A, B, C, and D refer to the four different conditions.  #@NEW_LINE#@#  The blocks of each condition were separated by a 200-m interval, until all four conditions were shown.  #@NEW_LINE#@#  After each ABCD block, a fixation cross was presented for 12 s. The ABCD presentation order was counterbalanced within and between runs to account for possible order effects.  #@NEW_LINE#@#  Each run lasted 7.5 min.  #@NEW_LINE#@#  

Task  #@NEW_LINE#@#  
In the scanner, subjects were asked to perform a one-back task in which they had to rate the conceptual dissimilarity of each presented stimulus to the previous stimulus on a scale of 14 with a button press.  #@NEW_LINE#@#  We gave each subject the verbal example of a cat meowing and a dog barking being conceptually similar, and a cat meowing and a car starting being dissimilar.  #@NEW_LINE#@#  Our rationale was that this task would encourage the participants to process the categorical information in the stimuli attentively.  #@NEW_LINE#@#  Before scanning, but after the participants were provided with hearing protection, we adjusted the volume to an individually comfortable audio level.  #@NEW_LINE#@#  
To verify whether subjects perceived the stimuli as intended, we investigated if the subjects rating of dissimilarity when one category block ended and a new category began was significantly higher than the dissimilarity rating within a category block.  #@NEW_LINE#@#  For this analysis, we excluded subjects who gave fewer than five responses at the between-block trials (which might happen because our stimulus speed was relatively fast to allow swift responses at a block change in this subjective comparison task).  #@NEW_LINE#@#  In addition, when multiple button presses were given in a trial, we only used the first response.  #@NEW_LINE#@#  Task performance shows that participants perceived the stimuli in a categorical manner.  #@NEW_LINE#@#  For the blind participants (n = 11), the mean dissimilarity (± SEM) of the first stimulus in a category block to the last stimulus in the previous block was 3.41 ± 0.12 across subjects, significantly higher [t (10) = 6.05, P less_than 0.001] than the mean dissimilarity of stimuli within a block, 2.33 ± 0.16 across subjects.  #@NEW_LINE#@#  For the control subjects (n = 15), the among-category dissimilarity rating was 3.62 ± 0.11 on average, whereas the within-category dissimilarity was rated significantly lower [2.40 ± 0.11, t (14) = 8.8, P less_than 0.001].  #@NEW_LINE#@#  This difference demonstrates that subjects picked up the category boundaries.  #@NEW_LINE#@#  
As an additional and conservative estimate of stimulus recognition and categorization, we designed two simple behavioral tasks.  #@NEW_LINE#@#  The first experiment was intended to quantify how well the sound stimuli could be categorized correctly.  #@NEW_LINE#@#  Twenty healthy volunteers (not included in the imaging experiment) were presented twice with all the sound stimuli in a random order.  #@NEW_LINE#@#  Participants were asked to determine the correct condition with a button press.  #@NEW_LINE#@#  On average, the face sounds were correctly categorized 89% of the time, the body sounds 76%, scene sounds 96%, and object sounds 89%.  #@NEW_LINE#@#  Note that these accuracies reflect the stimuli perceived in isolation, rather than in blocks of eight stimuli as during the scanning paradigm.  #@NEW_LINE#@#  Thus, these accuracies are a conservative estimate of perceived category membership in a blocked presentation.  #@NEW_LINE#@#  The second experiment was intended to test whether the individual stimuli could be recognized correctly.  #@NEW_LINE#@#  Twenty healthy volunteers (not included in the imaging experiment or in the first behavioral test) were presented with the individual sounds.  #@NEW_LINE#@#  On screen, 50 alphabetically ordered options were presented from which the participant had to pick the correct answer.  #@NEW_LINE#@#  Across subjects, 83% of the stimuli were recognized correctly on average.  #@NEW_LINE#@#  Divided per category, recognition accuracies were as follows: face sounds 98%, body sounds 81%, scene sounds 69%, and object sounds 86%.  #@NEW_LINE#@#  Because some of the answer options closely resembled other options in the same category, we also computed the accuracy with which subjects picked an answer from the correct category: face sounds 98%, body sounds 84%, scene sounds 96%, and object sounds 96%.  #@NEW_LINE#@#  

Data_Acquisition_Parameters  #@NEW_LINE#@#  
Functional and anatomical images were acquired on a 3T Philips Ingenia CX scanner (Department of Radiology, University of Leuven) with a 32-channel head coil.  #@NEW_LINE#@#  Each functional run consisted of 225 T2*-weighted echoplanar images (EPIs) [1.875 × 1.875 mm in-plane voxel size, 32 2.2-mm slices, interslice gap 0 mm, repetition time (TR) = 2,000 ms, echo time (TE) = 30 ms, 112 × 112 matrix].  #@NEW_LINE#@#  In addition to the functional images, we collected a high-resolution T1-weighted anatomical scan for each participant (182 slices, resolution 0.98 × 0.98 × 1.2 mm, TR = 9.6 ms, TE = 4.6 ms, 256 × 256 acquisition matrix).  #@NEW_LINE#@#  Stimuli were presented using custom-written MATLAB R2014a code (MathWorks Inc.) and Psychtoolbox-3 (57) via an NEC projector with an NP21LP lamp that projected the image onto a screen the participant viewed through a mirror.  #@NEW_LINE#@#  The viewing distance was 64 cm.  #@NEW_LINE#@#  

Data_Preprocessing  #@NEW_LINE#@#  
Functional and anatomical data were preprocessed using the BrainVoyager QX2.8 package (Brain Innovation).  #@NEW_LINE#@#  The remainder of the data processing and analyses were performed using custom-written MATLAB code in combination with the NeuroElf v0.9c toolbox (NeuroElf.net).  #@NEW_LINE#@#  Functional volumes were first corrected for differences in slice scan time and 3D head motion using three translation and three rotation parameters.  #@NEW_LINE#@#  Subsequently, linear trends and low-frequency temporal drifts were removed from the data using a high-pass filter, removing temporal frequencies below four cycles per run.  #@NEW_LINE#@#  After the preprocessing, functional data were coregistered to the high-resolution anatomical volume and were normalized to Talairach space.  #@NEW_LINE#@#  For each subject, a white-matter surface reconstruction was made for each hemisphere separately.  #@NEW_LINE#@#  

Trial_Estimation  #@NEW_LINE#@#  
Individual responses to the experimental trials were estimated by fitting a double-gamma hemodynamic response function (HRF) to each voxels time course, using the resulting beta as trial estimate.  #@NEW_LINE#@#  The trials were labeled according to their corresponding condition (face, body, scene, object).  #@NEW_LINE#@#  The functional voxel-based data were subsequently sampled to the reconstructed vertex-based surface (integrated in depth along the vertex normals from 1.0 mm to 3.0 mm using linear interpolation).  #@NEW_LINE#@#  

Cortex-Based_Alignment  #@NEW_LINE#@#  
A cortex-based alignment (CBA) procedure (58) was applied as implemented in the BrainVoyager QX 2.8 software package.  #@NEW_LINE#@#  For the left and right hemisphere individually, each subjects curvature information was aligned to the dynamic group (n = 34) average.  #@NEW_LINE#@#  On the resulting average surface reconstruction, VTC was defined manually by observing anatomical boundaries: the occipitotemporal sulcus, posterior transverse collateral sulcus, parahippocampal gyrus, and the anterior tip of the MFS (35).  #@NEW_LINE#@#  By using the vertex migration information that resulted from the CBA procedure, we could create anatomically matched VTC masks for each subject to use in the subsequent analyses.  #@NEW_LINE#@#  In addition to VTC, we also used V1 and A1 as ROIs by selecting the calcarine sulcus and Heschls gyrus, respectively, bilaterally on the average cortex reconstruction.  #@NEW_LINE#@#  

Univariate_Analyses  #@NEW_LINE#@#  
We first performed a univariate group analysis on the surface-based beta maps after CBA for the visual and auditory sessions individually.  #@NEW_LINE#@#  For each vertex in our ROI, the mean response (beta) to each condition was computed by first averaging the responses within each subject and then averaging across subjects.  #@NEW_LINE#@#  Based on these averages, we selected the condition with the strongest response for each vertex.  #@NEW_LINE#@#  We then performed a paired-samples t test across subjects between the condition that yielded the strongest response in that vertex and the mean response of the remaining three conditions, and the t statistic was assigned to the vertex.  #@NEW_LINE#@#  The resulting maps were corrected for multiple comparisons by maintaining a false-discovery rate (FDR) with a proportion of false discoveries (q) = 0.05.  #@NEW_LINE#@#  

Topographical_Selectivity_Maps  #@NEW_LINE#@#  
In addition to univariate statistical mapping, we plotted the unthresholded selectivity after normalizing for global differences in response.  #@NEW_LINE#@#  More specifically, we first computed the average response across subjects to each of the four conditions per vertex, after which we computed the average response across all vertices in VTC.  #@NEW_LINE#@#  We then subtracted this average per condition from each vertex and mapped the condition that had the strongest normalized beta for that vertex.  #@NEW_LINE#@#  This process resulted in distinct clusters of vertices that spatially distinguish themselves from their surround in terms of selectivity for a particular condition.  #@NEW_LINE#@#  

Within-Subject_Classification  #@NEW_LINE#@#  
For the classification procedure, we used a linear support-vector machine (l-SVM) (59, 60), based on the LIBSVM algorithm (https://www.csie.ntu.edu.tw/cjlin/libsvm/).  #@NEW_LINE#@#  We selected features (vertices) based on the anatomically defined VTC mask that was created after cortex-based alignment.  #@NEW_LINE#@#  Each trial was normalized (z-scored) to obtain a mean response of 0 and an SD of 1 across all vertices within our ROI.  #@NEW_LINE#@#  For each pairwise comparison of categories, we trained the classifier with the labeled training data using a leave-one-run-out cross-validation.  #@NEW_LINE#@#  Thus we ran the classification procedure four or eight times (depending on whether the participant was sighted or blind), using each run once as test data; then the final classification accuracy was computed by averaging over the four or eight folds.  #@NEW_LINE#@#  We used a randomization procedure to estimate the distribution parameters under the null hypothesis.  #@NEW_LINE#@#  For each of the cross-validation folds in the original classification, 1,000 permutations were run in which we randomly permuted the condition labels, restricted to a label shuffle within each run (with no shuffling across runs).  #@NEW_LINE#@#  From the 4 × 1,000 or 8 × 1,000 prediction accuracies, we obtained the 1,000 mean prediction accuracies under the null hypothesis by averaging over all four or eight folds.  #@NEW_LINE#@#  By ranking the true prediction accuracy against the ranked permutation scores, we subsequently assessed the significance of the classification for a particular subject.  #@NEW_LINE#@#  
To test the classification performance across all participants per condition pair, we tested the prediction accuracies against the averaged permutation accuracies using a nonparametric Wilcoxon sign rank test (61).  #@NEW_LINE#@#  The P values were corrected for multiple comparisons with an FDR correction with q = 0.05.  #@NEW_LINE#@#  
To compare the classification results between the blind and healthy subjects directly, we first averaged the prediction accuracies across category pairs per subject group.  #@NEW_LINE#@#  Then, we tested the mean difference between the groups against the distribution of means obtained by 10,000 shuffles of group labels across subjects.  #@NEW_LINE#@#  

Between-Subject_Classification  #@NEW_LINE#@#  
The rationale for the between-subject classification analyses was to quantify the extent to which the response profile of a given subject could be generalized to all other subjects.  #@NEW_LINE#@#  The procedure was largely similar to the within-subject analyses, with some small adjustments.  #@NEW_LINE#@#  First, we ran the classification analysis for each subject pair, i.e., the algorithm was trained on subject 1 and subsequently was tested on subjects 220.  #@NEW_LINE#@#  The 19 prediction accuracies were averaged and assigned to subject 1 as the generalization index for that subject.  #@NEW_LINE#@#  This procedure was repeated for all six pairwise comparisons of the categories.  #@NEW_LINE#@#  Here we also used a nonparametric Wilcoxon sign rank test (61).  #@NEW_LINE#@#  The P values were corrected for multiple comparisons with an FDR correction with q = 0.05.  #@NEW_LINE#@#  It is important to note that MVPA is often used to study neural representations that might not align spatially across subjects (62, 63).  #@NEW_LINE#@#  The rationale of our between-subject classification fits with the use of MVPA to study larger-scale maps (64, 65) and was appropriate, given that the category preferences that we target have been shown to fall in similar regions in different subjects, as already demonstrated for faces (1, 66), bodies (2), and scenes (5).  #@NEW_LINE#@#  

Testing_the_Topographical_Consistency_Across_Subjects  #@NEW_LINE#@#  
We directly compared the consistency of the functional spatial topography between subjects, disregarding univariate effect size differences between modalities.  #@NEW_LINE#@#  First, for each subject and for the auditory (AUD) and visual (VIS) runs separately, we created a 4 × V matrix, which contains the mean response amplitude (beta) per condition for each vertex.  #@NEW_LINE#@#  This value was obtained by averaging all betas across all runs per condition.  #@NEW_LINE#@#  To make the maps of the blind subjects comparable to those of the sighted controls in terms of the number of runs, we based the maps of the blind participants on only the first four runs.  #@NEW_LINE#@#  The resulting 4 × V matrix captures the full response topography of VTC of a subject in a given modality.  #@NEW_LINE#@#  We then computed the correlation between each subject pair, i.e., the Pearson correlation between the auditory (AUD) data of subject 1 and subject 2, and so forth.  #@NEW_LINE#@#  In addition to within-modality comparisons, we also computed the correlation coefficients between modalities, i.e., comparing AUD subject 1 and the visual data (VID) of subject 2.  #@NEW_LINE#@#  To prevent unspecific activity differences of vertices from biasing the correlation measures to be above zero even if no selectivity would be present, we subtract the vertexs mean response across conditions from each vertex (7, 8, 67).  #@NEW_LINE#@#  The resulting correlation coefficients were assigned to the upper diagonal of the matrix.  #@NEW_LINE#@#  Note that although the correlation matrix visually resembles a representational dissimilarity matrix (68, 69), the information content of our correlation matrix is fundamentally different: the rows and columns correspond not to conditions but to a combination of subjects and stimulus modalities.  #@NEW_LINE#@#  
To perform statistics on the pairwise correlation coefficients found between subjects, we used a randomization approach.  #@NEW_LINE#@#  For each pairwise comparison between subjects, we permuted the four runs of one of these subjects (the shuffled subject) by shuffling the four labels.  #@NEW_LINE#@#  There are 24 permutations of the four category labels, and because we have four runs, there are a total of 244 possible shuffles.  #@NEW_LINE#@#  We randomly sampled 1,000 shuffles from these possible permutations of labels.  #@NEW_LINE#@#  For each permutation, we computed the 4 × V matrix as described above but with randomly assigned condition labels per run.  #@NEW_LINE#@#  Then, the correlations were computed between the unaffected subject map and the 1,000 shuffled maps of the paired subject.  #@NEW_LINE#@#  By repeating this procedure for each subject pair, we created 1,000 correlation matrices under the null hypothesis.  #@NEW_LINE#@#  
For all statistical assessments of the correlation results, for a particular submatrix of interest (e.g., the between-subject consistency of the functional maps related to the visual stimulation), we averaged the correlation coefficients from the appropriate cells and ranked this average against the distribution of averaged correlation coefficients from the permutations.  #@NEW_LINE#@#  When comparing two submatrices, we computed the average correlation coefficients of the two respective submatrices and subtracted these values from each other.  #@NEW_LINE#@#  This process was repeated for the 1,000 permutation correlations for both submatrices; then the true difference score was ranked against the distribution of permutations.  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
We thank Tom de Graaf and Michelle Moerel for contributing to our stimulus set and Nicky Daniëls for technical and moral assistance.  #@NEW_LINE#@#  This project was funded by European Research Council Grant ERC-2011-Stg-284101, Federal Research Action Interuniversity Attraction Poles Grant P7/11, and Hercules Fund Grant ZW11_10.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  




