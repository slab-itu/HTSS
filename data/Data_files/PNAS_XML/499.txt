article id="http://dx.doi.org/10.1073/pnas.1614763114"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Spatiotemporal dynamics of similarity-based neural representations of facial identity  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Humans can rapidly discriminate among many highly similar facial identities across identity-preserving image transformations (e.g., changes in facial expression), an ability that requires the system to rapidly transform image-based inputs into a more abstract, identity-based representation.  #@NEW_LINE#@#  We used magnetoencephalography to provide a temporally precise description of this transformation within human face-selective cortical regions.  #@NEW_LINE#@#  We observed a transition from an image-based representation toward an identity-based representation after 200 ms, a result suggesting that, rather than computing a single representation, a given face-selective region may represent multiple distinct types of information about face identity at different times.  #@NEW_LINE#@#  Our results advance our understanding of the microgenesis of fine-grained, high-level neural representations of object identity, a process critical to human visual expertise.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Humans remarkable ability to quickly and accurately discriminate among thousands of highly similar complex objects demands rapid and precise neural computations.  #@NEW_LINE#@#  To elucidate the process by which this is achieved, we used magnetoencephalography to measure spatiotemporal patterns of neural activity with high temporal resolution during visual discrimination among a large and carefully controlled set of faces.  #@NEW_LINE#@#  We also compared these neural data to lower level image-based and higher level identity-based model-based representations of our stimuli and to behavioral similarity judgments of our stimuli.  #@NEW_LINE#@#  Between 50 and 400 ms after stimulus onset, face-selective sources in right lateral occipital cortex and right fusiform gyrus and sources in a control region (left V1) yielded successful classification of facial identity.  #@NEW_LINE#@#  In all regions, early responses were more similar to the image-based representation than to the identity-based representation.  #@NEW_LINE#@#  In the face-selective regions only, responses were more similar to the identity-based representation at several time points after 200 ms. Behavioral responses were more similar to the identity-based representation than to the image-based representation, and their structure was predicted by responses in the face-selective regions.  #@NEW_LINE#@#  These results provide a temporally precise description of the transformation from low- to high-level representations of facial identity in human face-selective cortex and demonstrate that face-selective cortical regions represent multiple distinct types of information about face identity at different times over the first 500 ms after stimulus onset.  #@NEW_LINE#@#  These results have important implications for understanding the rapid emergence of fine-grained, high-level representations of object identity, a computation essential to human visual expertise.  #@NEW_LINE#@#  

Examples of stimuli presented in the current study.  #@NEW_LINE#@#  (A) All faces are presented with a neutral facial expression.  #@NEW_LINE#@#  Eight additional face identities (models 21, 2427, and 3234) from the NimStim Face Stimulus Set (28) and four additional identities from the Psychological Image Collection at Sterling Pain Expressions Set (male models 5, 6, 8, and 10) were included in our stimulus set but cannot be reproduced here under the release terms of those stimulus sets.  #@NEW_LINE#@#  (B) All details as described for A, with the exception that faces are presented with happy expressions.  #@NEW_LINE#@#  
Heat maps showing pairwise distance values for image-based and identity-based representations of the facial identities in our stimulus set.  #@NEW_LINE#@#  Each cell of each matrix shows the distance value associated with the comparison of two identities, across a change in facial expression, with hotter colors indicating a greater distance (i.e., larger difference between the representations of each identity in the pair).  #@NEW_LINE#@#  
Results  #@NEW_LINE#@#  
Behavior_During_MEG_Face_Identity_Task  #@NEW_LINE#@#  
In each of the 2628 blocks of the task, participants viewed each of 91 face identities four times (twice per expression) while brain activity was recorded with MEG.  #@NEW_LINE#@#  Participants were instructed to maintain fixation and to press a button whenever they saw the same face identity repeated, regardless of facial expression.  #@NEW_LINE#@#  Across all participants and blocks, mean d-prime was 2.21 (SD = 0.52).  #@NEW_LINE#@#  

MEG  #@NEW_LINE#@#  

Behavioral_Similarity_Ratings  #@NEW_LINE#@#  
Behavioral dissimilarity ratings (Fig 6) were strongly and positively correlated with both the identity- (r = 0.89) and image-based (r = 0.79) representations but were significantly more strongly correlated with the former than with the latter, P less_than 0.0001 (31).  #@NEW_LINE#@#  Correlations between the behavioral and neural data were statistically significant at most postbaseline time points (Fig 6).  #@NEW_LINE#@#  Correlations were not significantly stronger for the face-selective regions than for the control region at any postbaseline time points.  #@NEW_LINE#@#  However, a multiple regression analysis indicated that responses in both face-selective regions predicted behavioral responses after controlling for responses in the control region (lV1) between 100 and 250 ms after stimulus onset, and also between 350 and 400 ms in rLO-faces (see Materials and Methods for details).  #@NEW_LINE#@#  Overall, this pattern indicates that behavioral responses primarily reflect an identity-based representation but may also reflect image-based properties to a lesser extent, and that these behavioral responses can be predicted by responses in face-selective regions during earlier (i.e., between 100 and 200 ms) and later (after 200 ms) time periods in which these regions seem to represent lower and higher level information about facial identity, respectively.  #@NEW_LINE#@#  

Left_Hemisphere  #@NEW_LINE#@#  
We extended the analyses described above to the left hemisphere (see Supporting Information and Fig S2 for details).  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
In the current study, we investigated when spatiotemporal patterns of activity within face-selective cortex carry information sufficient for discrimination of facial identity across changes in facial expression, what type of information about facial identity (e.g., low-level image-based information, higher level identity-based information, and information encoded in pairwise behavioral judgments of the stimuli) is represented in these patterns, and where in the brain (e.g., in face-selective or control regions) these different types of information are represented at different points in time.  #@NEW_LINE#@#  In two face-selective regions (rLO-faces and rFG-faces) and one control region (lV1) we first measured pairwise classification among all possible pairs of 91 face identities, across changes in facial expression so as to tap into a more abstract representation, invariant over the geometry of the input.  #@NEW_LINE#@#  We then compared the pairwise similarity structure of the data in each of these regions to an image-based representation based on relatively low-level visual information and to a higher level identity-based representation.  #@NEW_LINE#@#  We also compared the neural data to behavioral similarity judgments of the stimuli.  #@NEW_LINE#@#  Between 50 and 400 ms, we were able to decode face identity successfully in each of the three regions, with accuracy first peaking at values above 70% between 100 and 200 ms, and with a secondary peak in the face-selective regions between 200 and 300 ms.  #@NEW_LINE#@#  In all regions, neural responses were more similar to the image-based representation than to the identity-based representation until 200 ms.  #@NEW_LINE#@#  In the face-selective regions only, the correlation with the image-based representation dropped at several time points after 200 ms, so that responses were more similar to the identity-based representation.  #@NEW_LINE#@#  Behavioral responses were more similar to the identity-based representation than to the image-based representation, and their structure was predicted by responses in the face-selective regions between 100 and 250 ms, after controlling for responses in the control region.  #@NEW_LINE#@#  
Our finding of successful face identity classification across changes in facial expression between 50 and 400 ms in all regions suggests that all regions carried information relevant to discrimination over a relatively large span of time after stimulus onset.  #@NEW_LINE#@#  This result provides evidence that even very early visual regions can encode information sufficient for discriminating among highly similar complex visual patterns, with some degree of tolerance to identity-preserving transformations.  #@NEW_LINE#@#  Note, however, that the face images in the current study were carefully aligned to remove obvious cues to identity caused by misalignment, and that this alignment may amplify the extent to which relatively low-level image-based differences can be used for identity discrimination, even across changes in facial expression.  #@NEW_LINE#@#  Hence, it is not surprising that we were able to decode facial identity across changes in facial expression, even in left V1.  #@NEW_LINE#@#  That decoding accuracy reached an initial peak between 100 and 200 ms is consistent with previous findings that the N170 component in EEG and the corresponding M170 component in MEG are correlated with characteristics of individual face identities (1618).  #@NEW_LINE#@#  In addition, the finding that decoding performance remained above chance at several time points after 200 ms is consistent with findings from intracranial recordings from fusiform gyrus that facial identity could be decoded between 200 and 500 ms after stimulus onset (15).  #@NEW_LINE#@#  Also, the finding of secondary peaks in classification accuracy in the face-selective regions at around 250 ms seems to be compatible with data showing that subordinate-level processing of faces selectively enhances the N250 ERP component in EEG, which peaks 250 ms after stimulus onset (32, 33).  #@NEW_LINE#@#  In sum, our decoding analysis based on spatiotemporal patterns of activity captured many existing findings in the EEG and MEG literatures, while allowing further analyses of the temporal dynamics of the similarity structure of the neural representation of facial identity.  #@NEW_LINE#@#  
A particularly compelling aspect of our results is the transition between image-based and identity-based representations observed after 200300 ms in the face-selective regions, but not in the control region (lV1), and the relation between the timing of this transition and that of classification accuracy.  #@NEW_LINE#@#  In rLO-faces, there were two obvious peaks in which decoding accuracy exceeded 70%, one between 100 and 200 ms and the second between 200 and 300 ms. At the first peak, the similarity structure of the neural data was more similar to an image-based representation, whereas at the second peak the correlation with the image-based representation dropped, so that the neural data were more similar to the identity-based representation.  #@NEW_LINE#@#  This pattern was not observed in the control region (lV1), because the data were more similar to the image-based representation than to the identity-based representation at all time points.  #@NEW_LINE#@#  Previous studies have demonstrated that signals in occipitotemporal regions between 100 and 200 ms are related to image-based properties of faces (1618).  #@NEW_LINE#@#  However, the transition toward an identity-based representation within face-selective cortical regions after 200 ms has not been observed in previous studies of discrimination of facial identity, because previous studies either used fMRI (313), which lacks the temporal resolution required to resolve the temporal patterns observed in the current study, and/or because their analyses of the neural representations were based on different sensors for different time points (18), and/or were limited to image-based properties of the stimuli (15, 17, 18).  #@NEW_LINE#@#  The observed pattern provides evidence that spatiotemporal patterns of activity in at least some face-selective regions in human cortex encode qualitatively different information about face identity at different times over the first few 100 ms after stimulus onset, with a transition from a lower level representation to a higher level representation occurring around 200300 ms.  #@NEW_LINE#@#  This pattern suggests that models of the neural basis of face recognition that assign a single function to each face-selective cortical region (14) are likely to be incomplete, because they do not account for the possibility that a given face-selective region may play different functional roles at different times.  #@NEW_LINE#@#  Given that the identity-based representation used in the current study represents any exemplar of the same identity identically, whereas the image-based representation does not, this late transition could reflect the temporal emergence of a neural representation with high tolerance to identity-preserving transformations (19).  #@NEW_LINE#@#  Such a representation is highly relevant for real-world behavior, because many situations require the system to track a single identity and/or discriminate between identities across identity-preserving image transformations.  #@NEW_LINE#@#  Further support for the behavioral relevance of this representation comes from our finding that pairwise behavioral judgments of the stimuli were more strongly correlated with the identity-based representation than with the image-based representation, and that responses in the face-selective regions predicted behavior during temporal periods in which responses in the face-selective regions transitioned toward an identity-based representation.  #@NEW_LINE#@#  Given that the observed transition occurs relatively late, and that it corresponds with a secondary later peak in the classification accuracy function, it seems somewhat unlikely that it arises as a consequence of a single initial feedforward sweep (34) but instead reflects recurrent/feedback processing (19, 35).  #@NEW_LINE#@#  
One remaining question is how the face-selective regions identified in the current study (rLO-faces and rFG-faces) are related to the corresponding face-selective regions typically identified in fMRI studies of face processing: occipital face area (OFA) and fusiform face area (FFA).  #@NEW_LINE#@#  Given that all source points within rLO-faces and rFG-faces in the current study are face-selective and are located within the same anatomical subregions as OFA and FFA, respectively, it seems possible that their representations would overlap with those of OFA and FFA.  #@NEW_LINE#@#  However, at least two differences are likely to limit the degree of overlap.  #@NEW_LINE#@#  First, MEG and fMRI measure different aspects of neural activity and have different spatial signal distributions, and are therefore likely to be sensitive to different spatial patterns of activity.  #@NEW_LINE#@#  For example, MEG is less sensitive to signals from deeper and more gyral sources than it is to more superficial and sulcal sources (36).  #@NEW_LINE#@#  Given that the rFG-faces region used in the current study is deeper and more gyral than rLO-faces, it seems possible that rLO-faces would capture signals from the corresponding fMRI-defined region to a greater extent than rFG-faces.  #@NEW_LINE#@#  This could account for the lower sensitivity to face identity observed in rFG-faces than in rLO-faces.  #@NEW_LINE#@#  Second, our MEG data have rich temporal structure, but fMRI data do not, and so the MEG data are unlikely to correspond to the fMRI data at all time points.  #@NEW_LINE#@#  Hence, although it is possible that the representations measured from rLO-faces and rFG-faces in the current study reflect activity from OFA and FFA, they are likely to reflect different aspects of the representation of facial identity than those standardly measured in OFA and FFA with fMRI.  #@NEW_LINE#@#  
Taken together, our results provide important information about when spatiotemporal patterns in face-selective cortical regions discriminate among a large and carefully controlled set of face identities across changes in facial expression, and about what type of information is represented in each region, and when.  #@NEW_LINE#@#  Specifically, our results indicate spatiotemporal patterns of activity in both face-selective and control regions encode information about facial identity between 50 and 400 ms after stimulus onset.  #@NEW_LINE#@#  However, the face-selective regions, but not the control region, seem to encode qualitatively different information about facial identity at different times, with a transition from an image-based representation toward an identity-based representation after 200300 ms. As described above, these results have implications for understanding the microgenesis of fine-grained, high-level neural representations of object identity, a process critical to human visual expertise (19), and perhaps for distinguishing between feedforward versus recurrent/feedback accounts of visual processing.  #@NEW_LINE#@#  Overall, the current investigation represents a critical advancement toward understanding the temporal dynamics of visual pattern recognition in the human brain.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
Participants  #@NEW_LINE#@#  
All participants were Caucasian (white European), right-handed, and had normal or corrected-to-normal visual acuity and no history of eye problems.  #@NEW_LINE#@#  Participants in the MEG experiment were four adults (one female), aged 2327 y.  #@NEW_LINE#@#  Participants in the behavioral experiment were seven adult humans (five female), aged 1828 y, none of whom participated in the MEG experiment.  #@NEW_LINE#@#  No participants were tested but excluded.  #@NEW_LINE#@#  Protocols were approved by institutional review boards at Carnegie Mellon University and the University of Pittsburgh.  #@NEW_LINE#@#  All participants provided written informed consent before each session and received monetary compensation for their participation.  #@NEW_LINE#@#  

MEG  #@NEW_LINE#@#  

Behavioral_Similarity_Ratings  #@NEW_LINE#@#  
In each of two 1-h sessions, participants viewed a subset of pairs of faces from the stimulus set used in the MEG experiment (Fig 6) and rated the similarity of the face identities on an 8-point scale, with a value of 1 indicating very different face identities and a value of 8 indicating the same face identity.  #@NEW_LINE#@#  Within a pair, face images always differed in facial expression (Supporting Information).  #@NEW_LINE#@#  We used the methods described above to compare the behavioral data to the neural data and model-based representations.  #@NEW_LINE#@#  To examine whether neural data from the face-selective regions could predict the behavioral data after controlling for responses in lV1, we fit a multiple linear regression model in which the distance values for lV1 and a face-selective region were used to predict the behavioral data (37).  #@NEW_LINE#@#  


MEG  #@NEW_LINE#@#  
Acquisition  #@NEW_LINE#@#  
To allow correction of eye movement artifacts, we recorded the electrooculogram (EOG) from four electrodes.  #@NEW_LINE#@#  To allow correction of heartbeat artifacts, we recorded the ECG from a pair of electrodes.  #@NEW_LINE#@#  Four head position indicator (HPI) coils were used to monitor the participants head position within the MEG helmet.  #@NEW_LINE#@#  At the beginning of each session, a digitizing pen was used to record the shape of the participants head and the locations of the HPI coils on the head in 3D space.  #@NEW_LINE#@#  For all participants, head position was recorded from the HPI coils at the beginning of each block.  #@NEW_LINE#@#  For participants 1, 3, and 4, continuous HPI (cHPI) signals were recorded during each block, to allow movement compensation during preprocessing.  #@NEW_LINE#@#  cHPI signals were not recorded for participant 2 because enabling cHPI recording seemed to produce excessive artifacts.  #@NEW_LINE#@#  A Panasonic PT-D7700U projector (1,024 × 768 resolution, 60-Hz refresh rate) presented the stimuli at the center of a back-projection screen placed 120 cm from the participant.  #@NEW_LINE#@#  Face images were 6.87° high and 5° wide at this viewing distance.  #@NEW_LINE#@#  To track stimulus timing, we used a photodiode that emitted a continuous signal when the stimulus was on the screen.  #@NEW_LINE#@#  In addition, the experimental software sent a signal to the MEG acquisition computer whenever a stimulus was presented.  #@NEW_LINE#@#  Participants entered responses by pressing a button with their right index fingers.  #@NEW_LINE#@#  

Preprocessing  #@NEW_LINE#@#  
We first applied signal source separation in Maxfilter (Elekta AB), with head movement compensation enabled where applicable.  #@NEW_LINE#@#  The temporal extension (tSSS) was enabled for subject 4, for whom this extension was required to remove an artifact caused by an orthodontic appliance.  #@NEW_LINE#@#  tSSS was also enabled for the localizer data in subject 3, for whom this extension significantly boosted signal.  #@NEW_LINE#@#  We then carried out temporal filtering and artifact rejection and correction in MNE-Python (38) and applied a band-pass filter with lower and upper cutoffs set to 1 and 100 Hz, respectively.  #@NEW_LINE#@#  To remove power line noise, we applied notch filters at 60, 120, and 180 Hz.  #@NEW_LINE#@#  Empty room data were used to create signal space projectors, which were applied to the filtered raw data to remove environmental artifacts (39).  #@NEW_LINE#@#  To correct eye movement and heartbeat artifacts, we used MNE-Python to fit an independent components analysis model to the ECG and EOG data and remove components correlated with the MEG data.  #@NEW_LINE#@#  Finally, trials with signals exceeding standard thresholds (gradiometer = 4,000e 13, magnetometer = 4e 12) in at least one channel were rejected (38).  #@NEW_LINE#@#  For the main face identity task, at least 101, 92, 101, and 97 trials per face identity were retained for participants 14, respectively.  #@NEW_LINE#@#  

Source_Localization  #@NEW_LINE#@#  
For each participant, we acquired a T1-weighted MPRAGE anatomical MRI scan on a Siemens Verio 3T scanner (voxel size 1 mm3, flip angle 9°, TE = 1.97 ms, TR = 2,300 ms, FOV = 256 × 256 × 176 mm).  #@NEW_LINE#@#  All scans were carried out at the Scientific Imaging and Brain Research Center at Carnegie Mellon University.  #@NEW_LINE#@#  Freesurfer reconstructions based on the anatomical scan were used in source modeling of MEG signals.  #@NEW_LINE#@#  
For each participant, we used dynamic statistical parametric mapping (dSPM) (40) to project single-trial MEG data onto the cortical surface reconstructed from each participants MRI data.  #@NEW_LINE#@#  This approach allowed us to align and combine each participants data across sessions and allowed us to restrict our analyses to signals estimated to originate in specific regions on the cortical surface.  #@NEW_LINE#@#  We used the MNE watershed tool (39) and Freesurfer output to generate boundary element models (BEMs).  #@NEW_LINE#@#  In mne-analyze, we used the digitizer data and the BEMs to align each participants MEG data to their MRI anatomical image.  #@NEW_LINE#@#  In MNE, we generated a source space on the reconstructed cortical surface, with source points spaced 5 mm apart.  #@NEW_LINE#@#  We then generated a forward solution, which maps the MEG sensor space to the source space (39).  #@NEW_LINE#@#  Using this forward solution, we performed dSPM source localization in MNE-Python [single-trial data, noise covariance matrix calculated from baseline period ( 200 to 0 ms), signal-to-noise ration 3, no depth prior] (38, 39).  #@NEW_LINE#@#  Note that the estimates of activity provided by our inverse model do not directly measure neural activity, but instead represent a probability density function for activity across the source space.  #@NEW_LINE#@#  Because the inverse model provides an estimate of activity at a given point in the source space, we use activity to refer to this estimate throughout the paper, for simplicity.  #@NEW_LINE#@#  


Face_Identity_Task  #@NEW_LINE#@#  
Stimuli  #@NEW_LINE#@#  
Stimuli were color frontal photographs of 91 young Caucasian (white European) male face identities selected from six databases (28, 4144), including PICS (see Fig 1 for examples).  #@NEW_LINE#@#  The following male identities from the Karolinska Directed Emotional Faces database were included: 2, 3, 5, 6, 7, 10, 11, 12, 13, 17, 18, 21, 23, 24, 25, 26, 27, 28, 31, 32, and 35.  #@NEW_LINE#@#  The stimulus set included two expressions (neutral or happy) per identity.  #@NEW_LINE#@#  To eliminate global luminance and color cues, we converted each image to CIELAB color space and set the mean of the L (luminance), a (redgreen), and b (yellowblue) channels to the mean values across all identities.  #@NEW_LINE#@#  We also set the rms contrast of the L channel to the mean across all identities.  #@NEW_LINE#@#  To minimize differences in alignment, face images were transformed without altering aspect ratio, so that the eyes were in the same positions in each image.  #@NEW_LINE#@#  To eliminate hair cues, we applied an oval mask of constant size to each face image.  #@NEW_LINE#@#  To minimize other obvious cues, we excluded faces with facial hair, uneven lighting, and/or aspect ratios (defined here as the ratio of the horizontal interocular distance to the vertical eyemouth distance) greater than two standard deviations from the group mean.  #@NEW_LINE#@#  

Design  #@NEW_LINE#@#  
Before the MEG experiment, each participant first completed a single 1-h session of behavioral training on the task that they would complete during subsequent MEG sessions.  #@NEW_LINE#@#  In each block of training, participants viewed each of the 91 face identities four times (twice per expression), for a total of 364 trials.  #@NEW_LINE#@#  Participants were instructed to maintain fixation and to press a key on a computer keyboard whenever they saw the same face identity repeated, regardless of facial expression.  #@NEW_LINE#@#  During each of the MEG sessions except for the last part of the final MEG session participants did the same behavioral task as in the training session while MEG signals were recorded.  #@NEW_LINE#@#  
The order of trials was randomized for each block, with the exception that the same face identity was presented twice in a row on 36 trials per block.  #@NEW_LINE#@#  The two face images had the same expression on half of these repeat trials and had different expressions on the other half.  #@NEW_LINE#@#  The positions of the repeat trials within the block and the face identities to be presented on these repeat trials were randomly selected for each block.  #@NEW_LINE#@#  Each trial began with a white fixation dot presented for 500 ms, followed by a face image presented for 500 ms, then a blank response screen for 1,500 ms.  #@NEW_LINE#@#  

Localizer_Task  #@NEW_LINE#@#  
Each run of the MEG localizer consisted of 15 blocks, with a fixation baseline (8 s) between blocks.  #@NEW_LINE#@#  Within each run, there were three blocks for each of five categories of images (faces, objects, scrambled objects, houses, and words) presented in a random order.  #@NEW_LINE#@#  Within each block, 16 images from a single category were presented in a row (900 ms per image, 100-ms interstimulus interval), in a random order.  #@NEW_LINE#@#  Each participant completed six runs, for a total of 288 trials per category.  #@NEW_LINE#@#  Participants were instructed to press a button on the response glove with their right index fingers to indicate the presence of one repeated image within each block.  #@NEW_LINE#@#  
To identify face-selective source points in each participant, we used the statcond function in EEGLAB (45) to carry out a nonparametric, one-way permutation test on single-trial data for each participant, source point, and postbaseline time point (10,000 permutations per test).  #@NEW_LINE#@#  Each test yielded a P value indicating the extent to which activity differed between faces and objects.  #@NEW_LINE#@#  For each participant, P values were FDR-corrected across all source points and time points.  #@NEW_LINE#@#  A source point was considered to be face-selective if it responded significantly more strongly to faces than to objects (FDR less_than 0.05) at one or more time points and did not respond significantly more strongly to objects than to faces at any time point.  #@NEW_LINE#@#  

Classification_of_Face_Identity  #@NEW_LINE#@#  
For each brain region, time point, participant, and face identity, we extracted the spatiotemporal pattern of neural activity across all source points within the region and across all time points between the current time point and a time point 60 ms after the current time point.  #@NEW_LINE#@#  We then computed the pairwise Euclidean distance for each possible pair of identities.  #@NEW_LINE#@#  To ensure that, to the extent possible, this analysis captured information invariant to changes in facial expression (i.e., were specific to identity), all Euclidean distance values were computed across a change in facial expression.  #@NEW_LINE#@#  To increase power, we averaged Euclidean distance values across participants.  #@NEW_LINE#@#  
For a given time point, region of interest, and pair of face identities, inputs to the k-nearest-neighbor classifier were the cross-expression within-identity Euclidean distance value for the each of the two identities, and the cross-expression, cross-identity distance value.  #@NEW_LINE#@#  When spatiotemporal patterns of activity contain sufficient information to distinguish between face identities across a change in facial expression, the cross-identity distance will be larger than the within-identity distance.  #@NEW_LINE#@#  Hence, for each face in the pair, the classifier compared the within-identity distance to the cross-identity distance and decided that the smaller of the two (i.e., the nearest neighbor) was the within-identity distance.  #@NEW_LINE#@#  Overall classification accuracy was computed as the proportion of correct decisions across all face pairs.  #@NEW_LINE#@#  Because each decision is binary, accuracy fluctuates around 0.5 (50% correct) when there is no signal (i.e., in the baseline period; Fig 4).  #@NEW_LINE#@#  
To evaluate whether whether classification accuracy was significantly greater than chance, we computed a statistical threshold for accuracy [Bonferroni-corrected alpha = 0.001 (46)] from a null distribution with 10,000 values.  #@NEW_LINE#@#  Each value in the null distribution was generated by shuffling the labels in the neural data and recomputing classification performance.  #@NEW_LINE#@#  To allow for the possibility that different regions might have different statistical properties, and therefore different null distributions, we computed statistical thresholds separately for each region.  #@NEW_LINE#@#  The threshold represents the accuracy level associated with a probability of 0.001 under the null hypothesis that the neural data do not reliably encode information about facial identity.  #@NEW_LINE#@#  

Comparison_with_Model-Based_Representations  #@NEW_LINE#@#  
To generate an image-based representation of the stimuli, we first converted each face image to Lab color space and extracted the luminance component of the image.  #@NEW_LINE#@#  We then extracted activations for each image from the first layer (S1) of HMAX, which simulates responses of a population of simple cells in V1 (29).  #@NEW_LINE#@#  We used the vector of activations for each image to compute a cross-expression Euclidean distance matrix of the same form used for the classification analysis described above.  #@NEW_LINE#@#  The identity-based representation was a 91 × 91 matrix in which each row and column represents a facial identity, and each cell represents a comparison between two facial identities (Fig 2).  #@NEW_LINE#@#  All cells representing comparisons within identities (e.g., row 1, column 1) received a distance value of 0, which indicates identical representations.  #@NEW_LINE#@#  The remaining cells, which represent comparisons between identities, received a distance value of 1.  #@NEW_LINE#@#  This value indicates that representations of exemplars of different identities differ to the same extent.  #@NEW_LINE#@#  
For each brain region and postbaseline time point (from 0 ms onward) we computed the correlation between the cross-expression distance matrix for the neural data and that for each of the representations described above.  #@NEW_LINE#@#  We excluded the baseline period ( 200 to 0 ms) from this analysis because, during this period, classification accuracy did not exceed chance, and so there was no evidence that information about face identity was encoded.  #@NEW_LINE#@#  We then tested whether the correlation was stronger for the identity-based representation or the image-based representation (31).  #@NEW_LINE#@#  The resulting P values for each region were FDR-corrected (47) across all time points.  #@NEW_LINE#@#  

Behavioral_Similarity_Ratings  #@NEW_LINE#@#  
In each of two 1-h sessions each participant provided similarity ratings for 377 face pairs.  #@NEW_LINE#@#  These face pairs included all 91 within-identity pairs and an additional 286 between-identity pairs.  #@NEW_LINE#@#  The latter were randomly selected from among all possible between-identity face pairs.  #@NEW_LINE#@#  To allow aggregation of data across participants and sessions, we used the same procedure and face pairs for each participant and session.  #@NEW_LINE#@#  In each session, participants rated each face pair twice.  #@NEW_LINE#@#  For each face pair, the two face images were always presented with different facial expressions, with all possible combinations of facial expressions presented within each session.  #@NEW_LINE#@#  On each trial, a face pair was presented sequentially, in a random order.  #@NEW_LINE#@#  Each face image was presented for 500 ms, with a 500-ms interval between images.  #@NEW_LINE#@#  
To allow comparisons between behavioral similarity ratings and neural and model-based representations, we converted similarity ratings to dissimilarity ratings by subtracting each similarity value from 8, so that a similarity rating of 1 was converted to a dissimilarity rating of 8, and so on.  #@NEW_LINE#@#  

Left_Hemisphere  #@NEW_LINE#@#  
In the left hemisphere, we carried out the primary analyses performed for face-selective regions in the right hemisphere.  #@NEW_LINE#@#  In three of four participants we were able to identify face-selective regions in left lateral occipital cortex (lLO-faces) and fusiform gyrus (lFG-faces).  #@NEW_LINE#@#  As with the right hemisphere analyses, we used V1 in the opposite hemisphere (in this case, right V1) as a control region.  #@NEW_LINE#@#  With data from these regions, we performed classification of face identity and compared the neural data to model-based representations and behavioral judgments of the stimuli.  #@NEW_LINE#@#  Results for analyses of face-selective regions in the left hemisphere are shown in Fig S1.  #@NEW_LINE#@#  All analyses of neural data include only data from the three participants in which face-selective regions could be identified in the left hemisphere.  #@NEW_LINE#@#  
The positions of face-selective regions in the left hemisphere seemed to vary between participants to a greater extent than the corresponding regions in the right hemisphere.  #@NEW_LINE#@#  Classification of face identity was successful in both left hemisphere regions, but the pattern of correlations with the model-based representations seemed to be less temporally stable than in the right hemisphere.  #@NEW_LINE#@#  In the right hemisphere regions, all significant differences between the models favored the image-based representation before 200 ms and favored the identity-based representation after 200 ms.  #@NEW_LINE#@#  In contrast, differences in the left hemisphere fluctuated between the two representations after 200 ms, a result suggesting that the transition toward the identity-based representation after 200 ms may be less stable in the left hemisphere.  #@NEW_LINE#@#  Similar to lV1, significant differences between the two representations in rV1 consistently favored the image-based model, with the exception of a very early period during which classification performance in rV1 had only just begun to exceed chance.  #@NEW_LINE#@#  This pattern suggests that V1 primarily encoded relatively low-level image-based information in each hemisphere.  #@NEW_LINE#@#  

Comparison_of_Neural_Data_to_Deep_Neural_Network  #@NEW_LINE#@#  
To understand the nature of the information represented in each region of interest we compared the similarity structure of the neural representations to those learned by a deep neural network trained to recognize versions of the experimental face stimuli.  #@NEW_LINE#@#  Input to the network consisted of 8,918 48 × 65 grayscale images of faces.  #@NEW_LINE#@#  Forty-nine versions of each of 182 original face images (91 individuals × 2 expressions) were created by jittering the position of the image up to ±3 pixels in both the x and y directions.  #@NEW_LINE#@#  For each such input the network was trained to activate a particular 1 of 91 output units.  #@NEW_LINE#@#  
The architecture of the network had four intermediate or hidden layers between the input and output, connected in a feedforward manner.  #@NEW_LINE#@#  The first hidden layer (H1) had 2,684 (44 × 61) units with 5 × 5 rectangular receptive fields (RFs) from the input with a stride of 1 (i.e., RFs were positioned every one input unit in both the x and y directions).  #@NEW_LINE#@#  The second hidden layer (H2) had 580 (20 × 29) units with 7 × 7 RFs from H1 with a stride of 2, and the third (H3) had 84 (7 × 12) units with 9 × 9 RFS from H2 with a stride of 2.  #@NEW_LINE#@#  The fourth and final hidden layer (H4) consisted of 20 units that received connections from all H3 units and sent connections to all 91 output units.  #@NEW_LINE#@#  In total, the network had 108,570 connections (including bias connections to all noninput units).  #@NEW_LINE#@#  Unlike a convolutional neural network (29, 48), positional invariance was not imposed within layers (by using weight-sharing among features and having separate pooling layers); rather, it was left to the network to learn to be sensitive or insensitive to positional information at each level of representation to the degree it supports effective recognition.  #@NEW_LINE#@#  Hidden units used a sigmoid activation function to their net inputs; the output group consisted of normalized or soft-max units whose activities were constrained to sum to 1.0 (49).  #@NEW_LINE#@#  
The network was trained with back-propagation (50) using momentum descent with accumulated gradients clipped at 1.0, with a learning rate of 0.05 and momentum of 0.8.  #@NEW_LINE#@#  After 6,000 presentations of each image, performance had reached near-asymptote: The network produced virtually no error (mean cross-entropy error per image of 0.00013) and recognition performance was perfect.  #@NEW_LINE#@#  
Representational similarity analysis was then carried out for each layer of the network by computing the degree of similarity (correlation) of the activation patterns produced by each pair of face images (without jitter).  #@NEW_LINE#@#  These similarities were then correlated with analogous similarities for the neural representations in various brain regions across time.  #@NEW_LINE#@#  We focused on H1 and H4, because the former largely reflects visual (image) similarity whereas the latter reflects primarily identity information.  #@NEW_LINE#@#  Indeed, for H4, the mean correlation is 0.097 for different identities and 0.939 for same identities.  #@NEW_LINE#@#  Although H1 and H4 seem to primarily represent image and identity information, respectively, the representations from these regions are likely to be more closely related to each other than the image-based and identity-based representations described in the main text, because layers H1 and H4 are both trained to perform the same task.  #@NEW_LINE#@#  Indeed, the correlation between H1 and H4 (Pearson r = 0.46) was slightly higher than that between the identity- and image-based representations in the main text (Pearson r = 0.40).  #@NEW_LINE#@#  
Fig S2 shows the correlations over time of each hidden layer with the two face-selective regions (rLO-faces and rFG-faces) and the control region (lV1), considering only image pairs that differ in expression.  #@NEW_LINE#@#  For rLO-faces the correlation with H4 was stronger than the correlation with H1 at a series of time points between 50 and 100 ms, and a later series between 200 and 300 ms.  #@NEW_LINE#@#  In contrast, lV1 responses were more strongly correlated with H1 than H4 between 350 ms and 450 ms, with no significant differences in the opposite direction.  #@NEW_LINE#@#  In rFG, the correlation with H4 was stronger at only one early period (6090 ms).  #@NEW_LINE#@#  Hence, as in the comparison with image-based and identity-based representations in the main text, responses in rLO-faces seem to be more similar to a higher level representation, whereas responses in lV1 seem to be more similar to a lower level, more image-based representation.  #@NEW_LINE#@#  rFG seemed to show an intermediate response in the current analyses, as it did for most time points in the analyses presented in the main text.  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
This work was supported by Natural Sciences and Engineering Research Council PDF Award 471687-2015 (to M.D.V.  #@NEW_LINE#@#  ), a Small Grant from the Temporal Dynamics of Learning Center (to M.D.V.  #@NEW_LINE#@#  and M.B.  #@NEW_LINE#@#  ), Pennsylvania Department of Healths Commonwealth Universal Research Enhancement Program Grant SAP-14282-012 (to D.C.P.  #@NEW_LINE#@#  ), National Science Foundation Grant BCS0923763 (to M.B.  #@NEW_LINE#@#  and D.C.P.  #@NEW_LINE#@#  ), and Temporal Dynamics of Learning Center Grant SMA-1041755 (principal investigator: G. Cottrell) (to M.B.  #@NEW_LINE#@#  ).  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  


