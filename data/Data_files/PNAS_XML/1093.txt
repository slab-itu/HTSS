article id="http://dx.doi.org/10.1073/pnas.1714730115"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Algorithms in the historical emergence of word senses  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
How do words develop new senses?  #@NEW_LINE#@#  Unlike changes in sound or grammar where there are rich formal characterizations, semantic change is poorly understood.  #@NEW_LINE#@#  Changes in meaning are often considered intractable, with sparse attempts at formalizing and evaluating the principles against historical data at scale.  #@NEW_LINE#@#  We present a data-enriched formal approach that explores the time-varying lexicon.  #@NEW_LINE#@#  Motivated by prominent work in cognitive science and linguistics, we develop computational algorithms that predict the historical order in which the senses of a word have emerged, and we test these predictions against records of English over the past millennium.  #@NEW_LINE#@#  Our findings suggest that word senses emerge in ways that minimize cognitive costs, providing an efficient mechanism for expressing new ideas via a compact lexicon.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Human language relies on a finite lexicon to express a potentially infinite set of ideas.  #@NEW_LINE#@#  A key result of this tension is that words acquire novel senses over time.  #@NEW_LINE#@#  However, the cognitive processes that underlie the historical emergence of new word senses are poorly understood.  #@NEW_LINE#@#  Here, we present a computational framework that formalizes competing views of how new senses of a word might emerge by attaching to existing senses of the word.  #@NEW_LINE#@#  We test the ability of the models to predict the temporal order in which the senses of individual words have emerged, using an historical lexicon of English spanning the past millennium.  #@NEW_LINE#@#  Our findings suggest that word senses emerge in predictable ways, following an historical path that reflects cognitive efficiency, predominantly through a process of nearest-neighbor chaining.  #@NEW_LINE#@#  Our work contributes a formal account of the generative processes that underlie lexical evolution.  #@NEW_LINE#@#  

Chaining and the computational problem.  #@NEW_LINE#@#  (A) Hypothetical sense chaining for English face.  #@NEW_LINE#@#  Senses and dates of appearance in parentheses are from The Historical Thesaurus of English (12).  #@NEW_LINE#@#  (B) Illustration of the problem.  #@NEW_LINE#@#  Senses of a word are represented by dots, with red indicating the earliest recorded sense.  #@NEW_LINE#@#  Can the order of historical sense development be predicted, and, if so, by what algorithms?  #@NEW_LINE#@#  
Computational_Formulation_of_Theory  #@NEW_LINE#@#  
We formulate word sense extension as a computational problem, illustrated in Fig 1B.  #@NEW_LINE#@#  We ask how an individual words various senses could have emerged over time by attaching to existing senses of that word, and consider alternative extensional mechanisms that yield different paths.  #@NEW_LINE#@#  Because the space of possible extensional paths grows factorially with the number of senses a word develops (see Model Cost and Likelihood), we focus on the paths predicted by five probabilistic algorithms that have each been motivated by prior work on semantic representation.  #@NEW_LINE#@#  We show that the nearest-neighbor chaining algorithm tends to yield the most cost-effective sense extension strategy.  #@NEW_LINE#@#  We now present the algorithms and then define cost.  #@NEW_LINE#@#  
Algorithms_of_Word_Sense_Extension  #@NEW_LINE#@#  
Given the set of senses a word has developed over history, all algorithms that we propose infer which sense is likely to emerge at time t+1 (i.e., the next time point in history where new senses appeared), based on existing senses of a word up to time t: S(t)={s0,s1,,st}.  #@NEW_LINE#@#  Beginning with the earliest sense of a word s0, each algorithm predicts sequentially (from the candidate pool of yet-to-emerge senses) which will be the upcoming sense, based on a unique extensional mechanism that attaches novel senses to existing ones.  #@NEW_LINE#@#  As a result, each algorithm specifies a probability distribution over all of the possible historical orders in which a words senses could have emerged (see Model Cost and Likelihood).  #@NEW_LINE#@#  At each time point, an algorithm predicts the next emerging sense with a probability specified by Luces choice rule (17),sf(s,S(t))sS(t)f(s,S(t)).  #@NEW_LINE#@#  [1]S(t) represents the set of candidate senses given by the historical record that have not appeared up to time t, for a given word.  #@NEW_LINE#@#  Each model has a different likelihood function f(s,S(t)) that specifies the mechanism that links the candidate emerging sense to the existing senses.  #@NEW_LINE#@#  The likelihood functions specify computations based on semantic similarity between senses, which we describe below.  #@NEW_LINE#@#  To make minimal assumptions, all of the models are parameter-free, and hence are on equal footing in model complexity (i.e., 0).  #@NEW_LINE#@#  We describe and summarize the models in Table 1, along with a null model.  #@NEW_LINE#@#  

Random_Algorithm  #@NEW_LINE#@#  
This null model predicts the historical emergence of a words senses to be random.  #@NEW_LINE#@#  

Exemplar_Algorithm  #@NEW_LINE#@#  
This algorithm is motivated by Medin and Schaffer (8) and Nosofsky (9), whereby the emerging, to be lexicalized sense at t+1 is predicted with a probability based on average semantic similarity with existing, already lexicalized senses of a word up to time t.  #@NEW_LINE#@#  

Prototype_Algorithm  #@NEW_LINE#@#  
This algorithm is motivated by Rosch (5) and Geeraerts (7) and predicts the emerging sense at t+1 with a probability based on semantic similarity with the prototypical sense at time t. We define prototype at t as the sense with the highest semantic similarity to all other existing senses of the word: prototype(S(t))maxsiSjisim(si,sj).  #@NEW_LINE#@#  Thus, this algorithm allows the most representative sense of a word to change as a function of time, as a word accrues more senses.  #@NEW_LINE#@#  

Progenitor_Algorithm  #@NEW_LINE#@#  
This algorithm is a static variant of the prototype algorithm.  #@NEW_LINE#@#  It assumes a fixed prototype that is always the earliest recorded or progenitor word sense.  #@NEW_LINE#@#  It predicts the emerging sense at t+1 with a probability based on semantic similarity with the progenitor sense, for each candidate sense.  #@NEW_LINE#@#  

Local_Algorithm  #@NEW_LINE#@#  
This algorithm assumes that word senses emerge in a local temporal chain, where the emerging sense at t+1 is sampled with a probability based on semantic similarity to the sense that appears just before it, namely at time t (i.e., st).  #@NEW_LINE#@#  Thus, senses that appear before t have no influence on the emerging sense at t+1.  #@NEW_LINE#@#  This assumption posits that an emerging sense will be minimally distant from the most recent sense of a word (i.e., local minimum), in contrast with the next algorithm, which tends to minimize distance in a global way (i.e., between all sense pairs).  #@NEW_LINE#@#  

Nearest-Neighbor_Chaining_Algorithm  #@NEW_LINE#@#  
This algorithm is closely related to prior proposals about chaining.  #@NEW_LINE#@#  It approximates Prims algorithm for constructing a minimal spanning tree (18) (see Nearest-Neighbor Chaining and Minimal Spanning Tree), but with a fixed starting point; that is, it always begins with the progenitor sense of a word.  #@NEW_LINE#@#  The algorithm predicts the emerging sense of a word at t+1 with a probability based on the highest semantic similarity with any of the existing word senses up to t, rendering a chain that connects nearest-neighboring senses over time.  #@NEW_LINE#@#  In contrast with the other algorithms, this algorithm tends to construct a sense network at globally minimal cost (see Nearest-Neighbor Chaining and Minimal Spanning Tree), a metric that we describe in Cost of Word Sense Extension.  #@NEW_LINE#@#  

Cost_of_Word_Sense_Extension  #@NEW_LINE#@#  
Sense extension can be thought of as involving costs, such that certain historical paths can be considered more cognitively efficient or cost-effective than others.  #@NEW_LINE#@#  For example, extending the meaning of face via body partfacial expression might entail a lower cost than body partfront surface of an object, since the former pair of senses appear to be more semantically related and mentally associable than the latter.  #@NEW_LINE#@#  If sense extension tends to minimize cost in the historical path, then, given the initial body part sense of face, we expect facial expression to emerge earlier in history than front surface of an object.  #@NEW_LINE#@#  Whether historical paths do minimize costs is a key empirical question that we address.  #@NEW_LINE#@#  
We quantify the cost of the models by considering the degree to which they minimize cognitive effort in sense extension over time.  #@NEW_LINE#@#  Specifically, given that a novel sense appears at a certain time point and location in semantic space, the cost measure determines how efficient the path toward that location is.  #@NEW_LINE#@#  We do not predict the location of the new sense but instead evaluate how cost-effective the aggregated spatiotemporal path toward that sense is.  #@NEW_LINE#@#  For a given model m that prefers a certain historical path over alternatives, we define cost c asc(pathm)=tsiS(t),sjS(t)e(sisj).  #@NEW_LINE#@#  [2]Namely, the cost of a model is the aggregated effort (denoted by e) of extending existing senses to novel ones as predicted by that model, summed over all time points where senses have emerged for a word.  #@NEW_LINE#@#  We operationalize effort by semantic distance, the inverse of semantic similarity.  #@NEW_LINE#@#  A cost-effective model should tend to minimize this quantity in the historical extensional paths that it specifies.  #@NEW_LINE#@#  Given that each model predicts a path probabilistically, the average cost of a model considering all possible paths is p(pathm)c(pathm).  #@NEW_LINE#@#  It can be shown that the nearest-neighbor chaining model tends to produce nearminimal-cost paths, in contrast with the other competing models (see Results, Nearest-Neighbor Chaining and Minimal Spanning Tree, and Model Cost and Likelihood).  #@NEW_LINE#@#  Of course, the hypothesis that historical sense extension is best predicted by a low-cost model could be wrong, because word senses may not have developed in ways that minimize costs (Model Cost and Likelihood discusses how model cost and predictive likelihood are dissociable).  #@NEW_LINE#@#  Whether or not they do is an empirical question that we examine next.  #@NEW_LINE#@#  


Results  #@NEW_LINE#@#  
We assess our models in three steps.  #@NEW_LINE#@#  First, we demonstrate, in a simulation, that the nearest-neighbor chaining model generally yields the lowest cost in posited sense extensional paths, compared with alternative models.  #@NEW_LINE#@#  Second, we test the models ability to predict the order of sense emergence against chance (i.e., against the null model), using a large digitized historical lexicon of English.  #@NEW_LINE#@#  Third, we evaluate the models against each other and show that nearest-neighbor chaining dominates the other models in accounting for the historical data.  #@NEW_LINE#@#  
Model_Simulation  #@NEW_LINE#@#  
We first examined whether the nearest-neighbor model yields sense extensional paths that minimize cognitive cost.  #@NEW_LINE#@#  We simulated the proposed models in a hypothetically constructed semantic space, where we used Euclidean distance to represent the similarity between two senses.  #@NEW_LINE#@#  We used Euclidean distance only for this simulation, instead of the psychologically grounded measure of semantic similarity which we used for the empirical analyses.  #@NEW_LINE#@#  We placed 15 points randomly in a 2D plane that represents the semantic space of a single word, designating the bottom right point in the space as the initial sense of the word.  #@NEW_LINE#@#  We then applied the set of algorithms to the remaining data points and visualized the sense extensional paths specified by each algorithm.  #@NEW_LINE#@#  For simplicity, we display the paths based on model trajectories that maximize choice probability at each time step.  #@NEW_LINE#@#  The same result held when we varied the simulation parameters (see Nearest-Neighbor Chaining and Minimal Spanning Tree).  #@NEW_LINE#@#  
Fig 2 shows that these algorithms yield distinct temporal paths in the simulated space.  #@NEW_LINE#@#  For instance, the exemplar algorithm links novel senses to all existing senses based on average distances between them (illustrated by links that develop from spaces between senses as opposed to stemming directly from senses).  #@NEW_LINE#@#  The prototype algorithm predicts a dynamic radial structure (6), where temporal links are established by attaching novel senses to prototype senses, while allowing the prototype to change over time.  #@NEW_LINE#@#  The progenitor algorithm predicts a strict radial structure where all senses stem from the earliest progenitor sense.  #@NEW_LINE#@#  The local algorithm predicts a temporal linkage of senses by attaching each emerging sense to the existing sense of the word that appeared one time point earlier.  #@NEW_LINE#@#  Finally, the nearest-neighbor chaining algorithm renders a tree structure that branches off as needed to preserve nearest-neighbor relations between emerging and existing word senses.  #@NEW_LINE#@#  Importantly, although both the local and nearest-neighbor chaining algorithms tend to yield lower aggregated cognitive costs in sense extension compared with the other models, the latter algorithm yields the global (as opposed to temporally local) minimal cost in semantic space.  #@NEW_LINE#@#  

Model_Evaluation_Against_Historical_Sense_Records  #@NEW_LINE#@#  
We next assessed the extent to which the proposed models predict the historical emergence of word senses better than chance.  #@NEW_LINE#@#  In particular, we examined each models ability to predict the actual orders in which English words senses have emerged, relative to the null model.  #@NEW_LINE#@#  
We used The Historical Thesaurus of English (HTE) (12)the worlds first and largest digital historical dictionaryas a test bed.  #@NEW_LINE#@#  The HTE records word formsense entries from across the past millennium, sourced from The Oxford English Dictionary and compiled by historical lexicographers and period specialists.  #@NEW_LINE#@#  It provides the dates of emergence, or time stamps, of word senses (providing ground truths for the models), and a systematic classification scheme that sorts each word sense into a conceptual taxonomic hierarchy.  #@NEW_LINE#@#  We defined semantic similarity between two senses based on how closely they are related in this taxonomy (see Example Calculation of Conceptual Proximity and Illustration of Verb Taxonomy for examples), and we validated this measure against human similarity judgments (see Materials and Methods).  #@NEW_LINE#@#  
To test the null hypothesis, we compared the proposed algorithms predictions regarding the historical orders of emerging word senses for about 5,000 common words of English, drawn from the British National Corpus (BNC) (19), that appear in the HTE.  #@NEW_LINE#@#  Because HTE does not provide word frequency information, we used the BNC to identify the most common words.  #@NEW_LINE#@#  We used a standard statistical measurelog likelihood ratioto assess each algorithm against the null model (for more details and examples, see Materials and Methods and Model Cost and Likelihood).  #@NEW_LINE#@#  The log likelihood ratio quantifies the degree to which a model predicts the actual historical order in which a words senses have emerged.  #@NEW_LINE#@#  The null is rejected if this quantity exceeds 0, or chance level, substantially.  #@NEW_LINE#@#  
Fig 3A summarizes the mean log likelihood ratios across the words examined.  #@NEW_LINE#@#  The bar plot indicates that each of the proposed algorithms yields higher predictive likelihoods on the emerging order of word senses significantly better than chance (pless_than0.001 from all one-tailed t tests [n=4,164]: exemplar, t=47.5; prototype, t=26.3; progenitor, t=22.5; local, t=34.3; and nearest-neighbor chaining, t=36.7).  #@NEW_LINE#@#  This result provides strong evidence against the null: The order in which English word senses have emerged can be predicted better than chance by taking into account the semantic similarities between senses.  #@NEW_LINE#@#  To control for the possibility that differences in the relative ages of words might have affected our results (e.g., some words in the BNC may have been in existence longer than others), we also ran the same test on words from the HTE that have existed continuously from Old English to the present day.  #@NEW_LINE#@#  We obtained similar results for each model (pless_than0.001 from all one-tailed t tests [n=2,648]: exemplar, t=29.8; prototype, t=17.1; progenitor, t=12.1; local, t=23.5; and nearest-neighbor chaining, t=23.7), offering additional support that a words senses emerge in predictable ways.  #@NEW_LINE#@#  

Predominance_of_Nearest-Neighbor_Chaining  #@NEW_LINE#@#  
To explore whether the emergence of word senses follows nearminimal-costchained paths, we compared the nearest-neighbor algorithm against the competitor algorithms.  #@NEW_LINE#@#  Fig 3A provides support for our hypothesis: Nearest-neighbor chaining yields a substantially higher mean log likelihood compared with all competing models.  #@NEW_LINE#@#  Paired t tests show significant differences between the chaining model and each of the competitors (pless_than0.001 from all tests [n=4,164] with Bonferroni correction for multiple tests: against exemplar (t=24.8), prototype (t=26.9), progenitor (t=28.2), and local (t=20.6)).  #@NEW_LINE#@#  These results also hold for the word set that controls for age of words (see Model Comparison Controlling for Age of Words for details).  #@NEW_LINE#@#  Fig 3B visualizes the model performances under a more stringent winner-take-all measure from the log likelihood ratio tests.  #@NEW_LINE#@#  The percentages show the relative proportions of winning cases from the five models (the null model excluded in the figure explains 10.2% of the cases).  #@NEW_LINE#@#  As shown, nearest-neighbor chaining yields the highest percentage, best explaining the historical data.  #@NEW_LINE#@#  
To better understand the conditions that favor chaining relative to other mechanisms of sense extension, we examined the extent to which the chaining model outperformed other models on a word-by-word basis.  #@NEW_LINE#@#  For each word, we calculated the pairwise difference in log likelihood between the nearest-neighbor model and the remaining models.  #@NEW_LINE#@#  A positive score for a word indicates that chaining outperforms competing models in predicting the historical order of emergence of that words senses (see Analyses of Conditions That Favor Chaining for details).  #@NEW_LINE#@#  We then related these chaining superiority scores to properties of the individual words, i.e., their orthographic length, and their degree of polysemy (estimated by number of recorded senses in the HTE).  #@NEW_LINE#@#  We expected that, because short and/or polysemous words tend to be used frequently (20), cost-effective strategies of sense extension like chaining should be most relevant for these words.  #@NEW_LINE#@#  Fig 4 plots how chaining superiority scores correlate with these two variables.  #@NEW_LINE#@#  As can be seen, the chaining models success correlated strongly with number of word senses (r=0.68, pless_than0.001), and, to a lesser extent, with word length (r=0.28; pless_than0.001).  #@NEW_LINE#@#  Strikingly, the correlation between number of senses and chaining superiority scores remained strong even when partialing out word length (partial correlation =0.70, pless_than0.001), while the correlation between word length and chaining superiority was quite small after partialing out degree of polysemy (=0.13; pless_than0.001).  #@NEW_LINE#@#  These results suggest that the nearest-neighbor chaining model performed best for words that have developed many senses over time, i.e., precisely those words whose sense extensional paths could have been the most costly (see Analyses of Conditions That Favor Chaining for details and example words).  #@NEW_LINE#@#  
To illustrate the nearest-neighbor chaining process, we visualized its predicted path for the English word game.  #@NEW_LINE#@#  Fig 5 shows a low-dimensional projection (via multidimensional scaling with a random starting point) of all emerging senses for the word game as a noun in the HTE database.  #@NEW_LINE#@#  As can be seen, the nearest-neighbor chaining algorithm forms a minimal-spanning-tree-like path among the senses of game, by linking nodes that are semantically close.  #@NEW_LINE#@#  Importantly, this process supports branching and the formation of local clusters, identified roughly in this case as hunting (upper left cluster), plotting (upper middle cluster), and entertainment/sports (upper right cluster) in Fig 5.  #@NEW_LINE#@#  This process offers a computational basis for family resemblance (3) and polysemy, by allowing words to develop both related and distinct senses.  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
We provide three contributions.  #@NEW_LINE#@#  First, we showed why nearest-neighbor chaining might be a preferred algorithm for sense extension, making a connection to graph-theoretical work in computer science.  #@NEW_LINE#@#  Second, we developed an infrastructure using resources from the digital humanities, to enable large-scale computational explorations of the historical emergence of word senses.  #@NEW_LINE#@#  Finally, we provided a rigorous test of the ability of competing algorithms to recapitulate the evolution of English word senses over a thousand years.  #@NEW_LINE#@#  Our findings demonstrate that the historical order of emergence of word senses is predictable, and is best accounted for by an algorithm that tends to minimize cognitive costs over time.  #@NEW_LINE#@#  
The fact that the nearest-neighbor chaining model best explained the historical dataespecially for words that have developed many senses over timemay reflect cognitive pressures on lexical evolution.  #@NEW_LINE#@#  This algorithm may minimize the costs associated with communicating new ideas and learning a lexicon.  #@NEW_LINE#@#  Interlocutors may find it relatively effortless to encode a new intended meaning by recycling an existing word that has a closely related sense, and addressees may find it easy to understand such new word uses (13).  #@NEW_LINE#@#  Further, language learners may find it easy to learn a network of senses where each sense is highly associable with other senses (14).  #@NEW_LINE#@#  
Much past work has described patterns of semantic change such as broadening and narrowing (21), but less progress has been made in understanding the principled mechanisms that produce such changes.  #@NEW_LINE#@#  Large digital databases and computational modeling techniques open new avenues for forging a deeper understanding.  #@NEW_LINE#@#  Our work advances previous proposals about cognitive efficiency and least effort in language change by formulating and testing algorithmic accounts of the processes that generate polysemy.  #@NEW_LINE#@#  
While our models focused on how the senses of individual words have emerged over time, they could be extended to address the more general question of how new senses are incorporated into the lexicon.  #@NEW_LINE#@#  Presumably, novel senses enter the lexicon due to communicative need, but what factors explain whether a new sense will be expressed by reusing an existing word vs. creating a new word form?  #@NEW_LINE#@#  What factors explain which of the existing words in a lexicon will be selected to express a new sense?  #@NEW_LINE#@#  Our findings suggest that new senses will often be expressed by existing words with closely related senses, but this constraint might interact with other factors that shape lexical evolution.  #@NEW_LINE#@#  For instance, more-frequent word forms might be preferred over rarer ones for labeling new senses, since the former word forms may be more accessible (22).  #@NEW_LINE#@#  Further, speakers knowledge of existing, generative patterns of polysemy (2325), and their pragmatic reasoning about what senses are most likely to be understood in the current context (26), will also help explain how words accrue new senses over time, as will understanding the relative cognitive costs of generating novel words vs. reusing existing ones.  #@NEW_LINE#@#  
The current study focused on taxonomically based extensions of meaning: those in which extensions tend not to cross ontological domains.  #@NEW_LINE#@#  However, polysemy also encompasses other types of extensions, such as metonymy (27) (e.g., dish to refer to an object or the food it contains) and metaphorical mapping (6) (e.g., grasping an object vs. an idea), which often cross domains.  #@NEW_LINE#@#  Generating, understanding, and learning such diverse senses of words may draw on cognitive processes beyond those addressed here, and it is an open question whether the development of these forms of polysemy also minimizes cognitive costs (cf.  #@NEW_LINE#@#  ref.  #@NEW_LINE#@#  28).  #@NEW_LINE#@#  Our current work provides a starting point toward unlocking the algorithms that generate new word senses in lexical evolution.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
Historical_Database_of_Word_Senses  #@NEW_LINE#@#  
HTE (12) is a public dictionary that includes approximately 800,000 word formsense records, documented for a span of over 1,000 y, ranging from Old English to the present day.  #@NEW_LINE#@#  Each word sense in the HTE is annotated with the date of its emergence (and, where applicable, obsolescence) and part of speech, and is structured in a fine-grained taxonomic hierarchy that features about a quarter of a million concepts.  #@NEW_LINE#@#  Consecutive tiers of the hierarchy typically follow an A is a B or A is part of B relation.  #@NEW_LINE#@#  For example, one sense of the word game under the HTE code 01.07.04.04 is defined in a terms of four-tier hierarchy: The world (01)Food and drink (01.07)Hunting (01.07.04)Thing hunted/game (01.07.04.04).  #@NEW_LINE#@#  

Semantic_Similarity  #@NEW_LINE#@#  
We defined semantic similarity based on the taxonomic hierarchy in the HTE and then validated it against human judgments.  #@NEW_LINE#@#  We approximated psychological similarity between a pair of word senses sim(si,sj) by a measure bounded in the range of (0,1) (9): sim(si,sj)=ed(si,sj).  #@NEW_LINE#@#  Here d(si,sj) represents conceptual distance between senses, which we defined by the inverse of a conceptual proximity measure [c(,)] commonly used in natural language processing (29): d(si,sj)=1c(si,sj)=12×|p|l(si)+l(sj); |p| is the number of parent tiers shared by senses si and sj, and l() is the depth of a sense in the semantic hierarchy.  #@NEW_LINE#@#  This measure gives 1 if two senses are identical, and 0 if they have nothing in common.  #@NEW_LINE#@#  We validated this measure of semantic similarity via standard techniques in natural language processing, by evaluating its performance in predicting human judgments of word similarities (instead of judgments of sense similarities, which are not available at a broad scale).  #@NEW_LINE#@#  Following Resnik (30), we approximated word similarity by using the pair of senses for the two words that results in maximum sense similarity, defined as follows: wordsim(wi,wj)=maxsisenses(wi),sjsenses(wj)s(si,sj).  #@NEW_LINE#@#  Because this word similarity measure depends solely on the relations between word senses, it serves as a proxy indicator of word sense similarity.  #@NEW_LINE#@#  Our measure of semantic similarity yielded a Spearmans correlation of 0.441 (pless_than0.001) on Lex-999 (31), which is a well-known challenging dataset of human word similarity judgments.  #@NEW_LINE#@#  The performance of our measure of semantic similarity is better than that of the corpus-based skip-gram (Word2Vec) model, which has been trained on 1 billion words of Wikipedia text (32) and is roughly on par with the same model trained on 300 billion words (33).  #@NEW_LINE#@#  In addition, our measure of semantic similarity obtained a Spearmans correlation of 0.467 (pless_than.001) on Sim-353 (34), another common dataset of human word relatedness judgments, which is comparable to the state-of-the-art Global Vectors for Word Representation word vector model, which has been trained on 6 billion words (33, 35).  #@NEW_LINE#@#  We also considered the linear version of similarity without the exponential transformation (i.e., c(si,sj)), but the fit to human data was substantially worse (Spearmans correlations 0.361 on Lex-999 and 0.139 on Sim-353), so we chose not to use it for our analyses.  #@NEW_LINE#@#  

Model_Evaluation  #@NEW_LINE#@#  
We used log likelihood ratio LLR=log(Lm/Lnull) to assess the performance of each proposed algorithm against the null.  #@NEW_LINE#@#  For any given word, the predictive density of the null can be determined theoretically, and it is the inverse of factorial of N1 for a word with N senses: Lnull=1×1/N1×1/N2××1/1=1/(N1)!.  #@NEW_LINE#@#  Because each model is parameter-free, metrics that take into account model complexity such as the Akaike/Bayesian Information Criterion would yield equivalent results to those from this likelihood measure.  #@NEW_LINE#@#  For a stream of senses, the likelihood L is the joint probability of observing such a sequence under a certain model Lm=p(pathtrue)=p(s0)p(s1|s0)p(s2|s1,s0)p(st|st1,,s0).  #@NEW_LINE#@#  We assumed that the initial sense is always given, so p(s0)=1.  #@NEW_LINE#@#  At each year where emerging senses appeared, we removed senses that had become obsolete by that year according to the time stamps in the HTE, so those senses had no influence on model prediction.  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
We thank Charles Kemp, Terry Regier, Eve Sweetser, Josh Abbott, Aida Nematzadeh, and the Computational and Experimental Methods group in Linguistics at University of California, Berkeley for discussion.  #@NEW_LINE#@#  We thank Marc Alexander, University of Glasgow, and Oxford University Press for licensing.  #@NEW_LINE#@#  This project was funded by National Science Foundation Grants Social, Behavioral & Economic Sciences (SBE)-1041707 to the Spatial Intelligence and Learning Center and SBE-16302040 to M.S.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  

Published under the PNAS license.  #@NEW_LINE#@#  

