article id="http://dx.doi.org/10.1073/pnas.1606316113"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Accurate and scalable social recommendation using mixed-membership stochastic block models  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Recommendation systems are designed to predict users preferences and provide them with recommendations for items such as books or movies that suit their needs.  #@NEW_LINE#@#  Recent developments show that some probabilistic models for user preferences yield better predictions than latent feature models such as matrix factorization.  #@NEW_LINE#@#  However, it has not been possible to use them in real-world datasets because they are not computationally efficient.  #@NEW_LINE#@#  We have developed a rigorous probabilistic model that outperforms leading approaches for recommendation and whose parameters can be fitted efficiently with an algorithm whose running time scales linearly with the size of the dataset.  #@NEW_LINE#@#  This model and inference algorithm open the door to more approaches to recommendation and to other problems where matrix factorization is currently used.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
With increasing amounts of information available, modeling and predicting user preferencesfor books or articles, for exampleare becoming more important.  #@NEW_LINE#@#  We present a collaborative filtering model, with an associated scalable algorithm, that makes accurate predictions of users ratings.  #@NEW_LINE#@#  Like previous approaches, we assume that there are groups of users and of items and that the rating a user gives an item is determined by their respective group memberships.  #@NEW_LINE#@#  However, we allow each user and each item to belong simultaneously to mixtures of different groups and, unlike many popular approaches such as matrix factorization, we do not assume that users in each group prefer a single group of items.  #@NEW_LINE#@#  In particular, we do not assume that ratings depend linearly on a measure of similarity, but allow probability distributions of ratings to depend freely on the users and items groups.  #@NEW_LINE#@#  The resulting overlapping groups and predicted ratings can be inferred with an expectation-maximization algorithm whose running time scales linearly with the number of observed ratings.  #@NEW_LINE#@#  Our approach enables us to predict user preferences in large datasets and is considerably more accurate than the current algorithms for such large datasets.  #@NEW_LINE#@#  

A_Mixed-Membership_Block_Model_with_Metadata  #@NEW_LINE#@#  
Our approach begins with the mixed-membership stochastic block model (MMSBM), which has been used to model networks.  #@NEW_LINE#@#  As in the original MMSBM (3) and related models (11), we assume that each node in the bipartite graph of users and items belongs to a mixture of groups.  #@NEW_LINE#@#  However, unlike refs.  #@NEW_LINE#@#  3 and 11, we do not assume that these group memberships affect the presence or absence of a link.  #@NEW_LINE#@#  Instead, we take the set of links as given and attempt to predict the ratings.  #@NEW_LINE#@#  We do this with an MMSBM-like model where the rating a user gives an item is drawn from a probability distribution that depends on their group memberships.  #@NEW_LINE#@#  
First we set down some notation.  #@NEW_LINE#@#  We have N users and M items and a bipartite graph R={(u,i)} of links, where the link (u,i) indicates that item i was given a rating (observed or unobserved) by user u.  #@NEW_LINE#@#  For each (u,i)R, the rating rui belongs to some finite set S such as {1,2,3,4,5}.  #@NEW_LINE#@#  Given a set RO of observed ratings, our goal is to classify the users and the items and to predict the rating rui of a link (u,i)R for which the rating is not yet known.  #@NEW_LINE#@#  
Our generative model for the ratings is as follows.  #@NEW_LINE#@#  There are K groups of users and L groups of items.  #@NEW_LINE#@#  For each pair of groups k,, there is a probability distribution pk(r) over S of the rating r that u gives i, assuming that u belongs entirely to group k and i belongs entirely to group .  #@NEW_LINE#@#  
To model mixed group memberships, each user u has a vector uK, where uk denotes the extent to which user u belongs to group k. Similarly, each item i has a vector iL.  #@NEW_LINE#@#  These vectors are normalized; i.e., kuk=i=1.  #@NEW_LINE#@#  The probability distribution of the rating rui is then a convex combination,Pr[rui=r]=k,ukipk(r).  #@NEW_LINE#@#  [1]  #@NEW_LINE#@#  
Abbreviating all these parameters as ,,p, the likelihood of the observed ratings is thenP(RO|,,p)=(u,i)ROk,ukipk(rui).  #@NEW_LINE#@#  [2]  #@NEW_LINE#@#  
As we discuss below, we infer the values of the parameters ^,^,p^ that maximize this likelihood using an efficient expectation-maximization algorithm.  #@NEW_LINE#@#  We can then use the inferred model to predict unobserved ratings rui for pairs (u,i)RO.  #@NEW_LINE#@#  
Our work differs from previous work on collaborative filtering in several ways.  #@NEW_LINE#@#  First, unlike matrix factorization approaches such as ref.  #@NEW_LINE#@#  12 or their probabilistic counterparts (1315), we do not think of the ratings rui{1,2,3,4,5} as integers or real values.  #@NEW_LINE#@#  As has been established in the literature (16), giving a movie a rating of 5 instead of 1 does not mean the user likes it five times as much.  #@NEW_LINE#@#  Our results suggest that it is better to think of different ratings simply as different labels that appear on the links of the network.  #@NEW_LINE#@#  Moreover, our method yields a distribution over the possible ratings directly, rather than a distribution over integers or real numbers that must be somehow mapped to the space of possible ratings (1315).  #@NEW_LINE#@#  From this point of view, our model is a bipartite MMSBM with metadata (or labels) on the edges; a similar model based on the stochastic block model (SBM), where each user and item belong to only one group, is given in ref.  #@NEW_LINE#@#  2.  #@NEW_LINE#@#  An alternative approach would be to consider a multilayer representation of the data as in ref.  #@NEW_LINE#@#  4.  #@NEW_LINE#@#  
Second, we do not assume that the matrices p have any particular structure.  #@NEW_LINE#@#  In particular, we do not assume homophily, where groups of users correspond to groups of items, and users prefer items that belong to their own group: That is, we do not assume that p(r) is larger on the diagonal for higher ratings r. Thus, our model can have arbitrary couplings between groups of users and items that are independent for each possible rating.  #@NEW_LINE#@#  
Third, unlike some approaches that use inference methods similar to ours (17), as stated above, our goal is not to predict the existence of links.  #@NEW_LINE#@#  In particular, we do not assume that users see only movies (say) that they like, and we do not treat missing links as zeros or low ratings.  #@NEW_LINE#@#  To put this differently, we are not trying to complete R to a full matrix of ratings, but only to predict the unobserved ratings in RRO.  #@NEW_LINE#@#  Thus, the only terms in the likelihood of our model correspond to observed ratings.  #@NEW_LINE#@#  
As we describe below, our model also has the advantage of being mathematically tractable.  #@NEW_LINE#@#  It yields a highly efficient expectation-maximization algorithm for fitting the parameters: The time each iteration takes is linear on the number of users, items, and observed links.  #@NEW_LINE#@#  As a result, we are able to handle large datasets and achieve a higher accuracy than standard methods.  #@NEW_LINE#@#  We also show that the probabilistic predictions made by our model are well calibrated in the frequentist sense (5), producing distributions of ratings statistically similar to real data.  #@NEW_LINE#@#  

Scalable_Inference_of_Model_Parameters  #@NEW_LINE#@#  
In most practical situations, marginalizing exactly over the group membership vectors  and  and the probability matrices p (similar to ref.  #@NEW_LINE#@#  2) is too computationally expensive.  #@NEW_LINE#@#  As an alternative we propose to obtain the model parameters that maximize the likelihood (2), using an expectation-maximization (EM) algorithm.  #@NEW_LINE#@#  
In particular, we use a classic variational approach (Materials and Methods) to obtain the following equations for the model parameters that maximize the likelihood:uk=iului(k,)du,[3]i=uikui(k,)di,[4]pk(r)=(u,i)RO|rui=rui(k,)(u,i)ROui(k,).  #@NEW_LINE#@#  [5]  #@NEW_LINE#@#  
Here u={i|(u,i)RO} and i={u|(u,i)RO} denote the neighborhoods of u and i, respectively; du=|u| and di=|i| are the node degrees, i.e., the number of observed ratings for user u and item i, respectively; andui(k,)=ukipk(rui)k,ukipk(rui)[6]is the variational methods estimate of the probability that the rating rui is due to u and i belonging to groups k and , respectively.  #@NEW_LINE#@#  
These equations can be solved with an EM algorithm.  #@NEW_LINE#@#  Starting with an estimate of , , and p, we repeat the following steps until the parameters converge to a fixed point: (i) (expectation step) use Eq.  #@NEW_LINE#@#  6 to compute ui(k,) for (u,i)RO; (ii) (maximization step) use Eqs.  #@NEW_LINE#@#  35 to compute , , and p.  #@NEW_LINE#@#  
The number of parameters and terms in the sums in Eqs.  #@NEW_LINE#@#  36 is NK+ML+|RO|KL.  #@NEW_LINE#@#  Assuming that K and L are constant, each EM step is O(N+M+|RO|) and hence linear in the size of the dataset (Fig S1A).  #@NEW_LINE#@#  As the set of observed ratings RO is typically very sparse because only a small fraction of all possible useritem pairs have observed ratings, our algorithm is feasible even for very large datasets.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
The_MMSBM_Predicts_Ratings_Accurately  #@NEW_LINE#@#  
We test the performance of our algorithm in six datasets: the MovieLens 100K and 10M datasets, respectively; Yahoo!  #@NEW_LINE#@#  Songs; Amazon books (18, 19); and the LibimSeTi.cz dating agency (20), which (because it is primarily heterosexual) we split into two datasets, consisting of males rating females and vice versa.  #@NEW_LINE#@#  These datasets are diverse in the types of items, the sizes |S| of the sets of possible ratings, and the density of observed ratings (Table S1).  #@NEW_LINE#@#  For each dataset we perform a fivefold cross-validation.  #@NEW_LINE#@#  
We compare our algorithm to four benchmark algorithms (see Supporting Information, Benchmark Algorithms): a baseline naive algorithm that assigns to each test rating rui the average of the observed ratings for item i; the itemitem algorithm (21), which predicts rui based on the observed ratings of user u for items that are the most similar to i; classical matrix factorization (12); and mixed-membership matrix factorization (MMMF) (22).  #@NEW_LINE#@#  For all these benchmark algorithms except MMMF we use the implementation in the LensKit package (16), which is fast, highly optimized, and makes our results easily reproducible.  #@NEW_LINE#@#  For MMMF, we use the Matlab implementation provided by the authors (https://code.google.com/archive/p/m3f/).  #@NEW_LINE#@#  Additionally, for the smallest datasets, we also use the (unmixed) stochastic block model of ref.  #@NEW_LINE#@#  2; however, that algorithm does not scale well to larger datasets (Fig S1B).  #@NEW_LINE#@#  
For our algorithm, we set K=L=10; i.e., we assume that there are 10 groups of users and 10 groups of items (recall that we do not assume any correspondence between these groups).  #@NEW_LINE#@#  We considered some other choices of K and L, but we found no differences in performance for K,L10 (Fig S2).  #@NEW_LINE#@#  Because iterating the EM algorithm of Eqs.  #@NEW_LINE#@#  36 can lead to different fixed points depending on its initial conditions, we perform 500 independent runs.  #@NEW_LINE#@#  We average the predicted probability distribution of ratings over the resulting fixed points, because we find they typically have comparable likelihood values (Fig S3).  #@NEW_LINE#@#  
We can translate the resulting probability distribution of ratings into a single predicted rating by choosing an estimator; which one is optimal depends on the loss function or equivalently the measure of accuracy.  #@NEW_LINE#@#  We focus on two measures.  #@NEW_LINE#@#  For each algorithm, we define the accuracy as the fraction of ratings that are predicted exactly, and we also measure the mean absolute error (MAE).  #@NEW_LINE#@#  For these two, the optimal estimator is the mode and the median, respectively.  #@NEW_LINE#@#  
We find that in most datasets our approach outperforms the itemitem algorithm, matrix factorization (MF), and MMMF (Fig 1).  #@NEW_LINE#@#  Indeed, the accuracy, i.e., the fraction of exactly correct predictions, of the MMSBM is significantly higher than that of MF and MMMF for all of the datasets we tested and higher than the itemitem algorithm in five of six datasets, the only exception being Amazon Books.  #@NEW_LINE#@#  The MAE of the MMSBM is the best (lowest) in four of the six datasets; itemitem produces smaller MAE in Amazon Books; and itemitem, MF, and MMMF produce smaller MAE in MovieLens 10M.  #@NEW_LINE#@#  
Interestingly, our approach produces results that are almost identical to those of the unmixed, fully marginalized SBM (2) for the two examples for which inference with the SBM is feasible.  #@NEW_LINE#@#  In particular, we achieve the same accuracy with K=L=10 in the mixed-membership model as with 50 groups in the unmixed SBM.  #@NEW_LINE#@#  This result suggests that many of the groups observed in ref.  #@NEW_LINE#@#  2 are in fact mixtures of a smaller number of groups and that the additional expressiveness of the MMSBM allows us to succeed with a lower-dimensional model.  #@NEW_LINE#@#  Moreover, the fact that the maximum-likelihood estimator of the MMSBM gives results that are as accurate as those obtained by sampling over SBMs suggests that the mixing of memberships is an appropriate substitute for sampling over partitions of users and items.  #@NEW_LINE#@#  

The_MMSBM_Generalizes_Matrix_Factorization  #@NEW_LINE#@#  
MF is one of the most successful and popular approaches to collaborative filtering, both in its classical (12) and its probabilistic form (1317).  #@NEW_LINE#@#  However, as discussed, our MMSBM gives more accurate ratings, often by a large margin.  #@NEW_LINE#@#  Here, we propose an explanation for this improvement.  #@NEW_LINE#@#  
We start by giving an interpretation of MF as a special case of the MMSBM.  #@NEW_LINE#@#  In its simplest form, MF assumes that the expected rating that user u gives item i is r¯ui=ui, where u and i are K-dimensional vectors representing the user and the item, respectively.  #@NEW_LINE#@#  [One can apply a variety of noise models or loss functions, as well as regularization terms for the model parameters (12), but this does not significantly alter our discussion.]  #@NEW_LINE#@#  This can be interpreted as a mixed-membership model as follows: Assume that there are K=L groups of users and items, that uk is the probability that user u belongs to group k, and that ik is the probability that item i belongs to group k. Finally, assume that users in group k like only items in group k; in particular, users in k assign a baseline rating of 1 to items in group k and a rating of 0 to items in all other groups.  #@NEW_LINE#@#  Finally, let su0 and si0 be user and item intensities that correct for the fact that some users rate on average higher than others and that some items are generally more popular than others.  #@NEW_LINE#@#  Then the expected ratings are given byr¯ui=ksuuksiik.  #@NEW_LINE#@#  [7]  #@NEW_LINE#@#  
If we set uk=suuk and ik=siik, this becomes the MF model r¯ui=ui.  #@NEW_LINE#@#  Thus, MF corresponds to a model where there is a one-to-one correspondence between groups of users and groups of items, and users in a given group like only items in the corresponding group.  #@NEW_LINE#@#  If these assumptions do not hold, in general MF will not be able to properly model useritem ratings.  #@NEW_LINE#@#  
Our MMSBM relaxes these assumptions by allowing the distribution of ratings to be given by arbitrary matrices p. MF is roughly equivalent to assuming that pk is diagonal, at least for high ratings.  #@NEW_LINE#@#  We suggest that the improved performance of the MMSBM over MF is due to this greater expressive power.  #@NEW_LINE#@#  Indeed, Fig 2 shows that the matrices p inferred by our model are far from diagonal (see also Fig S4).  #@NEW_LINE#@#  
Moreover, the generality of the MMSBM allows it to account for many of the features of real ratings.  #@NEW_LINE#@#  For instance, different groups of users have different distributions of ratings: Users in group k=1 rate most movies with r=5, whereas those in k=7 often give ratings r=1.  #@NEW_LINE#@#  Similarly, movies in group =3 are consistently rated r=5 by most users, whereas movies in =9 are rated r=1 quite often.  #@NEW_LINE#@#  Interestingly, some groups of users agree on some movies but disagree on others: For example, users in groups k=9,10 agree that most movies in group =3 should be rated r=5, but they disagree on movies in =9, rating them r=1 and r=3, respectively.  #@NEW_LINE#@#  
One can also compare our MMSBM to MMMF, because both attempt to take the mixed-membership nature of users and items into account.  #@NEW_LINE#@#  However, the analogy is not perfect: MMMF models ratings as the sum of a MF term and a correction that uses mixed group memberships that are unrelated to the feature vectors (22).  #@NEW_LINE#@#  Although this is an improvement over MF, it does not fundamentally remove the assumption that each group of users has a corresponding group of items that it prefers.  #@NEW_LINE#@#  Indeed, our numerical results show that the performance of MMMF is fairly close to that of MF in the datasets we considered.  #@NEW_LINE#@#  

The_MMSBM_Makes_Well-Calibrated_Probabilistic_Predictions  #@NEW_LINE#@#  
Finally, our approach directly yields probabilistic predictions of the ratings, i.e., probability distributions on the discrete set S, and we can use the technique of frequentist calibration to see whether these predictions accurately capture the stochasticity of the data.  #@NEW_LINE#@#  Following ref.  #@NEW_LINE#@#  5, we perform two types of calibration experiments.  #@NEW_LINE#@#  Probabilistic calibration means that, for each rS and p[0,1], of the held-out pairs to which our approach assigns a rating of r with probability p, this is indeed the correct rating of a fraction p of them.  #@NEW_LINE#@#  (This differs slightly from ref.  #@NEW_LINE#@#  5, where a probabilistic forecaster predicts the cumulative distribution of a continuous variable, but it seems to be a reasonable definition for discrete values.)  #@NEW_LINE#@#  Marginal calibration means that for each rating rS, the average probability we assign to r coincides with its actual frequency among the held-out pairs.  #@NEW_LINE#@#  
As we show in Fig 3, the predictions of the MMSBM are indeed probabilistically and marginally well calibrated.  #@NEW_LINE#@#  Thus, in addition to giving accurate ratings in the sense of the MAE and the probability the rating is exactly correct, the MMSBM generates predictions that are statistically similar to real data, indicating that it captures the stochastic nature of the rating process.  #@NEW_LINE#@#  
Because MF and MMMF produce Gaussian distributions of real-valued ratings, to perform analogous calibration experiments we transform their predictions into a discrete probability distribution by integrating over the real numbers closest to each rS (Supporting Information).  #@NEW_LINE#@#  For instance, if S={1,2,3,4,5}, we define the probability that r=2 as the integral of this continuous distribution over the interval [1.5,2.5).  #@NEW_LINE#@#  Fig 3 shows that the resulting probabilistic predictions are not well calibrated, neither probabilistically nor marginally.  #@NEW_LINE#@#  One stark example of this is the MovieLens 10M dataset, where users use integer ratings much more often than half-integer ones.  #@NEW_LINE#@#  MF and MMMF cannot recognize this pattern and thus systematically underestimate and overestimate the probability of integer and half-integer ratings respectively.  #@NEW_LINE#@#  Similar, although less obvious, patterns cause MF and MMMF to be poorly calibrated in other datasets as well.  #@NEW_LINE#@#  Of course, one could attempt to infer a nonlinear mapping from continuous ratings to discrete ones, but this would increase the complexity of these models considerably.  #@NEW_LINE#@#  By treating each rating as a different label, the MMSBM adapts easily to the empirical distribution of ratings in each dataset.  #@NEW_LINE#@#  

The_MMSBM_Provides_a_Principled_Method_to_Deal_with_the_Cold_Start_Problem  #@NEW_LINE#@#  
Because the parameters of the MMSBM have a precise probabilistic interpretation, it can naturally deal with situations that are challenging for other algorithms.  #@NEW_LINE#@#  An example of this is the cold start problem, where we need to predict ratings for users or items for which we do not have training data (14, 23, 24).  #@NEW_LINE#@#  
In the MMSBM, the p matrices are the same for all users and items; in this sense, new users or items pose no particular difficulty.  #@NEW_LINE#@#  However, we have no information about their group membership vectors.  #@NEW_LINE#@#  In the absence of information about a new user n we can assume, a priori, that he or she belongs to each group to the same extent that a random existing user does.  #@NEW_LINE#@#  In practice, this means that we initially set his or her group membership vector to the average of the vectors of the observed users, nk=1Nuuk.  #@NEW_LINE#@#  We can treat i similarly for a new item i.  #@NEW_LINE#@#  This provides a principled method to deal with the cold start problem without additional elements (14).  #@NEW_LINE#@#  
In Fig 4 we show that, in cold start situations, the MMSBM outperforms the other algorithms in most cases.  #@NEW_LINE#@#  MMSBM is always more accurate than MF and MMMF (although in one case the difference is not significant).  #@NEW_LINE#@#  In all but one case, the MMSBM is also more accurate than an algorithm that assigns the most common rating to an item.  #@NEW_LINE#@#  In terms of mean absolute error, our approach is more accurate than MF and MMMF in four of five datasets (in one, not significantly) and more accurate than using the most common rating in four of five cases.  #@NEW_LINE#@#  
Note that none of these approaches takes metadata on users or items into account, which is a standard approach to the cold start problem.  #@NEW_LINE#@#  For instance, one could assume that a new user will behave similarly to others of the same age, gender, etc.  #@NEW_LINE#@#  (Fig S5), and compute the average membership vector over these users.  #@NEW_LINE#@#  We performed experiments restricting the average to users with same gender and/or age, but we found it did not significantly improve the performance (Fig S6).  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
We have shown that the MMSBM with its associated expectation-maximization algorithm is an accurate and scalable method to predict useritem ratings in a variety of contexts.  #@NEW_LINE#@#  It significantly outperforms other algorithms, including MF and MMMF, in most of the datasets we considered, both maximizing the probability that the predicted rating is exactly correct and minimizing the mean absolute error.  #@NEW_LINE#@#  
Additionally, because the model and its parameters are readily interpretable, it can be extended to (and performs well in) situations that are challenging for other approaches, such as a cold start where no prior information is available about a new user or item; one could also consider extensions of the model that take into account metadata for users (e.g., age and gender) and/or items (e.g., genre), analogous to unmixed stochastic block models with node metadata (25).  #@NEW_LINE#@#  
Finally, because the MMSBM assigns a probability to each possible rating, it is amenable to frequentist calibration, and we found that its predictions are in fact statistically similar to real data as measured by probabilistic and marginal calibration (5).  #@NEW_LINE#@#  We believe that this performance is due to the fact that the MMSBM is a more expressive generalization of matrix factorization, allowing each pair of user and item groups to have an arbitrary probability distribution of ratings.  #@NEW_LINE#@#  Matrix factorization is a widely used tool with many applications beyond recommendation; given our findings, it may make sense to use the MMSBM in those other applicationsas well.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
We maximize the likelihood 2 as a function of ,,p, using an EM algorithm.  #@NEW_LINE#@#  We start with a standard variational trick that changes the log of a sum into a sum of logs, writinglogP(RO|,,p)=(u,i)ROlogkukipk(rui)=(u,i)ROlogkui(k,)ukipk(rui)ui(k,)(u,i)ROkui(k,)logukipk(rui)ui(k,).  #@NEW_LINE#@#  [8]  #@NEW_LINE#@#  
Here ui(k,) is the estimated probability that a given ranking rui is due to u and i belonging to groups k and , respectively, and the lower bound in the third line is Jensens inequality logx¯logx¯.  #@NEW_LINE#@#  This lower bound holds with equality whenui(k,)=ukipk(rui)kukipk(rui),[9]giving us the update Eq.  #@NEW_LINE#@#  6 for the expectation step.  #@NEW_LINE#@#  
For the maximization step, we derive update equations for the parameters ,,p by taking derivatives of the log-likelihood Eq.  #@NEW_LINE#@#  8.  #@NEW_LINE#@#  Including Lagrange multipliers for the normalization constraints, we obtainuk=iului(k,)iukui(k,)=iului(k,)du,[10]i=uikui(k,)uikui(k,)=uikui(k,)di,[11]where du and di are the degrees of the user u and item i, respectively.  #@NEW_LINE#@#  Finally, including a Lagrange multiplier for the normalization constraints, we havepk(r)=(u,i)RO|rui=rui(k,)(u,i)ROui(k,).  #@NEW_LINE#@#  [12]  #@NEW_LINE#@#  

Scalability_with_the_Size_of_the_Dataset  #@NEW_LINE#@#  

Datasets  #@NEW_LINE#@#  
We perform experiments on six different datasets: the MovieLens 100K and 10M datasets, Yahoo!  #@NEW_LINE#@#  Songs, and the LibimSeTi.cz dating agency.  #@NEW_LINE#@#  We split the LibimSeTi.cz dataset into two datasets: women rating men (W-M) and men rating women (M-W).  #@NEW_LINE#@#  We neglected the links of women rating women and men rating men; unfortunately these links constituted only 1% of the dataset.  #@NEW_LINE#@#  In Table S1, we show the characteristics of each dataset in terms of the scale of ratings S, the total number of users, the total number of items, the number of ratings, and the average percentage of cold start cases.  #@NEW_LINE#@#  The MovieLens 100K dataset also provides demographic information for the users, namely the age in years and gender.  #@NEW_LINE#@#  

Benchmark_Algorithms  #@NEW_LINE#@#  
Naive_Model  #@NEW_LINE#@#  
As a baseline for comparison, we consider a naive model.  #@NEW_LINE#@#  Its prediction for a rating rui is simply the average of is observed ratings,rui=1diuirui,[S1]where i is the number of users that rate item i and di|i|.  #@NEW_LINE#@#  

ItemItem  #@NEW_LINE#@#  
The itemitem algorithm uses the cosine similarity between items, based on the N-dimensional vectors of ratings they have received, adjusted to remove user biases toward higher or lower ratings (21).  #@NEW_LINE#@#  The cosine similarity of items i and j is then cos(ri,rj)=uNriurju/(|ri|2|rj|2).  #@NEW_LINE#@#  The predicted rating rui is the similarity-weighted average of the k closest neighbors of i that user u has rated.  #@NEW_LINE#@#  We use the default, optimized implementation of the algorithm in LensKit (16) with k=50.  #@NEW_LINE#@#  

MF  #@NEW_LINE#@#  
One of the most widely used recommendation algorithms is MF (12, 26).  #@NEW_LINE#@#  Like in the block model, the intuition behind matrix factorization is that there should be some latent features that determine how a user rates an item.  #@NEW_LINE#@#  However, MF uses linear algebra to reduce the dimensionality of the problem.  #@NEW_LINE#@#  Specifically, it assumes that the matrix of ratings R (with N rows and M columns) is of rank k, in which case it can be written R=PQ, where P is an N×k matrix and Q is a k×M matrix.  #@NEW_LINE#@#  If we denote the rows of matrix P as pu and the columns of Q as qi, then user ratings are inner products rui=puqi.  #@NEW_LINE#@#  
We then assume that some noise and/or bias has been applied to R to produce the observed ratings RO.  #@NEW_LINE#@#  For example, some users rate items higher than others, and some items are systematically highly rated.  #@NEW_LINE#@#  To take this into consideration, the unobserved ratings rui are estimated usingrui(p,q,,b)=puqi++bu+bi,[S2]where bu and bi are the biases of users and items, respectively, and  is the average rating in RO.  #@NEW_LINE#@#  For the purpose of making recommendations, it is convenient to pose the decomposition problem as an optimization one; in particular, minimizing the 2 error and applying a regularization term gives{pu,qi}ss=argminpu,qi(u,i)RO[(ruipuqibubi)2+(pu2+qi2)],[S3]where  is a regularization parameter.  #@NEW_LINE#@#  As Funks originally proposed (12) one can solve this problem numerically, using stochastic gradient descent (27).  #@NEW_LINE#@#  We use the LensKit implementation of the algorithm, with k=50 and a learning rate of 0.002 as suggested in ref.  #@NEW_LINE#@#  16.  #@NEW_LINE#@#  
To perform the calibration analysis we need to compute the ratings probability distribution.  #@NEW_LINE#@#  Because in MF we obtain the probabilistic prediction by minimizing the sum of quadratic errors, it is equivalent to obtaining the maximum-likelihood estimators of the parameters when ratings are subject to a Gaussian noise,rui=rui(p,q,,b)+,[S4]where rui(p,q,,b) is the estimated rating in Eq.  #@NEW_LINE#@#  S2 and  is a random variable N(0,2).  #@NEW_LINE#@#  The deviation  can be estimated from the predicted data as=(u,i)R0(ruirealrui(p,q,,b))2|R0|.  #@NEW_LINE#@#  [S5]  #@NEW_LINE#@#  
In this case, the probability of each rating rS, such as S={1,2,3,4,5}, can be obtained by integrating the Gaussian N(rui(p,q,,b),2) in the binnings boundaries corresponding to S [the binnings boundaries in this example would be {(,1.5),[1.5,2.5),[2.5,3.5),[3.5,4.5),[4.5,)}, which are the same intervals that are used to compute the accuracy for rating predictions].  #@NEW_LINE#@#  

MMMF  #@NEW_LINE#@#  
The MMMF algorithm combines matrix factorization with mixed-membership context bias.  #@NEW_LINE#@#  In MMMF, users and items are endowed with both latent factor vectors (pu and qi) and discrete topic distribution parameters for users and items (ukUKU and ijMKM).  #@NEW_LINE#@#  Together with the user and item topics, MMMF models also define the affinity of user u to item topic k as cuk and the affinity of item i to user topic j as dij.  #@NEW_LINE#@#  The topic distribution parameters and the affinity of users and items to the topics jointly specify a context bias uijk.  #@NEW_LINE#@#  Therefore, a user generates a rating for an item by adding the contextual bias to the MF inner product with some Gaussian noise,ruiN(puqi+uijk,2).  #@NEW_LINE#@#  [S6]  #@NEW_LINE#@#  
In ref.  #@NEW_LINE#@#  22 the authors consider two different MMMF models that differ in how the contextual bias is built.  #@NEW_LINE#@#  The topic-indexed bias (TIB) model assumes that the contextual bias decomposes into a latent user bias and a latent item bias so that uijk=k=1KMcut(t)ikM(y)+j=1KUdij(t)ujU(t).  #@NEW_LINE#@#  The topic-indexed factor (TIF) model assumes that the joint contextual bias is an inner product of TIF vectors, so that uijk=k=1KMj=1KUikM(y)ujU(t)cuk(t)dij(t).  #@NEW_LINE#@#  They use a Gibbs sampling Markov chain Monte Carlo procedure to draw samples of topic and parameter variables.  #@NEW_LINE#@#  Then, the posterior mean prediction for each useritem pair under these MMMF models is1Tt=1T(pu(t)qi(t)+uijk).  #@NEW_LINE#@#  [S7]  #@NEW_LINE#@#  
We use their code implementation (https://code.google.com/archive/p/m3f/) with T=50 Gibbs iterations.  #@NEW_LINE#@#  The results shown in their paper are for the MMMF-TIF model, given that it outperform the MMMF-TIB model in all of the datasets.  #@NEW_LINE#@#  
The probability distribution of ratings for the MMMF would follow a similar procedure to that of MF.  #@NEW_LINE#@#  In this case, the probabilistic interpretation is raised directly form Eq.  #@NEW_LINE#@#  S6.  #@NEW_LINE#@#  The deviation  could also be computed from the predicted data.  #@NEW_LINE#@#  Then, the probability of each rating rS is obtained by integrating the Gaussian N(rui,2) in the corresponding binning boundaries for the ratings set S, where rui is the predicted rating from the MMMF.  #@NEW_LINE#@#  

SBM  #@NEW_LINE#@#  
The SBM (2830) assumes that the probability that two nodes form a link between them, such as a relationship between actors in a social network, depends on what groups they belong to.  #@NEW_LINE#@#  Analogously, the SBM recommender algorithm (2) assumes that the probability of a rating rui of a user u for an item i depends on the groups u, i to which they belong; unlike this paper, it assumes that each user or item belongs to a single group rather than a mixture.  #@NEW_LINE#@#  It uses a Bayesian approach that deals rigorously with the uncertainty associated with the models that could potentially account for the observed ratings.  #@NEW_LINE#@#  Mathematically, the problem is to estimate p(rui=r|RO) such that the unobserved rating of item i by user u is rui=r given the observable ratings RO.  #@NEW_LINE#@#  This is an integral over all possible block models M,p(rui=r|RO)=MdMp(rui=r|M)p(M|RO),[S8]where p(rui=r|M) is the probability that rui=r if the ratings were actually generated using model M, and p(M|RO) is the probability of model M given the observation (assuming for simplicity that all models M are equally likely a priori).  #@NEW_LINE#@#  This integral is over the continuous and discrete parameters of the block model.  #@NEW_LINE#@#  In particular, for each r and each pair of groups k, we integrate over the continuous parameters Pr[rui=r|u=k,i=]=pk(r); this part of the integral can be carried out analytically.  #@NEW_LINE#@#  However, the integral S8 also averages over all assignments  of groups to users and items; this expectation is estimated by MetropolisHastings sampling.  #@NEW_LINE#@#  Finally the prediction for each rating is the maximum-marginal estimate,rui=argmaxrpSBM(rui=r|RO).  #@NEW_LINE#@#  [S9]  #@NEW_LINE#@#  


Performance_as_a_Function_of_the_Number_of_Groups  #@NEW_LINE#@#  

Comparison_of_the_Predictions_of_the_Maximum_Likelihood_vs_the_Prediction_of_the_Average_over_all_of_the_Sampling  #@NEW_LINE#@#  
The solutions obtained from the sampling set of 500 independent initial conditions are different, but typically similar to each other as is shown in Fig S3 (Left column).  #@NEW_LINE#@#  Therefore, it is not clear that we can assign the solution with the maximum likelihood over all of the set as the best one.  #@NEW_LINE#@#  This is different from what one would expect from well-behaved physical systems, where typically one solution is much better than the others.  #@NEW_LINE#@#  As a result, we built our predictions by sampling from different maximum-likelihood solutions.  #@NEW_LINE#@#  Specifically, for each of these solutions we get a probability distribution of ratings for each useritem pair (u,i).  #@NEW_LINE#@#  The maximum-likelihood approach is taken as a final prediction for each pair (u,i), the probability distribution of ratings corresponding to the particular realization with maximum likelihood from all of the sampling, whereas for the average solution, the probability distribution of ratings for each pair (u,i) would be the average over all realizations in the sampling (we find that a simple average gives better predictions than a weighted average, where the weights are the likelihood of each realization).  #@NEW_LINE#@#  As is shown in Fig S3, we find that the best approach, in terms of both accuracy and MAE, is to average over all solutions rather than take only the solution with the maximum likelihood of all of the sampling set.  #@NEW_LINE#@#  

Top-Membership_Scores_Coverage_for_MF_and_MMSBM  #@NEW_LINE#@#  
As explained in the main text, both the MF and MMSBM consider mixing or membership vectors as main parameters in the model.  #@NEW_LINE#@#  Differences in the distributions obtained for such parameters could contribute to the differences in accuracy we observe.  #@NEW_LINE#@#  As a matter of fact, flat distributions of such parameters could mean that there are truly no strong group membership patterns and we would expect low performances.  #@NEW_LINE#@#  To investigate whether this could be the cause of the different performances of the MF and the MMMSBM, we compute the probability distributions of the membership vector scores for both MF and MMSBM algorithms.  #@NEW_LINE#@#  We define the MF membership vector as the normalized vectors of features pu/LU and qi/LI, where LU and LI are the largest feature values for all user/items vectors; and for the MMSBM we use the already normalized group membership vectors u and i.  #@NEW_LINE#@#  In Fig S4 we compare the score distributions of the membership vectors that represent each user/item with the distribution of the top-membership scores, that is, the distribution of the largest membership values from each membership vector.  #@NEW_LINE#@#  We found that, even though the top-membership distribution for the MF is apparently sharper than for the MMSBM, they present similar 95% coverage, that is, the fraction of all membership scores that fall into the 95% interval of the top-membership score distributions (95% coverage for MF, users 0.12, items 0.12; and for MMSBM, users 0.14, items 0.15).  #@NEW_LINE#@#  

Comparison_of_the_Social_Trends_Found_with_Different_Approaches  #@NEW_LINE#@#  
In a collaborative-filtering approach to recommendation, the assumption is that one can predict user affinities to query items based on the affinities of similar users to those items.  #@NEW_LINE#@#  Then, a plausible hypothesis is that similar users are likely to share some demographic characteristics such as gender or age.  #@NEW_LINE#@#  In particular, from the different user representations in each of the models in the main text we can investigate the social and psychological processes that determine user behaviors.  #@NEW_LINE#@#  To illustrate this idea, we analyze the user profiles in the MovieLens 100K dataset, which lists the age and gender of each user.  #@NEW_LINE#@#  
Specifically, we compare the user profiles of pairs of users (u,v) by computing the cosine similarity kukvk/(|u|2|v|2).  #@NEW_LINE#@#  Due to the expressiveness of the MMSBM, the user ratings profiles are represented directly by the users mixed-membership vectors.  #@NEW_LINE#@#  We can also measure the users cosine similarity with some of the benchmark algorithms in the main text such as the MF and the useruser version of the itemitem algorithm.  #@NEW_LINE#@#  For the MF approach, we use as the users profiles the users K-dimensional feature vectors.  #@NEW_LINE#@#  The users profiles in the useruser approach would be analogous to the items vector of ratings in the itemitem algorithm; that is, each user is represented by an M-dimensional vector where each entry is the rating value he or she gives to each movie or zero otherwise.  #@NEW_LINE#@#  
Fig S5 shows that independently of the model we use, when we divide users according to gender, pairs of male users have more similar profiles than pairs of female users or malefemale pairs.  #@NEW_LINE#@#  However, the different approaches differ when we combine gender and age to define user groups.  #@NEW_LINE#@#  Whereas our MMSBM and useruser approaches suggest pair similarities decrease with age, MF shows opposite results.  #@NEW_LINE#@#  

Users__Cold_Start_Approach_Using_Gender_and_Age_Similarities  #@NEW_LINE#@#  
Here we investigate whether the similarities found between certain classes of users (Fig S5) can be leveraged to provide better predictions in cold start situations.  #@NEW_LINE#@#  For our analysis we used the MovieLens 100K dataset, for which we know the gender and age of each user.  #@NEW_LINE#@#  We have no cold start users in the MovieLens 100K dataset.  #@NEW_LINE#@#  Nonetheless, we simulate cold start situations by removing all ratings of a number of users from the training set and making predictions on those users.  #@NEW_LINE#@#  In particular, we perform leave-one-out cross-validation experiments over 50 males and 50 females in the dataset.  #@NEW_LINE#@#  
Our approach in the main text was to assign to any new user n a group membership vector that is the average of the vectors of the observed users,nk=1Nuuk.  #@NEW_LINE#@#  [S10]  #@NEW_LINE#@#  
Here, we consider two approaches.  #@NEW_LINE#@#  First, we use two distinct group membership vectors, nkF for females and nkM for males:nkF=1NFu{F}uk,nkM=1NMu{M}uk.  #@NEW_LINE#@#  [S11]  #@NEW_LINE#@#  
Second, for each cold start user, we use a weighted average over observed users where the weight is the similarity between the queried user and the known users in terms of their age and gender,nk=1Nuuksim(n,u),[S12]where sim(n,u) is the average cosine similarity between membership vectors of all users in the agegender groups corresponding to users n and u (same groups as in Fig S5).  #@NEW_LINE#@#  As shown in Fig S6 we do not observe any significant improvement in performance when using either the gender-specific averages or the similarity-weighted averages.  #@NEW_LINE#@#  Therefore, we conclude that, despite there being significant correlations, the social trends found are not sufficient to significantly improve the predictions.  #@NEW_LINE#@#  

Acknowledgments  #@NEW_LINE#@#  
We thank C. Shalizi for helpful comments and suggestions.  #@NEW_LINE#@#  We also thank L. Mackey providing us his code and assistance for the MMMF algorithm.  #@NEW_LINE#@#  This work was supported by a James S. McDonnell Foundation Research Award (to R.G.  #@NEW_LINE#@#  and M.S.-P.), Spanish Ministerio de Economia y Comptetitividad Grants FIS2013-47532-C3 (to A.G.L., R.G., and M.S.-P.) and FIS2015-71563-ERC (to R.G.  #@NEW_LINE#@#  ), European Union Future and Emerging Technologies (FET) Grant 317532 [multilevel complex networks and systems (MULTIPLEX), to R.G.  #@NEW_LINE#@#  and M.S.-P.], the John Templeton Foundation (C.M.  #@NEW_LINE#@#  ), and the army research office (ARO) under Contract W911NF-12-R-0012 (to C.M.  #@NEW_LINE#@#  ).  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  




