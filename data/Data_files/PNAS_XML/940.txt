article id="http://dx.doi.org/10.1073/pnas.1703715114"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Motion microscopy for visualizing and quantifying small motions  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Humans have difficulty seeing small motions with amplitudes below a threshold.  #@NEW_LINE#@#  Although there are optical techniques to visualize small static physical features (e.g., microscopes), visualization of small dynamic motions is extremely difficult.  #@NEW_LINE#@#  Here, we introduce a visualization tool, the motion microscope, that makes it possible to see and understand important biological and physical modes of motion.  #@NEW_LINE#@#  The motion microscope amplifies motions in a captured video sequence by rerendering small motions to make them large enough to see and quantifies those motions for analysis.  #@NEW_LINE#@#  Amplification of these tiny motions involves careful noise analysis to avoid the amplification of spurious signals.  #@NEW_LINE#@#  In the representative examples presented in this study, the visualizations reveal important motions that are invisible to the naked eye.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Although the human visual system is remarkable at perceiving and interpreting motions, it has limited sensitivity, and we cannot see motions that are smaller than some threshold.  #@NEW_LINE#@#  Although difficult to visualize, tiny motions below this threshold are important and can reveal physical mechanisms, or be precursors to large motions in the case of mechanical failure.  #@NEW_LINE#@#  Here, we present a motion microscope, a computational tool that quantifies tiny motions in videos and then visualizes them by producing a new video in which the motions are made large enough to see.  #@NEW_LINE#@#  Three scientific visualizations are shown, spanning macroscopic to nanoscopic length scales.  #@NEW_LINE#@#  They are the resonant vibrations of a bridge demonstrating simultaneous spatial and temporal modal analysis, micrometer vibrations of a metamaterial demonstrating wave propagation through an elastic matrix with embedded resonating units, and nanometer motions of an extracellular tissue found in the inner ear demonstrating a mechanism of frequency separation in hearing.  #@NEW_LINE#@#  In these instances, the motion microscope uncovers hidden dynamics over a variety of length scales, leading to the discovery of previously unknown phenomena.  #@NEW_LINE#@#  

Increasing the phase of complex steerable pyramid coefficients results in approximate local motion of the basis functions.  #@NEW_LINE#@#  (A) A 1D slice of a complex steerable pyramid basis function.  #@NEW_LINE#@#  (B) The basis function is multiplied by several complex coefficients of constant amplitude and increasing phase to produce the real part of a new basis function that is approximately translating.  #@NEW_LINE#@#  Copyright (2016) Association for Computing Machinery, Inc. Reprinted with permission from ref.  #@NEW_LINE#@#  37.  #@NEW_LINE#@#  
A 1D example illustrating how the local phase of complex steerable pyramid coefficients is used to amplify the motion of a subtly translating step edge.  #@NEW_LINE#@#  (A) Frames (two shown) from the video.  #@NEW_LINE#@#  (B) Sample basis functions of the complex steerable pyramid.  #@NEW_LINE#@#  (C) Coefficients (one shown per frame) of the frames in the complex steerable pyramid representation.  #@NEW_LINE#@#  The phases of the resulting complex coefficients are computed.  #@NEW_LINE#@#  (D) The phase differences between corresponding coefficients are amplified.  #@NEW_LINE#@#  Only a coefficient corresponding to a single location and scale is shown; this processing is done to all coefficients.  #@NEW_LINE#@#  (E) The new coefficients are used to shift the basis functions.  #@NEW_LINE#@#  (F) A reconstructed video is produced by inverse transforming the complex steerable pyramid representation.  #@NEW_LINE#@#  The motion of the step edge is magnified.  #@NEW_LINE#@#  Copyright (2016) Association for Computing Machinery, Inc. Reprinted with permission from ref.  #@NEW_LINE#@#  37.  #@NEW_LINE#@#  
A comparison of our quantitative motion estimation vs. a laser vibrometer.  #@NEW_LINE#@#  Several videos of a cantilevered beam excited by a shaker were taken with varying focal length, exposure times, and excitation magnitude.  #@NEW_LINE#@#  The horizontal, lateral motion of the red point was also measured with a laser vibrometer.  #@NEW_LINE#@#  (A) A frame from one video.  #@NEW_LINE#@#  (B) The correlation between the two signals across the videos vs. root mean square (RMS) motion size in pixels (px).  #@NEW_LINE#@#  Only motions at the red point in A were used in our analysis.  #@NEW_LINE#@#  More results are in Fig S4.  #@NEW_LINE#@#  
Noise model of local phase.  #@NEW_LINE#@#  (A) A frame from a synthetic video with noise.  #@NEW_LINE#@#  (B) The real part of a single level of the complex steerable pyramid representation of A.  #@NEW_LINE#@#  (C) The imaginary part of the same level of the complex steerable pyramid representation of A.  #@NEW_LINE#@#  (D) A point cloud over noisy values of the real and imaginary values of the complex steerable pyramid representation at the red, green, and blue points in B.  #@NEW_LINE#@#  (E) The corresponding histogram of phases.  #@NEW_LINE#@#  
A comparison of our quantitative motion estimation vs. a laser vibrometer.  #@NEW_LINE#@#  Several videos of a cantilevered beam excited by a shaker were taken, with varying focal length, exposure times, and excitation magnitude.  #@NEW_LINE#@#  (A) A frame from one video.  #@NEW_LINE#@#  (B) Motions from the motion microscope at the red point are compared with the integrated velocities from a laser vibrometer.  #@NEW_LINE#@#  (C) B from 0.5 s to 1.5 s. (D) B from 11 s to 12 s. (E) The correlation between the two signals across the videos vs. the RMS motion size in pixels, measured by the laser vibrometer.  #@NEW_LINE#@#  (F) The correlation between the two signals across the videos vs. the RMS motion size in pixels measured by the laser vibrometer.  #@NEW_LINE#@#  (G) The correlation between the signals vs. focal length (exposure time, 490 s; excitation magnitude, 15).  #@NEW_LINE#@#  (H) Correlation vs. exposure time (focal length, 85 mm; excitation magnitude, 15).  #@NEW_LINE#@#  Cropped frames from the corresponding videos are shown above.  #@NEW_LINE#@#  (I) Correlation vs. relative excitation magnitude (focal length, 85 mm; exposure time, 490 s).  #@NEW_LINE#@#  Only motions at the red point in A were used in our analysis.  #@NEW_LINE#@#  
An evaluation of our motion estimation method and Ncorr (30) on a synthetic dataset of images.  #@NEW_LINE#@#  (A) Sample frames from real videos used to create the dataset.  #@NEW_LINE#@#  (B) Sample of synthetic motion fields of various motion size and spatial scale used to create the dataset.  #@NEW_LINE#@#  (C) The motion microscope and Ncorr are used to estimate the motion field, and the average relative error is displayed for both methods as a function of motion size and spatial scale.  #@NEW_LINE#@#  Both methods are only accurate for spatially smooth motion fields.  #@NEW_LINE#@#  Our method is twice as accurate for spatially smooth, subpixel motion fields.  #@NEW_LINE#@#  Ncorr is more accurate for larger motions.  #@NEW_LINE#@#  
Magnification of a spatial smooth and temporally filtered noise can look like a real signal.  #@NEW_LINE#@#  (A) Frames and time slices from a synthetic 300-frame video created by replicating a single frame 300 times and adding a different realistic noise pattern to each frame.  #@NEW_LINE#@#  (B) Corresponding frame and time slices from the synthetic video motion magnified 600× in a temporal band of 40 Hz to 60 Hz.  #@NEW_LINE#@#  Time slices from the same parts of each video are shown on the right for comparison.  #@NEW_LINE#@#  
Using a probabilistic simulation to compute the noise covariance of the motion estimate.  #@NEW_LINE#@#  (A) A single frame from an input video, in this case, of an elastic metamaterial.  #@NEW_LINE#@#  (B) Simulated, but realistic noise (contrast enhanced 80×).  #@NEW_LINE#@#  (C) Synthetic video with no motions, consisting of the input frame replicated plus simulated noise (noise contrast-enhanced 80×).  #@NEW_LINE#@#  (D) Estimated motions of this video.  #@NEW_LINE#@#  (E) Sample variances and sample covariance of the vertical and horizontal components of the motion are computed to give an estimate of how much noise is in the motion estimate.  #@NEW_LINE#@#  
Synthetic experiments showing that our noise covariance estimation, which assumes that the motions are zero, is also accurate for small nonzero motions.  #@NEW_LINE#@#  (A) The motion between synthetic frames with noise and slightly translated versions (not shown) are computed over 4,000 runs at the marked point in red for several different translation amounts.  #@NEW_LINE#@#  Each time, different but independent noise is added to the frames.  #@NEW_LINE#@#  (B) The sample covariance vs. motion size.  #@NEW_LINE#@#  (C) Relative error of horizontal variance vs. motion size.  #@NEW_LINE#@#  (D) Relative error of vertical variance vs. motion size.  #@NEW_LINE#@#  C and D are on the same color scale.  #@NEW_LINE#@#  
Validation of our noise estimation on real data.  #@NEW_LINE#@#  (A) A frame from a real video of an accelerometer attached to a beam.  #@NEW_LINE#@#  (B) The accelerometer shows there are no motions in the frequency band 600 Hz to 700 Hz.  #@NEW_LINE#@#  (C) The variance of our motion estimate in the 600- to 700-Hz band serves as a ground-truth measure of noise, as there are no motions.  #@NEW_LINE#@#  (D) The estimated noise level vs. intensity for a signal-dependent noise model and a constant noise model.  #@NEW_LINE#@#  (E) The noise estimate produced by our Monte Carlo simulation with a signal-dependent model.  #@NEW_LINE#@#  All variances are of the motions projected onto the direction of least variance.  #@NEW_LINE#@#  Textureless regions, where the motion estimation is not meaningful, have been masked out in black.  #@NEW_LINE#@#  (F) Difference in decibels between ground truth and E. (G) Noise estimate produced by the Monte Carlo simulation with a constant noise model.  #@NEW_LINE#@#  (H) Difference in decibels between ground truth and G.  #@NEW_LINE#@#  
Relation_Between_Local_Phase_Differences_and_Motions  #@NEW_LINE#@#  
Fleet and Jepson have shown that contours of constant phase in image subbands such as those in the complex steerable pyramid approximately track the motion of objects in a video (7).  #@NEW_LINE#@#  We make a similar phase constancy assumption, in which the following equation relates the phase of the frame at time 0 to the phase of future frames:r,(x,y,0)=r,(xu(x,y,t),yv(x,y,t),t),[S1]where (x,y,t):=(u(x,y,t),v(x,y,t)) is the motion we seek to compute.  #@NEW_LINE#@#  We Taylor-expand the right-hand side around (x,y) to getr,=(r,x,r,y)(u,v)+O(u2,v2),[S2]where r,(x,y,t):=r,(x,y,t)r,(x,y,0), arguments have been suppressed and O(u2,v2) represents higher order terms in the Taylor expansion.  #@NEW_LINE#@#  Because we assume the motions are small, higher order terms are negligible and the local phase variations are approximately equal to only the linear term,r,=(r,x,r,y)(u,v).  #@NEW_LINE#@#  [S3]  #@NEW_LINE#@#  
Fleet has shown that the spatial gradients of the local phase, (r,x,r,y), are roughly constant within a subband and that they are approximately equal to the peak tuning frequency of the corresponding subbands filter (33).  #@NEW_LINE#@#  This frequency is a 2D vector oriented orthogonal to the direction the subband selects for, which means that the local phase changes only provide information about the motions perpendicular to this direction.  #@NEW_LINE#@#  

Noise_Model_and_Creating_Synthetic_Video  #@NEW_LINE#@#  
We adopt a signal-dependent noise model, in which each pixel is contaminated with spatially independent Gaussian noise with variance f(I), where I is the pixels mean intensity (26, 34).  #@NEW_LINE#@#  Liu et al.  #@NEW_LINE#@#  (26) refer to this function f as a noise level function, and we do the same.  #@NEW_LINE#@#  This reflects that sensor noise is well modeled by the sum of zero-mean Gaussian noise sources, some of which have variances that depend on intensity (5).  #@NEW_LINE#@#  We show that this noise model is an improvement over a constant variance noise model in Fig S9.  #@NEW_LINE#@#  
The noise level function f is estimated from temporal variations in the input video, with observed intensities I(x,y,t).  #@NEW_LINE#@#  Assuming that I is the sum of noiseless intensity I0 and a zero-mean Gaussian noise term In with variance f(I0), the temporal variations are given by the following Taylor expansion:I(x,y,t)=I0(x,y,t)+In(x,y,t)[S17]=I0(xu(x,y,t),yv(x,y,t),0)+In(x,y,t)[S18]I0(x,y,0)I0xu(x,y,t)I0yv(x,y,t)+In(x,y,t).  #@NEW_LINE#@#  [S19]The second equality is the brightness constancy assumption of optical flow (35, 36).  #@NEW_LINE#@#  We exclude pixels where the spatial gradient (I0x,I0y) has high magnitude from our analysis.  #@NEW_LINE#@#  At the remaining pixel, temporal variations in I are mostly due to noise,I(x,y,t)I0(x,y,0)+In(x,y,t).  #@NEW_LINE#@#  [S20]At these pixels, we take the temporal variance and mean of I, which, in expectation, are f(I0) and I0, respectively.  #@NEW_LINE#@#  To increase robustness, we divide the intensity range into 64 equally sized bins.  #@NEW_LINE#@#  For each bin, we take all those pixels with mean inside that bin and take the mean of the corresponding temporal variances of I to estimate the noise level function f.  #@NEW_LINE#@#  
With f in hand, we can take frames from existing videos and use them to create simulated videos with realistic noise, but with known, zero motion.  #@NEW_LINE#@#  In Fig S6A, we take a frame I0(x,y,0) from a video of the metamaterial, filmed with a Phantom V-10, and add noise to it via the equationIS(x,y,t)=I0(x,y,0)+In(x,y,t)f(I0(x,y,0)),[S21]where In now is Gaussian noise with unit variance.  #@NEW_LINE#@#  We motion-magnify the resulting video 600 times in a 20-Hz band centered at 50 Hz to show that motion-magnified noise can cause spurious motions (Fig S6B).  #@NEW_LINE#@#  
We use the same simulation to create synthetic videos with which to estimate the covariance matrix of the motion vectors.  #@NEW_LINE#@#  
We quantify the noise in the motion vectors by estimating their covariance matrices (x,y).  #@NEW_LINE#@#  These matrices reflect variations in the motion caused by noise.  #@NEW_LINE#@#  It is not usually possible to directly estimate them from the input video, because both motions and noise vary across frames and the true motions are unknown.  #@NEW_LINE#@#  Therefore, we create a noisy, synthetic video IS(x,y,t) with known zero true motion (Eq.  #@NEW_LINE#@#  S21 and Fig S7 AC).  #@NEW_LINE#@#  
We estimate the motions in IS (Fig S7D) using our technique with spatial smoothing, but without temporal filtering, which we handle in a later step.  #@NEW_LINE#@#  This results in a set of 2D motion vectors S(x,y,t), in which all temporal variations in S are due to noise.  #@NEW_LINE#@#  The sample covariance matrix over the time dimension is=1N1t(S(x,y,t)¯S(x,y))(S(x,y,t)¯S(x,y))T,[S22]where ¯S(x,y) is the mean over t of the motion vectors.  #@NEW_LINE#@#  is a 2×2 symmetric matrix, defined at every pixel, with only three unique components.  #@NEW_LINE#@#  In Fig S7E, we show these components, the variances of the horizontal and vertical components of the motion, and their covariance.  #@NEW_LINE#@#  
The motion  projected onto a direction vector :=(cos(),sin()) is  and has variance V2()=T.  #@NEW_LINE#@#  Of particular interest is the direction  of least variance that minimizes V2().  #@NEW_LINE#@#  In the case of an edge in the image, the direction of least variance is usually normal to the edge.  #@NEW_LINE#@#  

Analytic_Justification_of_Noise_Analysis  #@NEW_LINE#@#  
We analyze only the case when the amplitudes at a pixel in all subbands are large (Ar,2g2), because the local phases have a Gaussian distribution in this case.  #@NEW_LINE#@#  Such points intuitively correspond to places where there is image content in at least two directions.  #@NEW_LINE#@#  In this case, we show that the sample covariance matrix computed using a simulated video with no motions is accurate for videos with subpixel small motions.  #@NEW_LINE#@#  
We reproduce the linearization of phase constancy equation (Eq.  #@NEW_LINE#@#  S3) with noise terms added to the phase variations (nt) and phase gradient (nx,ny),r,+nt=(u,v)(r,x+nx,r,y+ny).  #@NEW_LINE#@#  [S23]The total noise term in this equation is nt+unx+vny.  #@NEW_LINE#@#  The noise terms nt, nx and ny are of the same order of magnitude.  #@NEW_LINE#@#  Since u and v are much less than 1 pixel, the predominant source of noise is from nt, the effects of nx and ny are negligible, and we can ignore them, allowing us to write the noisy version of the equation asr,+nt=(u,v)(r,x,r,y).  #@NEW_LINE#@#  [S24]  #@NEW_LINE#@#  
The motion estimate  is the solution to a weighted least squares problem, =(T)1T.  #@NEW_LINE#@#  To simplify notation, let =(T)1T, the parts of the equation that dont depend on time.  #@NEW_LINE#@#  Then, the flow estimate is=.  #@NEW_LINE#@#  [S25]where the elements of  are the local phase variations over time.  #@NEW_LINE#@#  and  contain the spatial gradients of phase and amplitude, respectively.  #@NEW_LINE#@#  We have demonstrated that  is close to noiseless (Eq.  #@NEW_LINE#@#  S24), and our assumption about the amplitudes being large means  is also approximately noiseless, which means that  is noiseless.  #@NEW_LINE#@#  
We split  into the sum of its mean 0 and variance, a multivariate Gaussian random variable, denoted as n, that has zero mean and variance that depends only on image noise and local image content.  #@NEW_LINE#@#  Then, the flow estimate is=0True flow+nNoise Term(Covariance Matrix).  #@NEW_LINE#@#  [S26]The noise term doesnt depend on the value of the true flow 0.  #@NEW_LINE#@#  Therefore, the estimated covariance matrix is valid even when the motions are nonzero but small.  #@NEW_LINE#@#  
From Eq.  #@NEW_LINE#@#  S15, we know that n is proportional to the amount of sensor noise.  #@NEW_LINE#@#  Therefore, an interesting consequence of Eq.  #@NEW_LINE#@#  S26 is that the motion covariance matrix will be linearly proportional to the variance of the sensor noise.  #@NEW_LINE#@#  

Results_and_Discussion  #@NEW_LINE#@#  
We applied the motion microscope to several problems in biology and engineering.  #@NEW_LINE#@#  First, we used it to reveal one component of the mechanics of hearing.  #@NEW_LINE#@#  The mammalian cochlea is a remarkable sensor that can perform high-quality spectral analysis to discriminate as many as 30 frequencies in the interval of a semitone (8).  #@NEW_LINE#@#  These extraordinary properties of the hearing organ depend on traveling waves of motion that propagate along the cochlear spiral.  #@NEW_LINE#@#  These wave motions are coupled to the extremely sensitive sensory receptor cells via the tectorial membrane, a gelatinous structure that is 97% water (9).  #@NEW_LINE#@#  
To better understand the functional role of the tectorial membrane in hearing, we excised segments of the tectorial membrane from a mouse cochlea and stimulated it with audio frequency vibrations (Movie S1 and Fig 2A).  #@NEW_LINE#@#  Prior work suggested that motions of the tectorial membrane would rapidly decay with distance from the point of stimulation (10).  #@NEW_LINE#@#  The unprocessed video of the tectorial membrane appeared static, making it difficult to verify this.  #@NEW_LINE#@#  However, when the motions were amplified 20 times, waves that persisted over hundreds of micrometers were revealed (Movie S1 and Fig 2 BE).  #@NEW_LINE#@#  
Subpixel motion analysis suggests that these waves play a prominent role in determining the sensitivity and frequency selectivity of hearing (1114).  #@NEW_LINE#@#  Magnifying motions has provided new insights into the underlying physical mechanisms of hearing.  #@NEW_LINE#@#  Ultimately, the motion microscope could be applied to see and interpret the nanoscale motions of a multitude of biological systems.  #@NEW_LINE#@#  
We also applied the motion microscope to the field of modal analysis, in which a structures resonant frequencies and mode shapes are measured to characterize its dynamic behavior (15).  #@NEW_LINE#@#  Common applications are to validate finite element models and to detect changes or damage in structures (16).  #@NEW_LINE#@#  Typically, this is done by measuring vibrations at many different locations on the structure in response to a known input excitation.  #@NEW_LINE#@#  However, approximate measurements can be made under operational conditions assuming broadband excitation (17).  #@NEW_LINE#@#  Contact accelerometers have been traditionally used for modal analysis, but densely instrumenting a structure can be difficult and tedious, and, for light structures, the accelerometers mass can affect the measurement.  #@NEW_LINE#@#  
The motion microscope offers many advantages over traditional sensors.  #@NEW_LINE#@#  The structure is unaltered by the measurement, the measurements are spatially dense, and the motion-magnified video allows for easy interpretation of the motions.  #@NEW_LINE#@#  While only structural motions in the image plane are visible, this can be mitigated by choosing the viewpoint carefully.  #@NEW_LINE#@#  
We applied the motion microscope to modal analysis by filming the left span of a suspension bridge from 80 m away (Fig 3A).  #@NEW_LINE#@#  The central span was lowered and impacted the left span.  #@NEW_LINE#@#  Despite this, the left span looks completely still in the input video (Fig 3B).  #@NEW_LINE#@#  Two of its modal shapes are revealed in Movie S2 when magnified 400× (1.6 Hz to 1.8 Hz) and 250× (2.4 Hz to 2.7 Hz).  #@NEW_LINE#@#  In Fig 3 C and D, we show time slices from the motion-magnified videos, displacements versus time at three points, and the estimated noise standard deviations.  #@NEW_LINE#@#  We also used accelerometers to measure the motions of the bridge at two of those points (Fig 3B).  #@NEW_LINE#@#  The motion microscope matches the accelerometers within error bars.  #@NEW_LINE#@#  In a second example, we show the modal shapes of a pipe after it is struck with a hammer (Modal Shapes of a Pipe, Fig S10, and Movie S3).  #@NEW_LINE#@#  
In our final example, we used the motion microscope to verify the functioning of elastic metamaterials, artificially structured materials designed to manipulate and control the propagation of elastic waves.  #@NEW_LINE#@#  They have received much attention (18) because of both their rich physics and their potential applications, which include wave guiding (19), cloaking (20), acoustic imaging (21), and noise reduction (22).  #@NEW_LINE#@#  Several efforts have been made to experimentally characterize the elastic wave phenomena observed in these systems.  #@NEW_LINE#@#  However, as the small amplitude of the propagating waves makes it impossible to directly visualize them, the majority of the experimental investigations have focused on capturing the band gaps through the use of accelerometers, which only provide point measurements.  #@NEW_LINE#@#  Visualizing the mechanical motions everywhere in the metamaterials has only been possible using expensive and highly specialized setups like scanning laser vibrometers (23).  #@NEW_LINE#@#  
We focus on a metamaterial comprising an elastic matrix with embedded resonating units, which consists of copper cores connected to four elastic beams (24).  #@NEW_LINE#@#  Even when vibrated, this metamaterial appears stationary, making it difficult to determine if the metamaterial is functioning correctly (Movies S4 and S5).  #@NEW_LINE#@#  Previously, these miniscule vibrations were measured with two accelerometers (24).  #@NEW_LINE#@#  This method only provides point measurements, making it difficult to verify the successful attenuation of vibrations.  #@NEW_LINE#@#  We gain insight and understanding of the system by visually amplifying its motion.  #@NEW_LINE#@#  
The elastic metamaterial was forced at two frequencies, 50 Hz and 100 Hz, and, in each case, it was filmed at 500 frames per second (FPS) (Fig 4A).  #@NEW_LINE#@#  The motions in 20-Hz bands around the forcing frequencies were amplified, revealing that the metamaterial functions as expected (24), passing 50-Hz waves and rapidly attenuating 100-Hz waves (Movies S4 and S5).  #@NEW_LINE#@#  We also compared our results with predictions from a finite element analysis simulation (Fig 4 B and C).  #@NEW_LINE#@#  In Fig 4D, we show heatmaps of the estimated displacement amplitudes overlaid on the motion-magnified frames.  #@NEW_LINE#@#  We interpolated displacements into textureless regions, which had noisy motion estimates.  #@NEW_LINE#@#  The agreement between the simulation (Fig 4C) and the motion microscope (Fig 4D) demonstrates the motion microscopes usefulness in verifying the correct function of the metamaterial.  #@NEW_LINE#@#  

Modal_Shapes_of_a_Pipe  #@NEW_LINE#@#  
We made a measurement of a pipe being struck by a hammer, viewed end on by a camera, to capture its radialcircumferential vibration modes.  #@NEW_LINE#@#  A standard 4 schedule 40 PVC pipe was recorded with a high-speed camera at 24,000 FPS, at a resolution of 192 × 192 (Fig S10A).  #@NEW_LINE#@#  Fig S10 BF shows frames from the motion-magnified videos for different resonant frequencies showing the mode shapes, a comparison of the quantitatively measured mode shapes with the theoretically derived mode shapes, and the displacement vs. time of the specific frequency band and the estimated noise SD.  #@NEW_LINE#@#  The tiny modal motions are seen clearly.  #@NEW_LINE#@#  Obtaining vibration data with traditional sensors with the same spatial density would be extremely difficult, and accelerometers placed on the pipe would alter its resonant frequencies.  #@NEW_LINE#@#  
This sequence also demonstrates the accuracy of our noise analysis.  #@NEW_LINE#@#  The noise standard deviations show that the detected motions before impact, when the pipe is stationary, are likely spurious.  #@NEW_LINE#@#  

Synthetic_Validation  #@NEW_LINE#@#  
We validate the accuracy of our motion estimation on a synthetic dataset and compare its accuracy to Ncorr, a digital image correlation technique (30) used by mechanical engineers (31).  #@NEW_LINE#@#  In this experiment, we did not use temporal filtering.  #@NEW_LINE#@#  
We created a synthetic dataset of frame pairs with known ground-truth motions between them.  #@NEW_LINE#@#  We took natural images from the frames of real videos (Fig S5A) and warped them according to known motion fields using cubic b-spline interpolation (32).  #@NEW_LINE#@#  Sample motions fields, shown in Fig S5B, were produced by Gaussian-blurring IID Gaussian random variables.  #@NEW_LINE#@#  We used Gaussian blurs with SDs, ranging from zero (no filtering) to infinite (a constant motion field).  #@NEW_LINE#@#  We also varied the RMS amplitude of the motion fields from 0.001 pixels to three pixels.  #@NEW_LINE#@#  For each set of motion field parameters, we sampled five different motion fields to produce a total of 155 motion fields with different amplitudes and spatial coherence.  #@NEW_LINE#@#  To test the accuracy of the algorithms rather than their sensitivity to noise, no noise was added to the image pairs.  #@NEW_LINE#@#  
We ran our motion estimation technique and Ncorr on each image pair.  #@NEW_LINE#@#  We then computed the mean absolute difference between the estimated and ground-truth motion fields.  #@NEW_LINE#@#  Then, for each set of motion field parameters, we averaged the mean absolute differences across image pairs and divided the result by the RMS motion amplitude to make the errors comparable over motion sizes.  #@NEW_LINE#@#  The result is the average relative error as a percentage of RMS motion amplitude (Fig S5C).  #@NEW_LINE#@#  
Both Ncorr and our method perform best when the motions are spatially coherent (filter standard deviations greater than 10 pixels) with relative errors under 10%.  #@NEW_LINE#@#  This reflects the fact that both methods assume the motion field is spatially smooth.  #@NEW_LINE#@#  Across motion sizes, our method performs best for subpixel motions (5% relative error).  #@NEW_LINE#@#  This is probably because we assume that the motions are small when we linearize the phase constancy equation (Eq.  #@NEW_LINE#@#  S3).  #@NEW_LINE#@#  Ncorr has twice the relative error (10%) for the same motion fields.  #@NEW_LINE#@#  
The relative errors reported in Fig S5C are computed over all pixels, including those that are in smooth, textureless regions where it is difficult to estimate the motions.  #@NEW_LINE#@#  If we restrict the error metric to only take into account pixels at edges and corners, the average relative errors for small (less_than1 pixel RMS), spatially coherent (filter SD 10 pixels) motions drops by a factor of 2.5 for both methods.  #@NEW_LINE#@#  
We generated synthetic images that are slight translations of each other and added Gaussian noise to the frames (Fig S8A).  #@NEW_LINE#@#  For each translation amount, we compute the motion between the two frames over 4,000 runs.  #@NEW_LINE#@#  We compute the sample covariance matrix over the runs as a measure of the ground-truth noise level.  #@NEW_LINE#@#  We also used our noise analysis to estimate the covariance matrix at the points denoted in red.  #@NEW_LINE#@#  
The off-diagonal term of the covariance matrix should be zero for the synthetic frames in Fig S8A.  #@NEW_LINE#@#  For both examples, it is within 105 square pixels of zero for all translation amounts (Fig S8B).  #@NEW_LINE#@#  
The relative errors of the horizontal and vertical variances vs. translation (Fig S8 C and D) are less than 5% for subpixel motions.  #@NEW_LINE#@#  This is likely due to the random nature of the simulation.  #@NEW_LINE#@#  For motions greater than one pixel, the covariance matrix has a relative error of less than 25%.  #@NEW_LINE#@#  

Low-Amplitude_Coefficients_Have_Noisy_Phase  #@NEW_LINE#@#  
Each frame of the input video I(x,y,t) is transformed to the complex steerable pyramid representation by being spatially band-passed by a bank of quadrature pairs of filters gr, and hr,, where r corresponds to different spatial scales of the pyramid and  corresponds to different orientations.  #@NEW_LINE#@#  We use the filters of Portilla and Simoncelli, which are specified and applied in the frequency domain (25).  #@NEW_LINE#@#  For one such filter pair, the result is a set of complex coefficients Sr,+iTr, whose real and imaginary part are given bySr,=gr,IandTr,=hr,I,[S4]where the convolution is applied spatially at each time instant t. This filter pair is converted to amplitude Ar, and phase r, by the operationsAr,=Sr,2+Tr,2andr,=tan1(Tr,/Sr,).  #@NEW_LINE#@#  [S5]  #@NEW_LINE#@#  
Filters gr, and hr, are in quadrature relationship, which means that they select for the same frequencies, but are 90 degrees out of phase like sin and cos. A consequence is that they are uncorrelated and have equal RMS values.  #@NEW_LINE#@#  Complex coefficients at antipodal orientations are conjugate symmetric and contain redundant information.  #@NEW_LINE#@#  Therefore, we only use a half-circle of orientations.  #@NEW_LINE#@#  
Suppose the observed video I(x,y,t) is contaminated with IID noise In(x,y,t) of variance 2,I(x,y,t)=I0(x,y,t)+In(x,y,t),[S6]where I0(x,y,t) is the underlying noiseless video.  #@NEW_LINE#@#  This noise causes the complex steerable pyramid coefficients to be noisy, which causes the local phase to be noisy.  #@NEW_LINE#@#  We show that the local phase at a point has an approximate Gaussian distribution when the amplitude is high and is approximately uniformly distributed when the amplitude is low.  #@NEW_LINE#@#  
The transformed representation has responsegr,I0+gr,Inandhr,I0+hr,In.  #@NEW_LINE#@#  [S7]The first term in each expression is the noiseless filter response, which we denote S0,r,=gr,I0 for the real part and T0,r,=hr,I0 for the imaginary part.  #@NEW_LINE#@#  The second term in each expression is filtered noise, which we denote as Sn,r, and Tn,r,.  #@NEW_LINE#@#  At a single point, Sn,r, and Tn,r, are Gaussian random variables with covariance matrix equal to2(x,ygr,(x,y)2x,ygr,(x,y)hr,(x,y)x,ygr,(x,y)hr,(x,y)x,yhr,(x,y)2)=2x,ygr,(x,y)2I,[S8]where I is the identity matrix and equality follows from the fact that gr, and hr, are quadrature pairs.  #@NEW_LINE#@#  
We suppress the indices r, in this section for readability.  #@NEW_LINE#@#  From Eq.  #@NEW_LINE#@#  S5, the noiseless and noisy phases are given by0=tan1(T0/S0)and=tan1((T0+Tn)/(S0+Sn)).  #@NEW_LINE#@#  [S9]Their difference linearized around (S0,T0) istan1(T0+TnS0+Sn)tan1(T0S0)=SnS0TnT0A02+O(Sn2,SnTn,Tn2A04).  #@NEW_LINE#@#  [S10]The terms Sn2 and Tn2 are expected to be equal to their variance 2gr,(x,y)2.  #@NEW_LINE#@#  Therefore, if A022gr,(x,y)2, higher order terms are negligible.  #@NEW_LINE#@#  In this case, we see that the phase is approximately a linear combination of Gaussian random variables and is therefore Gaussian.  #@NEW_LINE#@#  This is illustrated empirically by local phase histograms of the green and blue points in Fig S3 AE.  #@NEW_LINE#@#  
For these high-amplitude points, we compute the variance of the phase of a coefficient,E[(tan1(T0+TnS0+Sn)tan1(T0S0))2][S11]E[(T0SnS0TnA02)2][S12]=E[T02Sn22T0S0SnTn+S02Tn2A04][S13]=2gr,2(T02+S02)A04[S14]=2gr,2A02.  #@NEW_LINE#@#  [S15]The first approximation follows from the linearization of Eq.  #@NEW_LINE#@#  S10.  #@NEW_LINE#@#  
When the amplitude is low compared with the noise level (A02less_thanless_than2gr,(x,y)2), the linearization of Eq.  #@NEW_LINE#@#  S10 is not accurate.  #@NEW_LINE#@#  In this case, S00 and T00, and phase is given bytan1(TnSn).  #@NEW_LINE#@#  [S16]Tn and Sn are uncorrelated Gaussian random variables with equal variance, which means that the phase is a uniformly random number.  #@NEW_LINE#@#  The phase at such points contains no information and intuitively corresponds to places where there is no image content in a given pyramid level (Fig S3E, red point).  #@NEW_LINE#@#  

Conclusion  #@NEW_LINE#@#  
Small motions can reveal important dynamics in a system under study, or can foreshadow large-scale motions to come.  #@NEW_LINE#@#  Motion microscopy facilitates their visualization, and has been demonstrated here for motion amplification factors from 20× to 400× across length scales ranging from 100 nm to 0.3 mm.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
Quantitative_Motion_Estimation  #@NEW_LINE#@#  
For every pixel at location (x,y) and time t, we combine spatial local phase information in different subbands of the frames of the input video using the least squares objective function,argminu,viAri,i2[(ri,ix,ri,iy)(u,v)ri,i]2.  #@NEW_LINE#@#  [2]Arguments have been suppressed for readability; Ari,i(x,y,t) and ri,i(x,y,t) are the spatial local amplitude and phase of a steerable pyramid representation of the image, and u(x,y,t) and v(x,y,t) are the horizontal and vertical motions, respectively, at every pixel.  #@NEW_LINE#@#  The solution (=(u,v)) is our motion estimate and is equal to=(T)1(T),[3]where  is N×2 with ith row (xri,i,yri,i),  is N×1 with ith row ri,i, and  is a diagonal N×N matrix with ith diagonal element Ari,i2.  #@NEW_LINE#@#  
To increase the signal-to-noise ratio, we assume the motion field is constant in a small window around each pixel.  #@NEW_LINE#@#  This gives additional constraints from neighboring pixels, weighted by both their amplitude squared and the corresponding value in a smoothing kernel K, to the objective described in Eq.  #@NEW_LINE#@#  3.  #@NEW_LINE#@#  To handle temporal filtering, we replace the local phase variations r,(x,y,t) with temporally filtered local phase variations.  #@NEW_LINE#@#  
We use a four-orientation complex steerable pyramid specified by Portilla and Simoncelli (25).  #@NEW_LINE#@#  We use only the two highest-frequency scales of the complex steerable pyramid, for a total of eight subbands.  #@NEW_LINE#@#  We use a Gaussian spatial smoothing kernel with a SD of 3 pixels and a support of 19×19 pixels.  #@NEW_LINE#@#  The temporal filter depends on the application.  #@NEW_LINE#@#  

Noise_Model_and_Creating_Synthetic_Video  #@NEW_LINE#@#  
We estimate the noise level function (26) of a video.  #@NEW_LINE#@#  We apply derivative of Gaussian filters to the image in the x and y directions and use them to compute the gradient magnitude.  #@NEW_LINE#@#  We exclude pixels where the gradient magnitude is above 0.05 on a 0 to 1 intensity scale.  #@NEW_LINE#@#  At the remaining pixels, we take the temporal variance and mean of the image.  #@NEW_LINE#@#  We divide the intensity range into 64 equally sized bins.  #@NEW_LINE#@#  For each bin, we take all pixels with mean inside that bin and take the mean of the corresponding temporal variances of I to form 64 points that are linearly interpolated to estimate the noise level function f.  #@NEW_LINE#@#  

Estimating_Covariance_Matrices_of_Motion_Vectors  #@NEW_LINE#@#  
For an input video I(x,y,t), we use the noise level function f to create a synthetic videoIS(x,y,t)=I0(x,y,0)+In(x,y,t)f(I0(x,y,0))[4]that is N frames long.  #@NEW_LINE#@#  We estimate the covariance matrices of the motion vectors by taking the temporal sample covariance of IS,=1N1t(S(x,y,t)¯S(x,y))(S(x,y,t)¯S(x,y))T,[5]where ¯S(x,y) is the mean over t of the motion vectors.  #@NEW_LINE#@#  
The temporal filter reduces noise and decreases the covariance matrix.  #@NEW_LINE#@#  Oppenheim and Schafer (27) show that a signal with independent and identically distributed (IID) noise of variance 2, when filtered with a filter with impulse response T(t), has variance tT(t)22.  #@NEW_LINE#@#  Therefore, when a temporal filter is used, we multiply the covariance matrix by tT(t)2.  #@NEW_LINE#@#  

Comparison_of_Our_Motion_Estimation_to_a_Laser_Vibrometer  #@NEW_LINE#@#  
We compare the results of our motion estimation algorithm to that of a laser vibrometer, which measures velocity using Doppler shift (28).  #@NEW_LINE#@#  In the first experiment, a cantilevered beam was shaken by a mechanical shaker at 7.3 Hz, 58.3 Hz, 128 Hz, and 264 Hz, the measured modal frequencies of the beam.  #@NEW_LINE#@#  The relative amplitude of the shaking signal was varied between a factor of 5 and 25 in 2.5 increments.  #@NEW_LINE#@#  We simultaneously recorded a 2,000 FPS video of the beam with a high-speed camera (VisionResearch Phantom V-10) and measured its horizontal velocity with a laser vibrometer (Polytec PDV100).  #@NEW_LINE#@#  We repeated this experiment for nine different excitation magnitudes, three focal lengths (24 mm, 50 mm, 85 mm) and eight exposure times (12.5 s, 25 s, 50 s, 100 s, 200 s, 300 s, 400 s, 490 s), for a total of 20 high-speed videos.  #@NEW_LINE#@#  The beam had an accelerometer mounted on it (white object in Fig 1A), but we did not use it in this experiment.  #@NEW_LINE#@#  
We used our motion estimation method to compute the horizontal displacement of the marked, red point on the left side of the accelerometer from the video (Fig 1A).  #@NEW_LINE#@#  We applied a temporal band-stop filter to remove motions between 67 Hz and 80 Hz that corresponded to camera motions caused by its cooling fans rotation.  #@NEW_LINE#@#  The laser vibrometer signal was integrated using discrete, trapezoidal integration.  #@NEW_LINE#@#  Before integration, both signals were high-passed above 2.5 Hz to reduce low-frequency noise in the integrated vibrometer signal.  #@NEW_LINE#@#  The motion signals from each video were manually aligned.  #@NEW_LINE#@#  For one video (exposure, 490 s; excitation, 25; and focal length, 85 mm), we plot the two motion signals (Fig S4 BD).  #@NEW_LINE#@#  They agree remarkably well, with higher modes well aligned and a correlation of 0.997.  #@NEW_LINE#@#  
To show the sensitivity of the motion microscope, we plot the correlation of our motion estimate and the integrated velocities from the laser vibrometer vs. motion size (RMS displacement).  #@NEW_LINE#@#  Because the motions average size varies over time, we divide each videos motion signal into eight equal pieces and plot the correlations of each piece in each video in Fig S4 E and F. For RMS displacements on the order of 1/100th of a pixel, the correlation between the two signals varies between 0.87 and 0.94.  #@NEW_LINE#@#  For motions larger than 1/20th of a pixel, the correlation is between 0.95 and 0.999.  #@NEW_LINE#@#  Possible sources of discrepancy are noise in the motion microscope signal, integrated low-frequency noise in the vibrometer signal, and slight misalignment between the signals.  #@NEW_LINE#@#  Displacements with RMS smaller than 1/100th of a pixel were noisier and had lower correlations, indicating that noise in the video prevents the two signals from matching.  #@NEW_LINE#@#  
As expected, correlation increases with focal length and excitation magnitude, two things that positively correlate with motion size (in pixels) (Fig S4 G and H).  #@NEW_LINE#@#  The correlation also increases with exposure, because videos with lower exposure times are noisier (Fig S4I).  #@NEW_LINE#@#  

Filming_Bridge_Sequence  #@NEW_LINE#@#  
The bridge was filmed with a monochrome Point Gray Grasshopper3 camera (model GS3-U3-23S6M-C) at 30 FPS with a resolution of 800×600.  #@NEW_LINE#@#  The central span of the bridge lifted to accommodate marine traffic.  #@NEW_LINE#@#  Filming was started about 5 s before the central span was lowered to its lowest point.  #@NEW_LINE#@#  
The accelerometer data were doubly integrated using trapezoidal integration to displacement.  #@NEW_LINE#@#  In Fig 3 C and D, both the motion microscope displacement and the doubly integrated acceleration were band-passed with a first-order band-pass Butterworth filter with the specified parameters.  #@NEW_LINE#@#  

Motion_Field_Interpolation  #@NEW_LINE#@#  
In textureless regions, it may not be possible to estimate the motion at all, and, at one-dimensional structures like edges, the motion field will only be accurate in the direction perpendicular to the edge.  #@NEW_LINE#@#  These inaccuracies are reflected in the motion covariance matrix.  #@NEW_LINE#@#  We show how to interpolate the motion field from accurate regions to inaccurate regions, assuming that adjacent pixels have similar motions.  #@NEW_LINE#@#  
We minimize the following objective function:(S()())1()(S()())T+SN()(S()S())(S()S())T,[6]where S is the desired interpolated field,  is the estimated motion field,  is its covariance, N() is the four-pixel neighborhood of x, and S is a user-specified constant that specifies the relative importance of matching the estimated motion field vs. making adjacent pixels have similar motion fields.  #@NEW_LINE#@#  The first term seeks to ensure that S is close to , weighted by the expected amount of noise at each pixel.  #@NEW_LINE#@#  The second term seeks to ensure that adjacent pixels have similar motion fields.  #@NEW_LINE#@#  
In Fig 4D, we produce the color overlays by applying the above processing to the estimated motion field with S=300 and then taking the amplitude of each motion vector.  #@NEW_LINE#@#  We also set components of the covariance matrix that were larger than 0.1 square pixels to be an arbitrarily large number (we used 10,000 square pixels).  #@NEW_LINE#@#  

Finite_Element_Analysis_of_Acoustic_Metamaterial  #@NEW_LINE#@#  
We use Abaqus/Standard (29), a commercial finite-element analyzer, to simulate the metamaterials response to forcing.  #@NEW_LINE#@#  We constructed a 2D model with 37,660 nodes and 11,809 eight-node plane strain quadrilateral elements (Abaqus element type CPE8H).  #@NEW_LINE#@#  We modeled the rubber as Neo-Hookean, with shear modulus 443.4 kPa, bulk modulus 7.39×105 kPa, and density 1,050kgm3 (Abaqus parameters C10=221.7 kPa, D1=2.71×109Pa1).  #@NEW_LINE#@#  We modeled the copper core with shear modulus 4.78×107kPa, bulk modulus 1.33×8kPa, and density 8,960kgm3 (Abaqus parameters C10=2.39×107kPa, D1=1.5×1011Pa1.  #@NEW_LINE#@#  Geometry and material properties are specified in Wang et al.  #@NEW_LINE#@#  (24).  #@NEW_LINE#@#  The bottom of the metamaterial was given a zero-displacement boundary condition.  #@NEW_LINE#@#  A sinusoidal displacement loading condition at the forcing frequency was applied to a node located halfway between the top and bottom of the metamaterial.  #@NEW_LINE#@#  

Validation_of_Noise_Analysis_with_Real_Video_Data  #@NEW_LINE#@#  
We took a video of an accelerometer attached to a beam (Fig S9A).  #@NEW_LINE#@#  We used the accelerometer to verify that the beam had no motions between 600 Hz and 700 Hz (Fig S9B).  #@NEW_LINE#@#  We then estimated the in-band motions from a video of the beam.  #@NEW_LINE#@#  Because the beam is stationary in this band, these motions are entirely due to noise, and their temporal sample covariance gives us a ground-truth measure of the noise level (Fig S9C).  #@NEW_LINE#@#  We used our simulation with a signal-dependent noise model to estimate the covariance matrix from the first frame of the video, the specific parameters of which are shown in Fig S9D.  #@NEW_LINE#@#  The resulting covariance matrices closely match the ground truth (Fig S9 E and F), showing that our simulation can accurately estimate noise level and error bars.  #@NEW_LINE#@#  
We also verify that the signal-dependent noise model performs better than the simpler constant variance noise model, in which noise is IID.  #@NEW_LINE#@#  The result of the constant noise model simulation produced results that are much less accurate than the signal-dependent noise model (Fig S9 G and H).  #@NEW_LINE#@#  
In Fig S9, we only show the component of the covariance matrix corresponding to the direction of least variance, and only at points corresponding to edges or corners.  #@NEW_LINE#@#  


Using_the_Motion_Microscope_and_Limitations  #@NEW_LINE#@#  
The motion microscope works best when the camera is stable, such as when mounted on a tripod.  #@NEW_LINE#@#  If the camera is handheld or the tripod is not sturdy, the motion microscope may only detect camera motions instead of subject motions.  #@NEW_LINE#@#  The easiest way to solve this problem is to stabilize the camera.  #@NEW_LINE#@#  If the camera motions are caused by some periodic source, such as camera fans, that occur at a temporal frequency that the subject is not moving at, they can be removed with a band-stop filter.  #@NEW_LINE#@#  
If adjacent individual pixels have different motions, it is unlikely that the motion microscope will be able to discern or differentiate them.  #@NEW_LINE#@#  The motion microscope assumes motions are locally constant, and it wont be able to properly process videos with high spatial frequency motions.  #@NEW_LINE#@#  The best way to solve this problem is to increase the resolving power or optical zoom of the imaging system.  #@NEW_LINE#@#  If noise is not a concern, reducing the width of the spatial Gaussian used to smooth the motions may also help.  #@NEW_LINE#@#  
It is not possible for the motion microscope to quantify the movement of textureless regions or the full 2D movement of 1D edges.  #@NEW_LINE#@#  However, the motion covariance matrices produced by the motion microscope will alert the user as to when this is happening.  #@NEW_LINE#@#  The visualization will still be reasonable, as it is not possible to tell if a textureless region was motion-amplified in the wrong way.  #@NEW_LINE#@#  
Saturated, pure white pixels may pose a problem for the motion microscope.  #@NEW_LINE#@#  Slight color changes are the underlying signal used to detect tiny motions.  #@NEW_LINE#@#  For pixels with clipped intensities, this signal will be missing, and it may not be possible to detect motion.  #@NEW_LINE#@#  The best ways to mitigate this problem are to reduce the exposure time, change the position of lights, or use a camera with a higher dynamic range.  #@NEW_LINE#@#  
The motion microscope only quantifies lateral, 2D motions in the imaging plane.  #@NEW_LINE#@#  Motions of the subject toward or away from the camera will not be properly reported.  #@NEW_LINE#@#  For a reasonable camera, these 2D motions can be multiplied by a constant to convert from pixels to millimeters or other units of interest.  #@NEW_LINE#@#  However, if the lens has severe distortion, the constant will vary across the scene.  #@NEW_LINE#@#  If quantification of the motions is important, the images will need to be lens distortion-corrected before the motion microscope is applied.  #@NEW_LINE#@#  
The motion microscope visualization is unable to push pixels beyond the support of the basis functions of the steerable pyramid.  #@NEW_LINE#@#  In practice, this results in a maximum amplified motion of around four pixels.  #@NEW_LINE#@#  If the amplification is high enough to push an image feature past this, ringing artifacts may occur.  #@NEW_LINE#@#  

Acknowledgments  #@NEW_LINE#@#  
We thank Professor Erin Bell and Travis Adams at University of New Hampshire and New Hampshire Department of Transportation for their assistance with filming the Portsmouth lift bridge.  #@NEW_LINE#@#  This work was supported, in part, by Shell Research, Quanta Computer, National Science Foundation Grants CGV-1111415 and CGV-1122374, and National Institutes of Health Grant R01-DC00238.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  

This is an open access article distributed under the PNAS license.  #@NEW_LINE#@#  

