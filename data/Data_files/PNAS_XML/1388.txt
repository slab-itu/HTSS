article id="http://dx.doi.org/10.1073/pnas.1804420115"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Convolutional neural networks automate detection for tracking of submicron-scale particles in 2D and 3D  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
The increasing availability of powerful light microscopes capable of collecting terabytes of high-resolution 2D and 3D videos in a single day has created a great demand for automated image analysis tools.  #@NEW_LINE#@#  Tracking the movement of nanometer-scale particles (e.g., virus, proteins, and synthetic drug particles) is critical for understanding how pathogens breach mucosal barriers and for the design of new drug therapies.  #@NEW_LINE#@#  Our advancement is to use an artificial neural network that provides, first and foremost, substantially improved automation.  #@NEW_LINE#@#  Additionally, our method improves accuracy compared with current methods and reproducibility across users and laboratories.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Particle tracking is a powerful biophysical tool that requires conversion of large video files into position time series, i.e., traces of the species of interest for data analysis.  #@NEW_LINE#@#  Current tracking methods, based on a limited set of input parameters to identify bright objects, are ill-equipped to handle the spectrum of spatiotemporal heterogeneity and poor signal-to-noise ratios typically presented by submicron species in complex biological environments.  #@NEW_LINE#@#  Extensive user involvement is frequently necessary to optimize and execute tracking methods, which is not only inefficient but introduces user bias.  #@NEW_LINE#@#  To develop a fully automated tracking method, we developed a convolutional neural network for particle localization from image data, comprising over 6,000 parameters, and used machine learning techniques to train the network on a diverse portfolio of video conditions.  #@NEW_LINE#@#  The neural network tracker provides unprecedented automation and accuracy, with exceptionally low false positive and false negative rates on both 2D and 3D simulated videos and 2D experimental videos of difficult-to-track species.  #@NEW_LINE#@#  

Sample frames from experimental videos, highlighting some of the challenging conditions for particle tracking.  #@NEW_LINE#@#  (Left to Right) Fifty-nanometer particles captured at low SNR and 200-nm particles with diffraction disc patterns, variable background intensity, and ellipsoid PSF shapes from 12m Salmonella.  #@NEW_LINE#@#  
Materials_and_Methods  #@NEW_LINE#@#  
To train the network on a wide range of video conditions, we developed video simulation software that accounts for a large range of conditions found in particle tracking videos (Fig 1).  #@NEW_LINE#@#  The primary advance is to include simulations of how particles moving in three dimensions appear in a 2D image slice captured by the camera.  #@NEW_LINE#@#  
A standard camera produces images that are typically single channel (gray scale), and the image data are collected into 4D (three space and one time dimension) arrays of 16-bit integers.  #@NEW_LINE#@#  The resolution in the (x,y) plane is dictated by the camera and can be in the megapixel range.  #@NEW_LINE#@#  The resolution in the z coordinate is much smaller, since each z-axis slice imaged by the camera requires a piezoelectric motor to move the lense relative to the sample.  #@NEW_LINE#@#  A good piezoelectric motor is capable of moving between z-axis slices within a few milliseconds, which means that there is a tradeoff between more z slices and the overall frame rate.  #@NEW_LINE#@#  For particle tracking, a typical video includes 10 to 50 z slices per volume.  #@NEW_LINE#@#  The length of the video refers to the number of time points, i.e., the number of volumes collected.  #@NEW_LINE#@#  Video length is often limited by photobleaching, which slowly lowers the SNR as the video progresses.  #@NEW_LINE#@#  
To simulate a particle tracking video, we must first specify how particles appear in an image.  #@NEW_LINE#@#  We refer to the pixel intensities captured by a microscope and camera resulting from a particle centered at a given position (x,y,z) as the observed point spread function (PSF), denoted by ijk(x,y,z), where i,j,andk are the pixel indices.  #@NEW_LINE#@#  The PSF becomes dimmer and less focused as the particle moves away from the plane of focus (z=0).  #@NEW_LINE#@#  Away from the plane of focus, the PSF also develops disc patterns caused by diffraction, which can be worsened by spherical aberration.  #@NEW_LINE#@#  While deconvolution can mitigate the disc patterns appearing in the PSF, the precise shape of the PSF must be known or unpredictable artifacts may be introduced into the image.  #@NEW_LINE#@#  
The shape of the PSF depends on several parameters that vary depending on the microscope and camera, including emitted light wavelength, numerical aperture, pixel size, and the separation between z-axis slices.  #@NEW_LINE#@#  While physical models based on optical physics that expose these parameters have been developed for colloidal spheres (27), it is not practical for the purpose of automated particle tracking within complex biological environments.  #@NEW_LINE#@#  In practice, there are many additional factors that affect the PSF, such as the refractive index of the glass slide, of the lens oil (if oil-immersion objective is used), and of the medium containing the particles being imaged.  #@NEW_LINE#@#  The latter presents the greatest difficulty, since biological specimens are often heterogeneous and their optical properties are difficult to predict.  #@NEW_LINE#@#  The PSF can also be affected by particle velocity, depending on the duration of the exposure interval used by the camera.  #@NEW_LINE#@#  This makes machine learning particularly appealing, because we can simply randomize the shape of the PSF to cover a wide range of conditions, and the resulting CNN is capable of automatically deconvolving PSFs without the need to know any of the aforementioned parameters.  #@NEW_LINE#@#  
Low SNR is an additional challenge for tracking of submicron-size particles.  #@NEW_LINE#@#  High-performance digital cameras are used to record images at a sufficiently high frame rate to resolve statistical features of particle motion.  #@NEW_LINE#@#  Particles with a hydrodynamic radius in the range of 10 nm to 100 nm move quickly, requiring a small exposure time to minimize dynamic localization error (motion blur) (28).  #@NEW_LINE#@#  Smaller particles also emit less light for the camera to collect.  #@NEW_LINE#@#  To train the neural network to perform in these conditions, we add Poisson shot noise with random intensity to the training videos.  #@NEW_LINE#@#  We also add slowly varying random background patterns (SI Appendix and Fig 2).  #@NEW_LINE#@#  
An_Artificial_Neural_Network_for_Particle_Localization  #@NEW_LINE#@#  
The neurons of the artificial neural network are arranged in layers, which operate on multidimensional arrays of data.  #@NEW_LINE#@#  Each layer output is 3D, with two spatial dimensions and an additional feature dimension (Fig 2).  #@NEW_LINE#@#  Each feature within a layer is tuned to respond to specific patterns, and the ensemble of features is sampled as input to the next layer to form features that recognize more-complex patterns.  #@NEW_LINE#@#  For example, the lowest layer comprises features that detect edges of varying orientation, and the second-layer features are tuned to recognize curved lines and circular shapes.  #@NEW_LINE#@#  
Each neuron in the network processes information from spatially local inputs (either pixels of the input image or lower-layer neurons).  #@NEW_LINE#@#  This enables a neuron to see a local patch of the input image.  #@NEW_LINE#@#  The size of the image patch that affects the input to a given neuron is called its receptive field.  #@NEW_LINE#@#  The relationship of the input and output, denoted by Iij and Oij, for each neuron is given by Oij=F(i,jwi,jIi+i,j+jb), where the kernel weights wij and output bias b are trainable parameters.  #@NEW_LINE#@#  Each layer has its own set of biases, one for each feature, and each feature has its own set of kernel weights, one for each feature in the layer directly below.  #@NEW_LINE#@#  The nonlinearity F() is a prespecified function that determines the degree of activation or output; we use F(u)=log(eu+1).  #@NEW_LINE#@#  Inserting nonlinearity in between each layer of neurons is necessary for CNNs to robustly approximate nonlinear functions.  #@NEW_LINE#@#  The most common choice is called the rectified linear unit [F(u0)=u and F(uless_than0)=0].  #@NEW_LINE#@#  Instead, we use a function with a similar shape that is also continuously differentiable, which helps minimize training iterations where the model is stuck in local minima (29).  #@NEW_LINE#@#  
The neural network comprises three layers: 12 features in layer one, 32 features in layer two, and the final two output features in layer three.  #@NEW_LINE#@#  The output of the neural net, denoted by qijk, can be interpreted as the probability of a particle centered at pixel (i,j,k).  #@NEW_LINE#@#  We refer to these as detection probabilities.  #@NEW_LINE#@#  
While it is possible to construct a network that takes 3D image data as input, it is not computationally efficient.  #@NEW_LINE#@#  Instead, the network is designed to process a single 2D image slice at a time (so that it can also be applied to the large set of existing 2D imaging data) while still maintaining the ability to perform 3D tracking.  #@NEW_LINE#@#  Constructing 3D output qijk is achieved by applying the network to each z-axis slice of the input image, the same way a microscope obtains 3D images by sequentially capturing each z-axis slice.  #@NEW_LINE#@#  Two- or three-dimensional paths can then be reconstructed from the network output as described in Particle Path Linking.  #@NEW_LINE#@#  
We designed our network to be recurrent in time so that past and future observations are used to predict particle locations.  #@NEW_LINE#@#  In particular, we use the forwardbackward algorithm (30) to improve accuracy.  #@NEW_LINE#@#  Because detections include information from the past and future, the detection probabilities are reduced when a particle is not detected in the previous frame (the particle just appeared in the current frame) or is not detected in the following frame (the particle is about to leave the plane of focus).  #@NEW_LINE#@#  In Particle Path Linking, we show how the detection probabilities can be used by the linking algorithm to improve its performance.  #@NEW_LINE#@#  

Optimizing_the_Neural_Network_Parameters  #@NEW_LINE#@#  
The values of the trainable parameters in the network, including the kernel weights and biases, are optimized through the process of learning.  #@NEW_LINE#@#  Using known physical models of particle motion and imaging, we simulate random particle paths and image frames that cover a wide range of conditions, including particle PSF shape, variable background, particle number, particle mobility, and SNR.  #@NEW_LINE#@#  The ground truth for each image consists of a binary image with pixels values pijk=1 if (j,i,k)xnless_than2, and pijk=0 otherwise.  #@NEW_LINE#@#  Each training image is processed by the neural net, and the corresponding output is compared with the ground truth using the cross-entropy error H[p,q]=1Nijkpijklogqijk+(1pijk)log(1qijk), where N is the total number of pixels in the image.  #@NEW_LINE#@#  Further details can be found in SI Appendix.  #@NEW_LINE#@#  

Particle_Path_Linking  #@NEW_LINE#@#  
The dynamics of particle motion can vary depending on the properties of the surrounding fluid and the presence of active forces (e.g., flagellar-mediated swimming of bacteria and molecular motor cargo transport).  #@NEW_LINE#@#  To reconstruct accurate paths from a wide range of movement characteristics, we develop a minimal model.  #@NEW_LINE#@#  A minimal assumption for tracking is that the observation sequence approximates continuous motion of an object.  #@NEW_LINE#@#  To accurately capture continuous motion sampled at discrete time intervals, dictated by the camera frame rate, the particle motion must be sufficiently small between image frames.  #@NEW_LINE#@#  Hence, we assume only that particles move within a Gaussian range from one frame to the next.  #@NEW_LINE#@#  Further details can be found in SI Appendix.  #@NEW_LINE#@#  

Performance_Evaluation_and_Comparison_with_Existing_Software  #@NEW_LINE#@#  
We consider the primary goal for a high-fidelity tracker to be accuracy (i.e., minimize false positives and localization error), followed by the secondary goal of maximizing data extraction (i.e., minimize false negatives and maximize path length).  #@NEW_LINE#@#  To gauge accuracy, particle positions were matched to ground truth using optimal linear assignment.  #@NEW_LINE#@#  The algorithm finds the closest match between tracked and ground truth particle positions that are within a preset distance of five pixels; this is well above the subpixel error threshold of one pixel, but sufficiently small to ensure one-to-one matching.  #@NEW_LINE#@#  Tracked particles that did not match any ground truth particles were deemed false positives, and ground truth particles that did not match a tracked particle were deemed false negatives.  #@NEW_LINE#@#  To assess the performance of the NN, we analyzed the same videos using three different leading tracking software programs that are publicly available: Mosaic (Mos), an ImageJ plug-in capable of automated tracking in two and three dimensions (31); Icy, an open source bioimaging platform with preinstalled plugins capable of automated tracking in two and three dimensions (32, 33); and Video Spot Tracker (VST), a stand-alone application capable of 2D particle tracking, developed by the Center for Computer-Integrated Systems for Microscopy and Manipulation at University of North Carolina at Chapel Hill.  #@NEW_LINE#@#  VST also has a convenient graphic user interface that allows a user to add or eliminate paths (because human-assisted tracking is time-consuming, 100 2D videos were randomly selected from the 500-video set).  #@NEW_LINE#@#  
For the sake of visual illustration, we supplement our quantitative testing with a small sample of real and synthetic videos with localization indicators (see Movies S1 and S2).  #@NEW_LINE#@#  In each video, red diamond centers indicate each localization from the neural network.  #@NEW_LINE#@#  

Performance_on_Simulated_2D_and_3D_Videos  #@NEW_LINE#@#  
Because manual tracking by humans is subjective, our first standard for evaluating the performance of the NN and other publicly available software is to test on simulated videos, for which the ground truth particle paths are known.  #@NEW_LINE#@#  The test included 500 2D videos and 50 3D videos, generated using the video simulation methodology described in Materials and Methods and SI Appendix.  #@NEW_LINE#@#  Each 2D video contained 100 simulated particle paths for 50 frames at 512 × 512 resolution (see SI Appendix, Fig S2).  #@NEW_LINE#@#  Each 3D video contained 20 evenly spaced z-axis image slices of a 512 × 512 × 120 pixel region containing 300 particles.  #@NEW_LINE#@#  The conditions for each video were randomized, including variable background intensity, PSF radius (called particle radius for convenience), diffusivity, and SNR.  #@NEW_LINE#@#  Note that SNR is defined as the mean pixel intensity contributed by the particle PSFs divided by the SD of the background pixel intensities.  #@NEW_LINE#@#  
To assess the robustness of each tracking method/software program, we used the same set of tracker parameters for all videos (see SI Appendix for further details).  #@NEW_LINE#@#  Scatter plots of the 2D test video results for NN, Mosaic, and Icy are shown in Fig 3.  #@NEW_LINE#@#  For Mosaic, the false positive rate was generally quite low (2%) when SNR3, but showed a marked increase to 20% for SNRless_than3 (Fig 3A).  #@NEW_LINE#@#  The average false negative rates were in excess of 50% across most SNR3 (Fig 3B).  #@NEW_LINE#@#  In comparison, Icy possessed higher false positive rates than Mosaic at high SNR and lower false positive rates when SNR decreases below 2.5, with a consistent 5% false positive rate across all SNR values (Fig 3A).  #@NEW_LINE#@#  The false negative rates for Icy were greater than for Mosaic at high SNR, and exceeded 40% for all SNR tested (Fig 3B).  #@NEW_LINE#@#  
All three methods showed some minor sensitivity, in the false positive rate and localization error, to the PSF radius (Fig 3 E and G).  #@NEW_LINE#@#  (Note that the high sensitivity Mosaic displayed to changes in SNR made the trend for PSF radius difficult to discern.)  #@NEW_LINE#@#  Mosaic and Icy showed much higher sensitivity, in the false negative rate, to PSF radius, each extracting nearly fourfold more particles as the PSF radius decreased from eight to two pixels (Fig 3F).  #@NEW_LINE#@#  
One common method to analyze and compare particle tracking data is the ensemble mean squared displacement (MSD) calculated from particle traces.  #@NEW_LINE#@#  Since the simulated paths in the 2D and 3D test videos were all Brownian motion (with randomized diffusivity), we have that |x(t)|2=4Dt, where D is the diffusivity.  #@NEW_LINE#@#  To make a simple MSD comparison for Brownian paths, we computed estimated diffusivities using the MSD at the path duration 1less_thanT50, with D|x(T)|2/(4T).  #@NEW_LINE#@#  (See below and SI Appendix, Fig S4 for an MSD analysis on experimental videos of particle motion in mucus.)  #@NEW_LINE#@#  When estimating diffusivities, Icy exhibited increased false positive rates with faster-moving particles (Fig 3D), likely due to the linker compensating for errors made by the detection algorithm.  #@NEW_LINE#@#  In other words, while the linker was able to correctly connect less-mobile particles without confusing them with nearby false detections, when the diffusivity rose, the particle displacements tended to be larger than the distance to the nearest false detection.  #@NEW_LINE#@#  Consequently, when D2, the increased false positives along with increased increment displacements caused Icy to underestimate the diffusivity (Fig 3H), because paths increasingly incorporated false positives.  #@NEW_LINE#@#  
In contrast to Mosaic and Icy, the NN possessed a far lower mean false positive rate of 0.5% across all SNR values tested (Fig 3A).  #@NEW_LINE#@#  The NN was able to achieve this level of accuracy while extracting a large number of paths, with less_than20% false negative rate for all SNR2.5 and only a modest increase in the false negative rate at lower SNR (Fig 3B).  #@NEW_LINE#@#  Importantly, the NN performed well under low-SNR conditions by making fewer predictions, and the number of predictions made per frame is generally in reasonable agreement with the theoretical maximum (Fig 3C).  #@NEW_LINE#@#  Since the neural network was trained to recognize a wide range of PSFs, it also maintained excellent performance (less_than1% false positive, less_than20% false negative) across the range of PSF radius (Fig 3F).  #@NEW_LINE#@#  The NN possessed localization error that was as good as that of Mosaic and Ivy, less than one pixel on average and never more than two pixels, even though true positives were allowed to be as far as five pixels apart (Fig 3G).  #@NEW_LINE#@#  
When analyzing 3D videos, Mosaic and Icy were able to maintain false positive rates (5 to 8%) roughly comparable to their rates when analyzing 2D videos (Fig 3I).  #@NEW_LINE#@#  Surprisingly, analyzing 3D videos with the NN resulted in an even lower false positive rate than for 2D videos, with 0.2% false positives.  #@NEW_LINE#@#  All three methods capable of 3D tracking exhibited substantial improvements in reducing false negatives, reducing localization error, and increasing path duration (Fig 3 J and L).  #@NEW_LINE#@#  Strikingly, the neural network was able to correctly identify an average of 95% of the simulated particles in a 3D video, i.e., less_than5% false negatives, with the lowest localization error as well as the longest average path duration among the three methods.  #@NEW_LINE#@#  

Performance_on_Experimental_2D_Videos  #@NEW_LINE#@#  
Finally, we sought to evaluate the performance and rigor of the NN on experimentally derived rather than simulated videos, since the former can include spatiotemporal variations and features that might not be captured in simulated videos.  #@NEW_LINE#@#  Because analysis from the particle traces can directly influence interpretations of important biological phenomenon, the common practice is for the end user to supervise and visually inspect all traces to eliminate false positives and minimize false negatives.  #@NEW_LINE#@#  Against such rigorously verified tracking, the NN was able to produce particle paths with comparable MSDs across different time scales, alpha values, a low false positive rate, greater number of traces (i.e., decrease in false negatives), and comparable path length (see SI Appendix, Fig S4).  #@NEW_LINE#@#  Most importantly, these videos were processed in less than one-20th of the time it took to manually verify them, generally taking 30 s to 60 s to process a video, compared with 10 min to 20 min to verify accuracy.  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
The principal benefit of the trained CNN is robustness to changing conditions.  #@NEW_LINE#@#  For example, the net tracker was capable, without any modifications, of tracking Salmonella (Fig 1, Right and Movie S1), which are large enough to resolve and appear as rod-shaped in images.  #@NEW_LINE#@#  Even though the neural net was trained on rotationally symmetric particle shapes, rod-shaped cells were still recognized with strong confidence sufficient for high-fidelity tracking.  #@NEW_LINE#@#  Large polydisperse particles are also readily tracked, provided their PSF shape does not deviate too far from the rotationally symmetric training data.  #@NEW_LINE#@#  Our neural network does not recognize long filaments such as microtubules; such applications will require significant, targeted advances customized to the specific application.  #@NEW_LINE#@#  Another example of the robustness of the network is its ability to ignore background objects and effectively suppress false positives.  #@NEW_LINE#@#  The neural network does not recognize large bright objects that sometimes appear in videos (see Movie S1), even though it was trained on images containing slowly varying background intensity.  #@NEW_LINE#@#  
The particle localization method used the neural network output instead of computing the centroid position from the raw image data (as is typically done), and the resulting localization accuracy was comparable to other methods.  #@NEW_LINE#@#  However, some applications such as microrheology may require additional accuracy.  #@NEW_LINE#@#  Several high-quality localization algorithms have been developed that potentially might, given a local region of interest (provided by the neural network) in the raw image, estimate the particle center with more accuracy (34).  #@NEW_LINE#@#  One alternative to particle tracking microrheology is differential dynamic microscopy, which uses scattering methods to estimate dynamic parameters from microscopy videos (35).  #@NEW_LINE#@#  
Finally, tools based on machine learning for computer vision are advancing rapidly.  #@NEW_LINE#@#  Applications of neural network-based segmentation to medical imaging are already under development (3638).  #@NEW_LINE#@#  One recent study has used a pixels-to-pixelstype CNN to process raw stochastic optical reconstruction microscopy (STORM) data into superresolution images (39).  #@NEW_LINE#@#  The potential for this technology to address outstanding bioimaging problems is becoming clear, particularly for image segmentation, which is an active research area in machine learning (22, 4045).  #@NEW_LINE#@#  

Acknowledgments  #@NEW_LINE#@#  
J.M.N.  #@NEW_LINE#@#  would like to thank the Isaac Newton Institute for Mathematical Sciences for support and hospitality during the programme Stochastic Dynamical Systems in Biology when work on this paper was undertaken, including useful discussions with Sam Isaacson, Simon Cotter, David Holcman, and Konstantinos Zygalakis.  #@NEW_LINE#@#  Financial support was provided by the National Science Foundation Grants DMS-1715474 (to J.M.N.  #@NEW_LINE#@#  ), DMS-1412844 (to M.G.F.  #@NEW_LINE#@#  ), DMS-1462992 (to M.G.F.  #@NEW_LINE#@#  ), and DMR-1151477 (to S.K.L.  #@NEW_LINE#@#  ); National Institute of Health Grant R41GM123897 (to S.K.L.  #@NEW_LINE#@#  ); The David and Lucile Packard Foundation Grant 2013-39274 (to S.K.L.  #@NEW_LINE#@#  ); and the Eshelman Institute of Innovation (S.K.L).  #@NEW_LINE#@#  The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  

Published under the PNAS license.  #@NEW_LINE#@#  

