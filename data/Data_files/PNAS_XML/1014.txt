article id="http://dx.doi.org/10.1073/pnas.1711114115"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Toward a unified theory of efficient, predictive, and sparse coding  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Sensory neural circuits are thought to efficiently encode incoming signals.  #@NEW_LINE#@#  Several mathematical theories of neural coding formalize this notion, but it is unclear how these theories relate to each other and whether they are even fully consistent.  #@NEW_LINE#@#  Here we develop a unified framework that encompasses and extends previous proposals.  #@NEW_LINE#@#  We highlight key tradeoffs faced by sensory neurons; we show that trading off future prediction against efficiently encoding past inputs generates qualitatively different predictions for neural responses to natural visual stimulation.  #@NEW_LINE#@#  Our approach is a promising first step toward theoretically explaining the observed diversity of neural responses.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
A central goal in theoretical neuroscience is to predict the response properties of sensory neurons from first principles.  #@NEW_LINE#@#  To this end, efficient coding posits that sensory neurons encode maximal information about their inputs given internal constraints.  #@NEW_LINE#@#  There exist, however, many variants of efficient coding (e.g., redundancy reduction, different formulations of predictive coding, robust coding, sparse coding, etc.  #@NEW_LINE#@#  ), differing in their regimes of applicability, in the relevance of signals to be encoded, and in the choice of constraints.  #@NEW_LINE#@#  It is unclear how these types of efficient coding relate or what is expected when different coding objectives are combined.  #@NEW_LINE#@#  Here we present a unified framework that encompasses previously proposed efficient coding models and extends to unique regimes.  #@NEW_LINE#@#  We show that optimizing neural responses to encode predictive information can lead them to either correlate or decorrelate their inputs, depending on the stimulus statistics; in contrast, at low noise, efficiently encoding the past always predicts decorrelation.  #@NEW_LINE#@#  Later, we investigate coding of naturalistic movies and show that qualitatively different types of visual motion tuning and levels of response sparsity are predicted, depending on whether the objective is to recover the past or predict the future.  #@NEW_LINE#@#  Our approach promises a way to explain the observed diversity of sensory neural responses, as due to multiple functional goals and constraints fulfilled by different cell types and/or circuits.  #@NEW_LINE#@#  

Efficient_Coding_with_Varying_Objectives_Constraints  #@NEW_LINE#@#  
We consider a temporal stimulus, x:t(,xt1,xt), which elicits neural responses, r:t(,rt1,rt).  #@NEW_LINE#@#  We seek a neural code described by the probability distribution p(rt|x:t), such that neural responses within a temporal window of length  encode maximal information about the stimulus at lag  given fixed information about past inputs (Fig 1A).  #@NEW_LINE#@#  This problem can be formalized using the IB framework (3032) by seeking a code, p(rt|x:t), that maximizes the objective function:Lp(rt|x:t)=I(Rt:t;Xt+)I(Rt;X:t),[1]where the first term (to be maximized) is the mutual information between the responses between t and  and the stimulus at time t+, while the second term (to be constrained) is the mutual information between the response at time t and past inputs (which we call the coding capacity, C).  #@NEW_LINE#@#  A constant, , controls the tradeoff between coding fidelity and compression.  #@NEW_LINE#@#  This objective function can be expanded asLp(rt|x:t)=logp(xt+|rt:t)logp(xt+)logp(rt|xt:t)+logp(rt)p(r,x).  #@NEW_LINE#@#  [2]Previously, we showed that, in cases where it is not possible to compute this objective function directly, one can use approximations of p(xt+|rt:t) and p(rt) to obtain a lower bound, LL, that can be maximized tractably (31) (SI Appendix,General Framework).  #@NEW_LINE#@#  
From Eqs.  #@NEW_LINE#@#  1 and 2, we see that the optimal coding strategy depends on three factors: the decoding lag, ; the code length, ; and the coding capacity, C (determined by ).  #@NEW_LINE#@#  Previous theories of neural coding correspond to specific regions within the 3D parameter space spanned by , , and C (Fig 1B).  #@NEW_LINE#@#  For example, temporal redundancy reduction (3, 33) occurs (i) at low internal noise (i.e., high C), (ii) where the objective is to encode the recent past (less_than0), and (iii) where information about the stimulus can be read out by integrating neural responses over time (0).  #@NEW_LINE#@#  Increasing the internal noise (i.e., decreasing C) results in a temporally redundant robust code (3437) (blue sphere in Fig 1B).  #@NEW_LINE#@#  Recent work positing that neurons efficiently encode information about the future (0) looked exclusively at near-instantaneous codes, where 0 (red plane in Fig 1B) (15, 3840).  #@NEW_LINE#@#  Here, we investigate the relation between these previous works and focus on the (previously unexplored) case of neural codes that are both predictive (0) and temporal (0) and have varying signal to noise (variable C) (black sphere in Fig 1B).  #@NEW_LINE#@#  
To specialize our theory to the biologically relevant case, we later investigate efficient coding of natural stimuli.  #@NEW_LINE#@#  A hallmark of natural stimuli is their sparse latent structure (18, 22, 25, 26): stimulus fragments can be constructed from a set of primitive features (e.g., image contours), each of which occurs rarely (Fig 1C).  #@NEW_LINE#@#  Previous work showed that, in consequence, redundancy between neural responses is minimized by maximizing their sparsity (SI Appendix, Efficient Coding Models) (22).  #@NEW_LINE#@#  Here, we investigated what happens when the objective is not to minimize redundancy but rather, to efficiently predict future stimuli given finite coding capacity.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Dependence_of_Neural_Code_on_Coding_Objectives  #@NEW_LINE#@#  
Our initial goal was to understand the influence of different coding objectives in the simplest scenario, where a single neuron linearly encodes a 1-d input.  #@NEW_LINE#@#  In this model, the neural response at time t is rt=k==0wwkxtk+t, where w=(w0,,ww) are the linear coding weights and t is a Gaussian noise with unit variance.  #@NEW_LINE#@#  
With 1-d stimuli that have Gaussian statistics, the IB objective function takes a very simple form:L=12log(xt+k=0ukrtk)212logrt2,[3]where u=(u0,,u) are the optimal linear readout weights used to reconstruct the stimulus at time t+ from the responses between t and t. Thus, the optimal code is the one that minimizes the mean-squared reconstruction error at lag , constrained by the variance of the neural response (relative to the noise variance).  #@NEW_LINE#@#  
Initially, we investigated instantaneous codes, where =0, so that the stimulus at time t+ is estimated from the instantaneous neural response at time t (Fig 2A).  #@NEW_LINE#@#  We considered three different stimulus types, which are shown in Fig 2B.  #@NEW_LINE#@#  With a Markov stimulus (Fig 2B, Top and SI Appendix, Methods for Simulations in the Main Text), with a future trajectory that depended solely on the current state, xt, the neurons only needed to encode xt to predict the stimulus at a future time, xt+.  #@NEW_LINE#@#  Thus, when =0, we observed the trivial solution where rtxt, irrespective of the decoding lag,  (Fig 2 C and D and SI Appendix, Fig S2A).  #@NEW_LINE#@#  
With a two-timescale stimulus constructed from two Markov processes that vary over different timescales (Fig 2B, Middle), the optimal solution was a low-pass filter to selectively encode the predictive, slowly varying part of the stimulus.  #@NEW_LINE#@#  The strength of the low-pass filter increased monotonically with  (Fig 2 C and D and SI Appendix, Fig S2A).  #@NEW_LINE#@#  
Finally, with an inertial stimulus, with a future trajectory that depended on both the previous state, xt, and velocity, xtxt1 (Fig 2B, Bottom), the optimal solution was a high-pass filter so as to encode information about velocity.  #@NEW_LINE#@#  The strength of this high-pass filter also increased monotonically with  (Fig 2 C and D and SI Appendix, Fig S2A, Bottom).  #@NEW_LINE#@#  
With an instantaneous code, varying the coding capacity, C, only rescales responses (relative to the noise amplitude) so as to alter their signal-to-noise ratio.  #@NEW_LINE#@#  However, the response shape is left unchanged (regardless of the stimulus statistics) (Fig 2D).  #@NEW_LINE#@#  In contrast, with temporally extended codes, where 0 (so the stimulus at time t+ is estimated from the integrated responses between time t and t) (Fig 2A), the optimal neural code varies with the coding capacity, C. As with previous efficient coding models, at high C (i.e., high signal-to-noise ratio), neurons always decorrelated their input, regardless of both the stimulus statistics and the decoding lag,  (to achieve nonredundant responses) (SI Appendix, Efficient Coding Models), while decreasing C always led to more correlated responses (to achieve a robust code) (SI Appendix, Efficient Coding Models) (36).  #@NEW_LINE#@#  However, unlike previous efficient coding models at low to intermediate values of C (i.e., intermediate to low signal-to-noise ratio), the optimal code was qualitatively altered by varying the decoding lag, .  #@NEW_LINE#@#  With the Markov stimulus, increasing  had no effect; with the two-timescale stimulus, it led to low-pass filtering, and with the inertial stimulus, it led to stronger high-pass filtering.  #@NEW_LINE#@#  
Taken together, phase diagrams for optimal, temporally extended codes show how regimes of decorrelation/whitening (high-pass filtering) and of smoothing (low-pass filtering) are preferred depending on the coding capacity, C, and decoding lag, .  #@NEW_LINE#@#  We verified that a qualitatively similar transition from low- to high-pass filtering is also observed with higher dimensional stimuli and/or more neurons.  #@NEW_LINE#@#  Importantly, we show that these phase diagrams depend in an essential way on the stimulus statistics already in the linear Gaussian case.  #@NEW_LINE#@#  We next examined what happens for non-Gaussian, high-dimensional stimuli.  #@NEW_LINE#@#  

Efficient_Coding_of_Naturalistic_Stimuli  #@NEW_LINE#@#  
Natural stimuli exhibit a strongly non-Gaussian statistical structure, which is essential for human perception (22, 41).  #@NEW_LINE#@#  A large body of work has investigated how neurons could efficiently represent such stimuli by encoding their nonredundant or independent components (4).  #@NEW_LINE#@#  Under fairly general conditions (e.g., that stimuli have a sparse latent structure), this is equivalent to finding a sparse code: a form of neural population code, in which only small fractions of neurons are active at any one time (22).  #@NEW_LINE#@#  For natural images, this leads to neurons that are selective for spatially localized image contours, with receptive fields (RFs) that are qualitatively similar to the RFs of V1 simple cells (25, 26).  #@NEW_LINE#@#  For natural movies, this leads to neurons selective for a particular motion direction, again similar to observations in area V1 (27).  #@NEW_LINE#@#  
However, an independent (sparse) temporal code has only been shown to be optimal (i) when the goal is to maximize information about past inputs (i.e., less_than0) and (ii) at low noise (i.e., at high capacity; C0).  #@NEW_LINE#@#  We were interested, therefore, in what happens when these two criteria are violated: for example, when neural responses are optimized to encode predictive information (i.e., for 0).  #@NEW_LINE#@#  
To explore these questions, we modified the objective function of Eq.  #@NEW_LINE#@#  3 to deal with multidimensional stimuli and non-Gaussian statistics of natural images (SI Appendix, General Framework).  #@NEW_LINE#@#  Specifically, we generalized the second term of Eq.  #@NEW_LINE#@#  3 to allow optimization of the neural code with respect to higher-order (i.e., beyond covariance) response statistics.  #@NEW_LINE#@#  This was done by approximating the response distribution p(r) by a Student t distribution, with shape parameter, , learned directly from data (SI Appendix, Eq.  #@NEW_LINE#@#  S5) (31).  #@NEW_LINE#@#  Crucially, our modification permitsbut does not enforce by handsparse neural responses (42).  #@NEW_LINE#@#  For nonsparse, Gaussian stimuli, the IB algorithm returns , so that the Student t distribution is equivalent to a Gaussian distribution, and we obtain the results of the previous section; for natural image sequences, it replicates previous sparse coding results in the limit less_than0 and C0 (SI Appendix, Fig S5), without introducing any tunable parameters.  #@NEW_LINE#@#  
We investigated how the optimal neural code for naturalistic stimuli varied with the decoding lag, , while keeping coding capacity, C, and code length, , constant.  #@NEW_LINE#@#  Stimuli were constructed from 10 × 10-pixel patches drifting stochastically across static natural images (Fig 3A, SI Appendix, Methods for Simulations in the Main Text, and SI Appendix, Fig S3).  #@NEW_LINE#@#  Gaussian white noise was added to these inputs (but not the decoded variable, Xt+) (SI Appendix, Methods for Simulations in the Main Text).  #@NEW_LINE#@#  Neural encoding weights were optimized with two different decoding lags: for =6, the goal was to encode past stimuli, while for =1, the goal was to predict the near future.  #@NEW_LINE#@#  Fig 3B confirms that the codes indeed are optimal for recovering either the past (=6) or future (=1) as desired.  #@NEW_LINE#@#  
After optimization at both values of , individual neurons were selective to local oriented edge features (Fig 3 C and D) (25).  #@NEW_LINE#@#  Varying  qualitatively altered the temporal features encoded by each neuron, while having little effect on their spatial selectivity.  #@NEW_LINE#@#  Consistent with previous results on sparse temporal coding (27), with =6, single cells were responsive to stimuli moving in a preferred direction as evidenced by spatially displaced encoding filters at different times (Fig 3C and SI Appendix, Fig S6 AC) and a high directionality index (Fig 3E).  #@NEW_LINE#@#  In contrast, with =1, cells responded equally to stimuli moving in either direction perpendicular to their encoded stimulus orientation.  #@NEW_LINE#@#  This was evidenced by spatiotemporally separable RFs (SI Appendix, Fig S6 DF) and directionality indexes near zero.  #@NEW_LINE#@#  This qualitative difference between the two types of code for naturalistic movies was highly surprising, and we sought to understand its origins.  #@NEW_LINE#@#  

Tradeoff_Between_Sparsity_and_Predictive_Power  #@NEW_LINE#@#  
To gain an intuitive understanding of how the optimal code varies with decoding lag, , we constructed artificial stimuli from overlapping Gaussian bumps, which drifted stochastically along a single spatial dimension (Fig 4A and SI Appendix, Methods for Simulations in the Main Text).  #@NEW_LINE#@#  While simple, this stimulus captured two key aspects of the naturalistic movies.  #@NEW_LINE#@#  First, Gaussian bumps drifted smoothly in space, resembling stochastic global motion over the image patches; second, the stimulus had a sparse latent structure.  #@NEW_LINE#@#  
We optimized the neural code with  ranging from 2 to 2, holding the coding capacity, C, and code length, , constant.  #@NEW_LINE#@#  Fig 4B confirms that highest performance was achieved when the reconstruction performance was evaluated at the same lag for which each model was trained.  #@NEW_LINE#@#  This simpler setup recapitulated the surprising result that we obtained with naturalistic stimuli: namely, when less_than0, neurons were selective to a single preferred motion direction, while when 0, neurons responded equally to stimuli moving from either direction to their RF (Fig 4 C and D).  #@NEW_LINE#@#  
Predicting the future state of the stimulus requires estimating its current motion direction and speed.  #@NEW_LINE#@#  How is it possible then that optimizing the code for predictions (0) results in neurons being unselective to motion direction?  #@NEW_LINE#@#  This paradox is resolved by realizing that it is the information encoded by the entire neural population that counts, not the information encoded by individual neurons.  #@NEW_LINE#@#  Indeed, when we looked at the information encoded by the neural population, we did find what we had originally expected: when optimized with 0, the neural population as a whole encoded significantly more information about the stimulus velocity than its position (relative to when less_than0), despite the fact that individual neurons were unselective to motion direction (Fig 4 E and F).  #@NEW_LINE#@#  
The change in coding strategy that is observed as one goes from encoding the past (less_than0) to the future (0) is in part due to a tradeoff between maintaining a sparse code and cells responding quickly to stimuli within their RF.  #@NEW_LINE#@#  Intuitively, to maintain highly selective (and thus, sparse) responses, neurons first have to wait to process and recognize the complete stimulus feature; unavoidably, however, this entails a processing delay, which leads to poor predictions.  #@NEW_LINE#@#  This can be seen in Fig 4 G and H, which shows how both the response sparsity and delay to stimuli within a cells RF decrease with .  #@NEW_LINE#@#  In SI Appendix, Supplementary Simulations, we describe in detail why this tradeoff between efficiency and prediction leads to direction-selective filters when less_than0 but not when 0 (SI Appendix, Fig S7).  #@NEW_LINE#@#  
Beyond the effects on the optimal code of various factors explored in detail in this paper, our framework further generalizes previous efficient and sparse coding results to factors listed in SI Appendix, Table S1 and discussed in SI Appendix, Supplementary Simulations.  #@NEW_LINE#@#  For example, decreasing the capacity, C (while holding  constant at 2), resulted in neurons being unselective to stimulus motion (SI Appendix, Fig S8A), with a similar result observed for increased input noise (SI Appendix, Fig S8B).  #@NEW_LINE#@#  Thus, far from being generic, traditional sparse temporal coding, in which neurons responded to local motion, was only observed in a specific regime (i.e., less_than0, C0, and low input noise).  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
Efficient coding has long been considered a central principle for understanding early sensory representations (1, 3), with well-understood implications and generalizations (23, 37).  #@NEW_LINE#@#  It has been successful in predicting many aspects of neural responses in early sensory areas directly from the low-order statistics of natural stimuli (7, 22, 32, 43, 44) and has even been extended to higher-order statistics and central processing (45, 46).  #@NEW_LINE#@#  However, a criticism of the standard theory is that it treats all sensory information as equal, despite empirical evidence that neural systems prioritize behaviorally relevant (and not just statistically likely) stimuli (47).  #@NEW_LINE#@#  To overcome this limitation, Bialek and coworkers (14, 15) proposed a modification to the standard efficient coding theory, positing that neural systems are set up to efficiently encode information about the future given fixed information about the past.  #@NEW_LINE#@#  This is motivated by the fact that stimuli are only useful for performing actions when they are predictive about the future.  #@NEW_LINE#@#  
The implications of such a coding objective have remained relatively unexplored.  #@NEW_LINE#@#  Existing work only considered the highly restrictive scenario where neurons maximize information encoded in their instantaneous responses (15, 38, 40).  #@NEW_LINE#@#  In this case (and subject to some additional assumptions, such as Gaussian stimulus statistics and instantaneous encoding filters), predictive coding is formally equivalent to slow feature analysis (39).  #@NEW_LINE#@#  This is the exact opposite of standard efficient coding models, which (at low noise/high capacity) predict that neurons should temporally decorrelate their inputs (3, 33).  #@NEW_LINE#@#  
We developed a framework to clarify the relation between different versions of the efficient coding theory (14, 30, 31).  #@NEW_LINE#@#  We investigated what happens when the neural code is optimized to efficiently predict the future (i.e., 0 and 0) (Fig 1B).  #@NEW_LINE#@#  In this case, the optimal code depends critically on the coding capacity (i.e., signal-to-noise ratio), which describes how much information the neurons can encode about their input.  #@NEW_LINE#@#  At high capacity (i.e., low noise), neurons always temporally decorrelate their input.  #@NEW_LINE#@#  At finite capacity (i.e., mid to high noise), however, the optimal neural code varies qualitatively depending on whether the goal is to efficiently predict the future or reconstruct the past.  #@NEW_LINE#@#  
When we investigated efficient coding of naturalistic stimuli, we found solutions that are qualitatively different from known sparse coding results, in which individual neurons are tuned to directional motion of local edge features (27).  #@NEW_LINE#@#  In contrast, we found that neurons optimized to encode the future are selective for motion speed but not direction (Fig 3 and SI Appendix, Fig S6).  #@NEW_LINE#@#  Surprisingly, however, the neural population as a whole encodes motion even more accurately in this case (Fig 4E).  #@NEW_LINE#@#  We show that these changes are due to an implicit tradeoff between maintaining a sparse code and responding quickly to stimuli within each cells RF (Fig 4 G and H).  #@NEW_LINE#@#  
It is notable that, in our simulations, strikingly different conclusions are reached by analyzing single-neuron responses vs. the population responses.  #@NEW_LINE#@#  Specifically, looking only at single-neuron responses would lead one to conclude that, when optimized for predictions, neurons did not encode motion direction; looking at the neural population responses reveals that the opposite is true.  #@NEW_LINE#@#  This illustrates the importance of population-level analyses of neural data and how, in many cases, single-neuron responses can give a false impression of which information is represented by the population.  #@NEW_LINE#@#  
A major challenge in sensory neuroscience is to derive the observed cell-type diversity in sensory areas from a normative theory.  #@NEW_LINE#@#  For example, in visual area V1, one observes a range of different cell types, some of which have spatiotemporally separable RFs and others do not (48, 49).  #@NEW_LINE#@#  The question arises, therefore, whether the difference between cell types emerges because different subnetworks fulfill qualitatively different functional goals.  #@NEW_LINE#@#  One hypothesis, suggested by our work, is that cells with separable RFs have evolved to efficiently encode the future, while cells with nonseparable RFs evolved to efficiently encode the past.  #@NEW_LINE#@#  More generally, the same hypothesis could explain the existence of multiple cell types in the mammalian retina, with each cell type implementing an optimal code for a particular choice of optimization parameters (e.g., coding capacity or prediction lag).  #@NEW_LINE#@#  
Testing such hypotheses rigorously against quantitative data would require us to generalize our work to nonlinear encoding and decoding models (SI Appendix, Table S1).  #@NEW_LINE#@#  Here, we focused on a linear decoder to lay a solid theoretical foundation and permit direct comparison with previous sparse and robust coding models, which also assumed a linear decoder (2527, 35, 36).  #@NEW_LINE#@#  In addition, a linear decoder forces our algorithm to find a neural code for which information can be easily extracted by downstream neurons performing biologically plausible operations.  #@NEW_LINE#@#  While the linearity assumptions simplify our analysis, the framework can easily accommodate nonlinear encoding and decoding.  #@NEW_LINE#@#  For example, we previously used a kernel encoding model, where neural responses are described by a nonparametric and nonlinear function of the input (31).  #@NEW_LINE#@#  Others have similarly used a deep convolutional neural network as an encoder (50).  #@NEW_LINE#@#  
As mentioned earlier, predictive coding has been used to describe several different approaches.  #@NEW_LINE#@#  Clarifying the relationship between inequivalent definitions of predictive coding and linking them mathematically to coding efficiency provided one of the initial motivations for our work.  #@NEW_LINE#@#  In past work, alternative coding theories are often expressed using very different mathematical frameworks, impeding comparison between them and sometimes leading to confusion.  #@NEW_LINE#@#  In contrast, by using a single mathematical framework to compare different theoriesefficient, sparse, and predictive codingwe were able see exactly how they relate to each other, the circumstances under which they make opposing or similar predictions, and what happens when they are combined.  #@NEW_LINE#@#  

Acknowledgments  #@NEW_LINE#@#  
This work was supported by Agence Nationale de Recherche (ANR) Trajectory, the French State program Investissements dAvenir managed by the ANR [LIFESENSES: ANR-10-LABX-65], a European Commission Grant (FP7-604102), NIH Grant U01NS090501, and AVIESAN-UNADEV Grant (to O.M.  #@NEW_LINE#@#  ), and Austrian Science Fund Grant FWF P25651 (to G.T.  #@NEW_LINE#@#  ).  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  

Published under the PNAS license.  #@NEW_LINE#@#  

