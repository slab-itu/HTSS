article id="http://dx.doi.org/10.1073/pnas.1601827113"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Boosting medical diagnostics by pooling independent judgments  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Collective intelligence is considered to be one of the most promising approaches to improve decision making.  #@NEW_LINE#@#  However, up to now, little is known about the conditions underlying the emergence of collective intelligence in real-world contexts.  #@NEW_LINE#@#  Focusing on two key areas of medical diagnostics (breast and skin cancer detection), we here show that similarity in doctors accuracy is a key factor underlying the emergence of collective intelligence in these contexts.  #@NEW_LINE#@#  This result paves the way for innovative and more effective approaches to decision making in medical diagnostics and beyond, and to the scientific analyses of those approaches.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Collective intelligence refers to the ability of groups to outperform individual decision makers when solving complex cognitive problems.  #@NEW_LINE#@#  Despite its potential to revolutionize decision making in a wide range of domains, including medical, economic, and political decision making, at present, little is known about the conditions underlying collective intelligence in real-world contexts.  #@NEW_LINE#@#  We here focus on two key areas of medical diagnostics, breast and skin cancer detection.  #@NEW_LINE#@#  Using a simulation study that draws on large real-world datasets, involving more than 140 doctors making more than 20,000 diagnoses, we investigate when combining the independent judgments of multiple doctors outperforms the best doctor in a group.  #@NEW_LINE#@#  We find that similarity in diagnostic accuracy is a key condition for collective intelligence: Aggregating the independent judgments of doctors outperforms the best doctor in a group whenever the diagnostic accuracy of doctors is relatively similar, but not when doctors diagnostic accuracy differs too much.  #@NEW_LINE#@#  This intriguingly simple result is highly robust and holds across different group sizes, performance levels of the best doctor, and collective intelligence rules.  #@NEW_LINE#@#  The enabling role of similarity, in turn, is explained by its systematic effects on the number of correct and incorrect decisions of the best doctor that are overruled by the collective.  #@NEW_LINE#@#  By identifying a key factor underlying collective intelligence in two important real-world contexts, our findings pave the way for innovative and more effective approaches to complex real-world decision making, and to the scientific analyses of those approaches.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
We investigated the performance of virtual groups of diagnosticians using either of two collective intelligence rules: the confidence rule (17, 20) and the majority rule (34, 35).  #@NEW_LINE#@#  For any particular group evaluating any particular case, the confidence rule adopts the judgment of the most confident diagnostician, whereas the majority rule adopts the judgment receiving the most support within that group.  #@NEW_LINE#@#  For any of our groups, we compared the performance of these two rules with the performance of the best diagnostician in that group in terms of (i) sensitivity, (ii) specificity, and (iii) Youdens index (J).  #@NEW_LINE#@#  The last is a composite measure of accuracy that combines sensitivity and specificity (J = Sensitivity + Specificity  1) (36, 37).  #@NEW_LINE#@#  
Pooling the independent judgments of multiple diagnosticians with the confidence or the majority rule can only promote collective intelligence when two conditions are fulfilled.  #@NEW_LINE#@#  First, the judgments of different diagnosticians must not be perfectly correlated with each other (if different diagnosticians give identical judgments on all cases, there is no scope for collective intelligence).  #@NEW_LINE#@#  Second, in case of the confidence rule, there has to be a positive correlation between confidence and accuracy levels.  #@NEW_LINE#@#  Initial analyses of our datasets showed that both conditions are fulfilled in both diagnostic contexts (Fig 1).  #@NEW_LINE#@#  
We first considered groups of two diagnosticians using the confidence rule.  #@NEW_LINE#@#  In both diagnostic contexts, we found that as the difference in accuracy levels between two diagnosticians increases, their joint ability to outperform the better diagnostician decreases [breast cancer: estimate (est) ± SE = 1.03 ± 0.04, t = 24.9, P less_than 0.001, Fig 2A; skin cancer: est ± SE = 0.55 ± 0.03, t = 20.1, P less_than 0.001, Fig 2B].  #@NEW_LINE#@#  When diagnosticians accuracy levels were relatively similar (|J| less_than 0.1), the confidence rule outperformed the better diagnostician (Fig 2 A and B).  #@NEW_LINE#@#  In contrast, for relatively dissimilar groups, the better diagnostician outperformed the confidence rule.  #@NEW_LINE#@#  This effect was largely independent of the accuracy level of the better diagnostician (Fig 3 A and B), the accuracy level of the poorer diagnostician (SI Appendix, Fig S4 A and B), and the average accuracy level within groups (SI Appendix, Fig S5 A and B).  #@NEW_LINE#@#  When we analyzed the effects of similarity in accuracy on collective sensitivity and specificity, the same pattern emerged: In both diagnostic contexts, combining decisions using the confidence rule led to higher sensitivity and specificity (relative to the better individual), but only when the two diagnosticians accuracy levels were similar (SI Appendix, Fig S6).  #@NEW_LINE#@#  Moreover, independent of similarity, the confidence rule consistently outperformed the average individual performance within the group (SI Appendix, Fig S7).  #@NEW_LINE#@#  When considering groups of three and five diagnosticians using the confidence rule, we find that the above results generalize to these larger group sizes (SI Appendix, Fig S8).  #@NEW_LINE#@#  
Similarly, when considering groups of three diagnosticians using the majority rule, we found that as the differences in accuracy levels across the three diagnosticians increase, the groups joint ability to outperform the best diagnostician decreases in both diagnostic contexts (breast cancer: est ± SE = 1.26 ± 0.05, t = 27.7, P less_than 0.001, Fig 2C; skin cancer: est ± SE = 0.68 ± 0.03, t = 23.3, P less_than 0.001, Fig 2D).  #@NEW_LINE#@#  As in the case of the confidence rule, the majority rule outperformed the best diagnostician in that group only when the three diagnosticians accuracy levels were relatively similar (|J| less_than 0.1).  #@NEW_LINE#@#  Again, this effect was largely independent of the performance of the best diagnostician (Fig 3 C and D), the performance of the poorest diagnostician (SI Appendix, Fig S4 C and D), and the average performance within groups (SI Appendix, Fig S5 C and D), and it held for both sensitivity and specificity (SI Appendix, Fig S9).  #@NEW_LINE#@#  Moreover, independent of diagnostic similarity, the majority rule consistently outperformed the average individual performance within the group (SI Appendix, Fig S10).  #@NEW_LINE#@#  When considering groups of five diagnosticians using the majority rule, we find that the above results generalize to this larger group size (SI Appendix, Fig S11).  #@NEW_LINE#@#  SI Appendix, Fig S12 provides a direct comparison of the confidence and the majority rule for different group sizes.  #@NEW_LINE#@#  
To further understand the mechanisms underlying the above findings, we developed simplified analytical models of the two most basic scenarios investigated above, namely, two diagnosticians using the confidence rule and three diagnosticians using the majority rule (model details are provided in SI Appendix).  #@NEW_LINE#@#  To illustrate, consider two diagnosticians using the confidence rule.  #@NEW_LINE#@#  From the point of view of the better (i.e., more accurate) diagnostician, employing the confidence rule has two effects: The poorer diagnostician may overrule incorrect judgments of the better diagnostician (positive effect), and the poorer diagnostician may overrule correct judgments of the better diagnostician (negative effect).  #@NEW_LINE#@#  Importantly, our model shows that the strength of both effects depends on the similarity in accuracy levels between the two diagnosticians (SI Appendix).  #@NEW_LINE#@#  As similarity decreases (assuming constant average accuracy), the better diagnostician gets better and the poorer gets worse, thereby decreasing the positive effect (the better diagnostician makes fewer incorrect judgments and the poorer makes fewer correct judgments) and increasing the negative effect (the better diagnostician makes more correct judgments and the poorer makes more incorrect judgments).  #@NEW_LINE#@#  As a consequence, and in line with our main findings above (Fig 2), the model predicts that as similarity decreases, the ability of the group to outperform its better member also decreases.  #@NEW_LINE#@#  An analogous trend holds for the situation where three diagnosticians use the majority rule (SI Appendix).  #@NEW_LINE#@#  
Further analyses of our datasets showed that in both diagnostic contexts and for both the confidence and the majority rule, the number of correct and incorrect judgments of the best diagnostician that are overruled is fully in line with the prediction from the above modeling analysis.  #@NEW_LINE#@#  In particular, we find that as similarity decreases, (i) the positive effect above decreases (i.e., the number of incorrect judgments of the best diagnostician that were overruled by the poorer diagnostician/the majority decreases; Fig 4, green bars) and (ii) the negative effect above increases (i.e., the number of correct judgments of the best diagnostician that were overruled by the poorer diagnostician/the majority increases; Fig 4, red bars).  #@NEW_LINE#@#  Moreover, while the positive effect outweighs the negative effect for relatively high levels of similarity (|J| less_than 0.1), the reverse is true for relatively low levels of similarity (|J|  0.2), thereby explaining why only relatively similar groups can successfully use the confidence and majority rule to outperform their best member.  #@NEW_LINE#@#  

Discussion  #@NEW_LINE#@#  
Although collective intelligence has the potential to transform decision making in a wide range of domains, little is known about the conditions that underlie its emergence in real-world contexts.  #@NEW_LINE#@#  Drawing on large real-world datasets, involving more than 140 doctors performing more than 20,000 diagnoses, we have identified similarity in decision accuracy as a key factor underlying the emergence of collective intelligence in breast and skin cancer diagnostics.  #@NEW_LINE#@#  In particular, we have found that when a group of diagnosticians is characterized by relatively similar accuracy levels, combining their independent judgments improves decision accuracy relative to the best diagnostician within that group.  #@NEW_LINE#@#  In contrast, when accuracy levels become too disparate, combining independent judgments leads to poorer diagnostic outcomes relative to those diagnostic outcomes achieved by the best diagnostician.  #@NEW_LINE#@#  This result is highly robust and holds across different performance levels of the best diagnostician, different group sizes, and different collective intelligence rules (confidence rule and majority rule).  #@NEW_LINE#@#  
To reap the benefits associated with collective intelligence, we need to know which characteristics of decision makers and decision contexts favor the emergence of collective intelligence and which decision-making rules allow this potential to be harnessed.  #@NEW_LINE#@#  We have here provided answers to both questions in the domain of medical diagnostics.  #@NEW_LINE#@#  In particular, and in contrast to current practice, our findings strongly suggest that similarity in diagnostic accuracy should be a key criterion for assembling groups in medical diagnostics, such as in the context of independent double reading of mammograms, a standard practice in Europe (38).  #@NEW_LINE#@#  Our analyses suggest that groups of diagnosticians with similar accuracy levels can use simple algorithmic approaches (i.e., confidence rule, majority rule) to achieve a performance that is superior to their best member.  #@NEW_LINE#@#  At a group size of two, the confidence rule can be employed to outperform the best diagnostician.  #@NEW_LINE#@#  For a group size of three onward, the majority rule tends to outperform the confidence rule (SI Appendix, Fig S12).  #@NEW_LINE#@#  
Future studies should address at least three issues.  #@NEW_LINE#@#  First, we have focused on combining independent diagnostic judgments, thus not investigating situations in which diagnosticians directly communicate with each other.  #@NEW_LINE#@#  Therefore, one open question is the extent to which our findings generalize to face-to-face interactions and discussions within medical teams.  #@NEW_LINE#@#  Previous work in nonmedical contexts has shown that similarity in accuracy is a prerequisite for collective intelligence to arise during group discussions (15), suggesting that it may also matter in interacting medical teams.  #@NEW_LINE#@#  Second and more generally, it remains unknown how these two collective intelligence mechanisms (aggregation of independent judgments versus group discussions) compare in medical diagnostics (19).  #@NEW_LINE#@#  Group discussions are known to be a double-edged sword (39).  #@NEW_LINE#@#  Phenomena such as group think, interpersonal competition, social loafing, and obedience to authority can compromise group accuracy (4042), yet groups are known to outperform individuals across a range of tasks (7, 43).  #@NEW_LINE#@#  It will thus be important to compare the relative gains (or declines) in accuracy that these mechanisms afford across medical diagnostic contexts.  #@NEW_LINE#@#  Third, improving decision accuracy is of prime importance across a wide range of contexts (e.g., economic decision making, political decision making).  #@NEW_LINE#@#  Future work should investigate whether and to what extent similarity in decision accuracy is a key enabling factor of collective intelligence in these contexts.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
Our analyses are based on the two previously published datasets outlined below.  #@NEW_LINE#@#  
Breast_Cancer_Dataset  #@NEW_LINE#@#  
The breast cancer dataset comprises 16,813 interpretations of 182 mammograms made by 101 radiologists (mean number of mammograms evaluated per radiologist = 166, range: 161173) and is one of the largest mammography datasets available (32).  #@NEW_LINE#@#  Mammograms included in the test set were randomly selected from screening examinations performed on women aged 4069 y between 2000 and 2003 from US mammography registries affiliated with the Breast Cancer Surveillance Consortium (BCSC; Carolina Mammography Registry, New Hampshire Mammography Network, New Mexico Mammography Project, Vermont Breast Cancer Surveillance System, and Group Health Cooperative in western Washington).  #@NEW_LINE#@#  Radiologists who interpreted mammograms at facilities affiliated with these registries between January 2005 and December 2006 were invited to participate in this study, as well as radiologists from Oregon, Washington, North Carolina, San Francisco, and New Mexico.  #@NEW_LINE#@#  Of the 409 radiologists invited, 101 completed all procedures and were included in the data analyses.  #@NEW_LINE#@#  Each screening examination included images from the current examination and one previous examination (allowing the radiologists to compare potential changes over time), and presented the craniocaudal and mediolateral oblique views of each breast (four views per woman for each of the screening and comparison examinations).  #@NEW_LINE#@#  This approach is standard practice in the United States (32).  #@NEW_LINE#@#  Women who were diagnosed with cancer within 12 mo of the mammograms were classified as cancer patients (n = 51).  #@NEW_LINE#@#  Women who remained cancer-free for a period of 2 y were classified to be noncancer patients (n = 131; i.e., 28% prevalence).  #@NEW_LINE#@#  
Radiologists viewed the digitized images on a computer (home computer, office computer, or laptop provided as part of the original study).  #@NEW_LINE#@#  The computers were required to meet all viewing requirements of clinical practice, including a large screen and high-resolution graphics (1,280 × 1,024 pixels and a 1280MB video-card with 32-bit color).  #@NEW_LINE#@#  Radiologists saw two images at the same time (i.e., the left and right breasts) and were able to alternate quickly (1 s) between paired images, to magnify a selected part of an image, and to identify abnormalities by clicking on the screen.  #@NEW_LINE#@#  Each case presented craniocaudal and mediolateral oblique views of both breasts simultaneously, followed by each view in combination with its prior comparison image.  #@NEW_LINE#@#  
Cases were shown in random order.  #@NEW_LINE#@#  Radiologists were instructed to diagnose them using the same approach they used in clinical practice (i.e., using the breast imaging reporting and data system lexicon to classify their diagnoses, including their decision that a woman be recalled for further workup).  #@NEW_LINE#@#  
Radiologists evaluated the cases in two stages.  #@NEW_LINE#@#  For stage 1, four test sets were created, with each containing 109 cases (32).  #@NEW_LINE#@#  Radiologists were randomly assigned to one of the four test sets.  #@NEW_LINE#@#  For stage 2, one test set containing 110 cases was created and presented to all radiologists.  #@NEW_LINE#@#  Some of the cases used in stage 2 had already been evaluated by some of the radiologists in part 1.  #@NEW_LINE#@#  To avoid having the same radiologist evaluating a case twice, we excluded all cases from part 2 that had already been viewed by that radiologist in part 1.  #@NEW_LINE#@#  This procedure resulted in a total of 161 unique cases for radiologists in test sets 1 (n = 25 radiologists) and 2 (n = 30 radiologists) and 173 unique cases for radiologists in test sets 3 (n = 26 radiologists) and 4 (n = 20 radiologists), resulting in a total of 16,813 unique readings.  #@NEW_LINE#@#  Between the two stages, radiologists were randomly assigned to one of three intervention treatments.  #@NEW_LINE#@#  Because there were no strong treatment differences (44), we pooled the data from stages 1 and 2.  #@NEW_LINE#@#  For all group simulation analyses, radiologists were always grouped within the four test sets (because radiologists in the same test set had evaluated the same images).  #@NEW_LINE#@#  
In our analysis, we treated the recommendation that a woman should be recalled for further examination as a positive test result.  #@NEW_LINE#@#  After providing each final diagnosis, radiologists rated their confidence in it on a five-point scale.  #@NEW_LINE#@#  

Skin_Cancer_Dataset  #@NEW_LINE#@#  
The skin cancer dataset comprises 4,320 diagnoses by 40 dermatologists of 108 skin lesions and was collected as part of a consensus meeting via the internet, called the Consensus Net Meeting on Dermoscopy (33).  #@NEW_LINE#@#  Skin lesions were obtained from the Department of Dermatology, University Frederico II (Naples, Italy); the Department of Dermatology, University of LAquila (Italy); the Department of Dermatology, University of Graz (Austria); the Sydney Melanoma Unit, Royal Prince Alfred Hospital (Camperdown, Australia); and Skin and Cancer Associates (Plantation, FL).  #@NEW_LINE#@#  The lesions were selected based on the photographic quality of the clinical and dermoscopic images available.  #@NEW_LINE#@#  The goal of the study was to diagnose whether a skin lesion was a melanoma, the most dangerous type of skin cancer.  #@NEW_LINE#@#  Histopathological specimens of all skin lesions were available and judged by a histopathology panel (melanoma: n = 27, nonmelanoma: n = 81; i.e., 25% prevalence).  #@NEW_LINE#@#  
All participating dermatologists had at least 5 y of experience in dermoscopy practice, teaching, and research.  #@NEW_LINE#@#  They first underwent a training procedure in which they familiarized themselves with the studys definitions and procedures in web-based tutorials with 20 sample skin lesions.  #@NEW_LINE#@#  They subsequently evaluated 108 skin lesions in a two-step online procedure.  #@NEW_LINE#@#  First, they used an algorithm to differentiate melanocytic from nonmelanocytic lesions.  #@NEW_LINE#@#  Whenever a lesion was evaluated as melanocytic, the dermatologist was asked to classify it as either melanoma or a benign melanocytic lesion, using four different algorithms.  #@NEW_LINE#@#  Here, we focus on the diagnostic algorithm with the highest diagnostic accuracy which is also the one most widely used for melanoma detection: pattern analysis (33).  #@NEW_LINE#@#  It uses a set of global (textured patterns covering most of the lesion) and local features (representing characteristics that appear in part of the lesion) to differentiate between melanomas and benign melanocytic lesions.  #@NEW_LINE#@#  
We treated the decision to classify a lesion as melanoma as a positive test result.  #@NEW_LINE#@#  After providing each final diagnosis, dermatologists rated their confidence in it on a four-point scale.  #@NEW_LINE#@#  

Ethics_Statement  #@NEW_LINE#@#  
The breast cancer data were assembled at the BCSC Statistical Coordinating Center (SCC) in Seattle and analyzed at the Leibniz Institute of Freshwater Ecology and Inland Fisheries in Berlin (IGB), Germany.  #@NEW_LINE#@#  Each registry, the SCC and the IGB, received institutional review board approval for active and passive consent processes or were granted a waiver of consent to enroll participants, pool data, and perform statistical analysis.  #@NEW_LINE#@#  All procedures were in accordance with the Health Insurance Portability and Accountability Act.  #@NEW_LINE#@#  All data were anonymized to protect the identities of women, radiologists, and facilities.  #@NEW_LINE#@#  The BCSC holds legal ownership of the data.  #@NEW_LINE#@#  Information regarding data requests can be found at breastscreening.cancer.gov/work/proposal_data.html.  #@NEW_LINE#@#  
For the skin cancer data, the review board of the Second University of Naples waived approval because the study did not affect routine procedures.  #@NEW_LINE#@#  All participating dermatologists signed a consent form before participating in the study.  #@NEW_LINE#@#  The skin cancer dataset has been included in Dataset S1.  #@NEW_LINE#@#  

Collective_Intelligence_Rules  #@NEW_LINE#@#  
Both datasets include the judgments of experts who independently evaluated the same cases and rated their confidence in each diagnosis.  #@NEW_LINE#@#  We created virtual groups of diagnosticians who evaluated the cases together using two collective intelligence rules: the confidence rule (17, 20) and the majority rule (34, 35).  #@NEW_LINE#@#  

Statistical_Analyses  #@NEW_LINE#@#  
To determine the similarity in accuracy between group members, we calculated J for each group member and then used the mean pairwise absolute deviation (MPAD) to calculate the similarity in J among group members.  #@NEW_LINE#@#  MPAD =2n(n1)·iless_thanj|JiJj|, where n is the number of diagnosticians i and j.  #@NEW_LINE#@#  For a group size of two, this measure is simply the absolute difference in J between the two group members.  #@NEW_LINE#@#  For a group size of three or more, this measure is the expected absolute difference in J between two randomly chosen group members.  #@NEW_LINE#@#  We analyzed the effect of similarity in accuracy on a groups ability to outperform its best and average individual(s) using general linear models in R (version 3.2.2).  #@NEW_LINE#@#  Significance levels were derived from the t values and associated P values.  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
We thank the editor and three anonymous referees for numerous helpful suggestions on a previous version of our manuscript.  #@NEW_LINE#@#  We thank Susannah Goss, Tim Pleskac, Juliane Kämmer, Aleksandra Litvinova, and members of the Center for Adaptive Rationality for helpful comments on earlier versions of the manuscript.  #@NEW_LINE#@#  We thank Jose Cayere, Amy Buzby, and the American College of Radiology for their technical assistance in developing and supporting the implementation of the test sets; the expert radiologists Larry Bassett, Barbara Monsees, Ed Sickles, and Matthew Wallis; and the participating women, facilities, and radiologists for the data they provided.  #@NEW_LINE#@#  The BCSC investigators are listed at breastscreening.cancer.gov/.  #@NEW_LINE#@#  This work was supported by the American Cancer Society using a donation from the Longaberger Companys Horizon of Hope Campaign (Grants SIRSG-07-271, SIRSG-07-272, SIRSG-07-273, SIRSG-07-274-01, SIRSG-07-275, SIRSG-06-281, SIRSG-09-270-01, SIRSG-09-271-01, and SIRSG-06-290-04); by the Breast Cancer Stamp Fund; and by the National Cancer Institute Breast Cancer Surveillance Consortium (Grant HHSN261201100031C).  #@NEW_LINE#@#  The collection of cancer and vital status data used in this study was supported, in part, by several state public health departments and cancer registries throughout the United States.  #@NEW_LINE#@#  A full description of these sources is provided at www.breastscreening.cancer.gov/work/acknowledgement.html.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  




