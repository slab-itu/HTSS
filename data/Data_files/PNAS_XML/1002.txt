article id="http://dx.doi.org/10.1073/pnas.1712223114"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Musical training sharpens and bonds ears and tongue to hear speech better  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Musical training is a good thing, but does it benefit us in hearing speech in noise as has been suggested?  #@NEW_LINE#@#  If so, what is the brain mechanism behind this?  #@NEW_LINE#@#  Here we provide evidence of musician advantage on speech in noise perception at not only the behavioral level but also the level of neural representations of phonemes and functional connectivity.  #@NEW_LINE#@#  Results implicate better speech encoding in both auditory and speech motor regions, as well as stronger cross-modal auditorymotor integration in musicians than nonmusicians when processing speech, especially in noisy conditions.  #@NEW_LINE#@#  The idea that musical training improves speech in noise perception by enhancing auditorymotor integration is intriguing, with applications in alleviating speech perception difficulties in aging populations and hearing disorders.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
The idea that musical training improves speech perception in challenging listening environments is appealing and of clinical importance, yet the mechanisms of any such musician advantage are not well specified.  #@NEW_LINE#@#  Here, using functional magnetic resonance imaging (fMRI), we found that musicians outperformed nonmusicians in identifying syllables at varying signal-to-noise ratios (SNRs), which was associated with stronger activation of the left inferior frontal and right auditory regions in musicians compared with nonmusicians.  #@NEW_LINE#@#  Moreover, musicians showed greater specificity of phoneme representations in bilateral auditory and speech motor regions (e.g., premotor cortex) at higher SNRs and in the left speech motor regions at lower SNRs, as determined by multivoxel pattern analysis.  #@NEW_LINE#@#  Musical training also enhanced the intrahemispheric and interhemispheric functional connectivity between auditory and speech motor regions.  #@NEW_LINE#@#  Our findings suggest that improved speech in noise perception in musicians relies on stronger recruitment of, finer phonological representations in, and stronger functional connectivity between auditory and frontal speech motor cortices in both hemispheres, regions involved in bottom-up spectrotemporal analyses and top-down articulatory prediction and sensorimotor integration, respectively.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Behaviors  #@NEW_LINE#@#  
Fifteen musicians (see Table S1 for details) and 15 nonmusicians identified English phoneme tokens (/ba/, /ma/, /da/, and /ta/) either alone or embedded in broadband noise at multiple signal-to-noise ratios (SNRs) (12, 8, 4, 0, and 8 dB) during MRI scanning.  #@NEW_LINE#@#  The two groups did not differ in age (t28 = 0.55, P = 0.59), years of postsecondary education (t28 = 0.11, P = 0.91), pure-tone average thresholds (t28 = 0.11, P = 0.91), auditory working memory as measured by the forward and backward digit span subtest of the Wechsler Adult Intelligence Scale (26) (t28 = 0.25, P = 0.81), or nonverbal IQ as measured by Cattells culture fair intelligence test (scale 3 A, ref.  #@NEW_LINE#@#  27) (t28 = 0.51, P = 0.62) (Table S2).  #@NEW_LINE#@#  
A mixed-effects ANOVA on arcsine-transformed accuracy revealed better performance in musicians than nonmusicians (F1,28 = 12.42, P = 0.001, 2 = 0.31).  #@NEW_LINE#@#  Accuracy elevated with increasing SNR in both groups (F5,24 = 260.79, P less_than 0.001, 2 = 0.98), without a significant group × SNR interaction (F5,24 = 2.38, P = 0.07, 2 = 0.33; Fig 1A).  #@NEW_LINE#@#  Accuracy across conditions did not correlate with digit span score (r = 0.24, P = 0.21, n = 30) or culture fair intelligence score (r = 0.28, P = 0.13, n = 30), nor with years of musical practice (r = 0.33, P = 0.24, n = 15) or the age of training onset (r = 0.36, P = 0.19, n = 15) in musicians.  #@NEW_LINE#@#  The two groups did not differ in their reaction time (F1,28 = 0.06, P = 0.81, mixed-effects ANOVA).  #@NEW_LINE#@#  

Effects_on_Regional_Activity  #@NEW_LINE#@#  
Collapsing across listening conditions, both groups showed widespread activation of bilateral perisylvian areas and thalamus, as well as left motor and somatosensory regions when identifying syllables relative to the intertrial baseline [Fig 1B; familywise error-corrected P (PFWE) less_than 0.001].  #@NEW_LINE#@#  Compared with nonmusicians, musicians showed significantly stronger activity in Brocas area of left inferior frontal gyrus (IFG, BA 45), right inferior parietal lobule (IPL, BA 40), and right superior and middle temporal gyri (STG/MTG, BA22/21) (Fig 1C and Table S3; PFWE less_than 0.001).  #@NEW_LINE#@#  Within those three regions of interest (ROIs), the mean activity across conditions in left IFG and right STG/MTG (both r = 0.53, P = 0.043) positively correlated with the mean behavioral accuracy in musicians but not so in nonmusicians (IFG: r = 0.38, P = 0.16; STG/MTG: r = 0.01, P = 0.99).  #@NEW_LINE#@#  

Effects_on_Speech_Representations  #@NEW_LINE#@#  
We further assessed how musical expertise affects the decoding of speech representations via multivoxel pattern analysis (MVPA), which can detect fine-scale spatial patterns instead of mean neural activity elicited by different phonemes.  #@NEW_LINE#@#  MVPA was performed within 42 individually defined anatomical ROIs in both hemispheres, selected independently because they are critical for speech processing according to Neurosynth (Materials and Methods and Fig 2J).  #@NEW_LINE#@#  Those 42 ROIs largely overlap with the activation map elicited by the syllable in noise perception task in our participants (Fig S1), thus validating the selection process.  #@NEW_LINE#@#  
When nonmusicians identified syllables presented alone, significant phoneme classification [area under the curve (AUC)  greater than  0.5 chance level, one-sample t tests with FDR-corrected P less_than 0.05] was observed in posterior STG (pSTG) and postcentral gyrus (postCG) bilaterally and planum temporale (PT), supramarginal gyrus (SMG), precentral gyrus (preCG), pars opercularis (POp), and pars triangularis (PTr) of Brocas area in the left hemisphere (Fig 2B and Table S4).  #@NEW_LINE#@#  In contrast, for musicians at NoNoise condition, phoneme representations could be reliably classified in more widely distributed auditory and frontal motor regions than those found in nonmusicians (more so in the right hemisphere), including additional bilateral Heschls gyrus (HG) and inferior insula, left superior temporal sulcus (STS), MTG and central sulcus, and right PT, preCG, and POp (Fig 2A and Table S4).  #@NEW_LINE#@#  
Musicians also had increased resilience to noise interference on phoneme representations.  #@NEW_LINE#@#  For nonmusicians, weak noise (SNR = 8 dB) disrupted phoneme classification in bilateral auditory regions, with significant decoding revealed only in left frontal motor and somatosensory regions including POp, PTr, inferior precentral sulcus, preCG, and postCG (Fig 2D and Table S4).  #@NEW_LINE#@#  Phoneme specificity in those regions was mainly retained at 0 dB SNR, disappeared except in left postCG at 4 dB SNR, and was not detected at all when the noise further increased (Fig 2 F and H and Table S4).  #@NEW_LINE#@#  For musicians, phoneme classification was significant in bilateral pSTG, preCG, and postCG, as well as in left HG, STS, PT, SMG, central sulcus, and POp at 8 dB SNR (Fig 2C and Table S4), which did not dramatically differ from what was observed when the noise was absent.  #@NEW_LINE#@#  Significant phoneme decoding was further found in right pSTG until 0 dB SNR; in left pSTG, PT, postCG, central sulcus, preCG, and POp at 0 and 4 dB SNRs; and in left postCG and preCG at 8 dB SNR (Fig 2 E, G, and I and Table S4).  #@NEW_LINE#@#  
The musician advantage on phoneme decoding was statistically quantified by a mixed-effects ANOVA that revealed significant group difference on AUC scores (F1,28 = 4.60, P = 0.04, 2 = 0.14) without any significant interaction between group and ROI (F41,1148 = 1.20, P = 0.18) or between group and SNR (F5,140 = 0.94, P = 0.46).  #@NEW_LINE#@#  However, it appeared that training enhancement on phoneme specificity hierarchically moved from bilateral auditory cortices when the noise was weak to the speech motor regions when the noise was intensive.  #@NEW_LINE#@#  For instance, phoneme decoding was revealed in right pSTG until 0 dB SNR and in left pSTG until 4 dB SNR in musicians, whereas decoding was not reliable in nonmusicians auditory regions once the noise was present.  #@NEW_LINE#@#  Although phoneme classification was absent in speech motor regions when SNR less_than 0 dB in nonmusicians, it was significant in left POp until 4 dB SNR and in dorsal/ventral preCG until 8 dB SNR in musicians.  #@NEW_LINE#@#  Thus, musical training was associated with improvement in both auditory decoding and motor prediction-based decoding of speech signals in a pattern-specific fashion, which interacted with the noise intensity in the background.  #@NEW_LINE#@#  
Moreover, improved phoneme representations in speech motor regions and auditorymotor interfaces (e.g., left PT and SMG) predicted better behavioral performance.  #@NEW_LINE#@#  Within the 21 ROIs with significant phoneme decoding at NoNoise condition in musicians (Fig 2A), the overall accuracy positively correlated with the overall AUC scores in left POp (r = 0.70, uncorrected P = 0.000, FDR-corrected P less_than 0.05), left dorsal preCG (r = 0.39, uncorrected P = 0.03), and left SMG (r = 0.38, uncorrected P = 0.039) across all of the subjects (Fig 3A).  #@NEW_LINE#@#  For musicians alone, AUC scores in left POp, left PT, and left SMG predicted accuracy (all r  greater than  0.66, uncorrected P less_than 0.007, FDR-corrected P less_than 0.05).  #@NEW_LINE#@#  Such a correlation was also found in nonmusicians left POp (r = 0.69, uncorrected P = 0.005, FDR-corrected P less_than 0.05).  #@NEW_LINE#@#  

Effects_on_Functional_Connectivity  #@NEW_LINE#@#  
Generalized psychophysiological interaction (gPPI; ref.  #@NEW_LINE#@#  28) was used to investigate how the functional connectivity between auditory and motor regions was modulated by musical experience and the listening environment (i.e., SNR).  #@NEW_LINE#@#  gPPI is configured to evaluate how brain regions interact in a context-dependent manner, i.e., the modulation of functional connectivity by a psychological or behavioral context, when there are more than two task conditions.  #@NEW_LINE#@#  Here an auditory seed combining pSTG and PT Freesurfer ROIs (aparc 2009) in each hemisphere was selected because the two regions together act as auditorymotor interfaces during speech perception and are most likely to send and receive connections to and from motor regions (29).  #@NEW_LINE#@#  A group × SNR mixed-effects ANOVA on PPI estimates revealed significantly (PFWE less_than 0.01) stronger functional connectivity between the left auditory seed and left dorsal PMC (dPMC) and right IFG (including both BA 44 and BA 45) in musicians than nonmusicians.  #@NEW_LINE#@#  For the right auditory seed, stronger connectivity was found with right HG, right IFG (BA 44, BA 45), and left primary motor cortex (M1) regions in musicians than nonmusicians (PFWE less_than 0.01; Fig 4A and Table S5).  #@NEW_LINE#@#  Relative to musicians, nonmusicians showed stronger functional connectivity between bilateral auditory seeds and right cerebellum (Table S5).  #@NEW_LINE#@#  
A significant group by SNR interaction was revealed between the right auditory seed with right anterior STG (aSTG), bilateral ventral PMC (vPMC), and left angular gyrus (AG) (PFWE less_than 0.01; Fig 4B and Table S5).  #@NEW_LINE#@#  Further independent samples t tests showed significantly stronger connectivity in musicians than nonmusicians at 4 dB SNR between the right auditory seed and all four regions: right aSTG (t28 = 3.27, P = 0.003), left vPMC (t28 = 2.96, P = 0.006), right vPMC (t28 = 2.73, P = 0.01), and left AG (t28 = 2.09, P = 0.046).  #@NEW_LINE#@#  Only the connectivity with right aSTG and left vPMC passed correction for multiple comparisons.  #@NEW_LINE#@#  Thus, musicians showed strengthened functional connectivity between auditory regions and some of the motor regions (e.g., left dPMC) regardless of SNR and enhanced connectivity with other motor areas (e.g., left vPMC) when the noise was relatively intense.  #@NEW_LINE#@#  
Moreover, stronger functional connectivity between the right auditory seed and right IFG predicted higher behavioral accuracy across conditions and subjects (r = 0.38, uncorrected P = 0.039; Fig 3B).  #@NEW_LINE#@#  At 4 dB SNR, functional connectivity between the right auditory seed and right aSTG and right vPMC not only was stronger in musicians than nonmusicians (Fig 4B), but also positively correlated with accuracy across subjects (right aSTG: r = 0.47, uncorrected P = 0.009, FDR-corrected P less_than 0.05; right vPMC: r = 0.42, uncorrected P = 0.02; Fig 3C).  #@NEW_LINE#@#  No behaviorPPI correlation was found for each group alone, nor did any correlation reach significance between PPI and classification performance in any region.  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
This study investigates whether and how long-term musical training contributes to enhanced speech perception in noisy environments.  #@NEW_LINE#@#  Using a syllable in noise identification task, we did observe a musician benefit behaviorally.  #@NEW_LINE#@#  Moreover, relative to nonmusicians, improved performance in musicians was paralleled by (i) increased activity of Brocas area in left IFG and right auditory cortex, (ii) higher specificity of phoneme representations in both auditory and motor regions in both hemispheres, and (iii) stronger intrahemispherical and interhemispherical functional connectivity between auditory and motor regions.  #@NEW_LINE#@#  Our findings suggest that musical training may enhance auditory encoding, speech motor prediction, and auditorymotor integration that together contribute to superior speech perception in adverse listening conditions.  #@NEW_LINE#@#  
Musician_Benefit_and_Cognitive_Factors  #@NEW_LINE#@#  
Debates persist regarding whether musical expertise shapes speech perception in challenging listening environments.  #@NEW_LINE#@#  Two recent studies failed to reveal significant differences between musicians and nonmusicians in understanding sentences masked by speech-shaped noise or speech babble (13, 14).  #@NEW_LINE#@#  Still, a musician advantage has been found over a variety of masking conditions and timescales from phonemes to sentences (510).  #@NEW_LINE#@#  The discrepancy between studies might be due to sampling error, musician heterogeneity, or tasks that could be solved using multiple cues (e.g., spatial difference) that may close the gap between musicians and nonmusicians (10).  #@NEW_LINE#@#  In the present study, musicians outperformed nonmusicians in identifying syllables embedded in broadband noise at all SNRs but not in quiet (Fig 1A), supporting the notion that musicianship enhances resistance to noise.  #@NEW_LINE#@#  The task used involves no informational masking and relies little on executive functions such as auditory selective attention, working memory, and cognitive control because listeners do not need to segregate or suppress irrelevant sound or hold information in mind.  #@NEW_LINE#@#  Although enhanced higher-order cognitive processes are found to mediate improved speech in noise perception in musicians at the sentence level (5, 6, 14), they cannot account for the musician benefit here due to the task nature and balanced cognitive scores (auditory working memory and nonverbal IQ) between groups.  #@NEW_LINE#@#  Although there is likely an interplay between genetic and other predispositions with experience-dependent plasticity of brain circuitry that gives rise to the training effects (30), our findings strongly indicate that musical expertise can boost speech in noise perception which is grounded in enhanced neural processing of speech at the phonemic and syllabic levels, irrespective of higher-order cognitive factors.  #@NEW_LINE#@#  
No training effect was found for reaction time, which fits with previous findings (8), although most studies did not report it.  #@NEW_LINE#@#  Additional contrasts on BOLD activity, phoneme classification performance and functional connectivity between participants with shorter and longer reaction time did not reveal any significant effect either.  #@NEW_LINE#@#  Thus, musical training may improve speech discrimination at the perceptual level but may not accelerate the response selection and decision making process.  #@NEW_LINE#@#  Different from expectations (5, 13), accuracy here did not correlate with years of practice or the age of start.  #@NEW_LINE#@#  However, the present study was not designed to examine effects of age of start, because only musicians who started training before 7 were recruited; also, to test effects of years of practice a broader range may be necessary.  #@NEW_LINE#@#  

Enhanced_Auditory_Encoding  #@NEW_LINE#@#  
Speech encoding with high fidelity in the presence of noise along the auditory pathway is necessary for matching neural representations of incoming acoustic signals to stored lexical representations (9).  #@NEW_LINE#@#  Previous studies have underscored the importance of faithful encoding of speech (e.g., frequency following response) in brainstem, thalamus, and cortex for speech in noise perception (17).  #@NEW_LINE#@#  Musicians show superior frequency following responses, including more robust encoding of speech spectral features and greater neural temporal precision (18, 31).  #@NEW_LINE#@#  Musicianship also yields coordinated neural plasticity in brainstem and auditory cortex, and such a refined hierarchy of speech representations may provide more precise phonemic templates to linguistic decisions that contribute to better speech perception (31).  #@NEW_LINE#@#  Here phoneme representations with higher specificity and stronger resistance to noise degradation were revealed in bilateral auditory cortices in musicians than nonmusicians (Fig 2), confirming that enhanced auditory encoding may partially explain the musician benefit on speech in noise perception.  #@NEW_LINE#@#  Notably, it is bilateral pSTG, the auditorymotor interface in the auditory dorsal stream (29), where the phoneme representations were sharpened in noisy conditions by musical experience.  #@NEW_LINE#@#  Additionally, musicians compared with nonmusicians showed stronger recruitment of the right auditory cortex in which the activity scaled with performance.  #@NEW_LINE#@#  

Improved_Speech_Motor_Prediction  #@NEW_LINE#@#  
Motor contributions to speech perception in disambiguating phonological information under adverse listening contexts are increasingly emphasized recently (24).  #@NEW_LINE#@#  For instance, more robust phoneme representations in speech motor regions are suggested to compensate for noise-impoverished speech representations in auditory cortex (22), and such a mechanism becomes more critical for counteracting the age-related declines in speech perception for older listeners (23).  #@NEW_LINE#@#  Because playing music is one of the most complex sensorymotor activities, requiring precise timing of several hierarchically organized actions as well as precise control over pitch interval production (21), plasticity in the motor network is commonly found in musicians (32, 33).  #@NEW_LINE#@#  It is plausible that the motor plasticity may bolster musicians ability to generate more accurate articulatory predictions in a cross-domain fashion that enhances the top-down motoric modulation of speech perception, particularly when the auditory system cannot adequately parse speech signals due to noise masking.  #@NEW_LINE#@#  Indeed, musicians exhibited stronger activity of Brocas area, as well as higher and more robust phoneme specificity in the speech motor system than nonmusicians.  #@NEW_LINE#@#  Specifically, training effects on phoneme representations were found in right PMC and IFG (the counterpart of Brocas area, BA44) when the noise was absent or weak and in left PMC and Brocas area (BA44) when the noise was relatively strong (Fig 2).  #@NEW_LINE#@#  Moreover, how well phonemes were represented in Brocas area (BA44) and auditorymotor interfaces (left PT and SMG) predicted task performance in musicians, and phoneme specificity in left PMC correlated with behavioral accuracy across subjects (Fig 3A), suggesting a direct link between improved speech motoric representations and better performance.  #@NEW_LINE#@#  

Strengthened_AuditoryMotor_Integration  #@NEW_LINE#@#  
Years of musical training should affect not only the auditory and motor modalities separately but also their interactions.  #@NEW_LINE#@#  In fact, musicians show better organized white matter connections (34), greater resting-state connectivity (35), and higher intercorrelations of cortical thickness (32) between auditory and frontal motor cortices.  #@NEW_LINE#@#  In line with the literature, here musicians showed enhanced intrahemispherical and interhemispherical functional connectivity between auditory and speech motor regions (e.g., left dPMC and bilateral vPMC) (Fig 4).  #@NEW_LINE#@#  Furthermore, strengthened auditorymotor connectivity was associated with improved speech in noise perception because subjects with higher connectivity strength between the right auditory seed and right frontal motor regions (e.g., IFG and vPMC) performed better (Fig 3 B and C).  #@NEW_LINE#@#  It is proposed that the vPMC and dPMC are implicated in direct and indirect sensorimotor transformations, respectively (21).  #@NEW_LINE#@#  Mapping auditory features to motor commands when learning to play an instrument engages the vPMC (36) and dPMC (3739).  #@NEW_LINE#@#  The dPMC is putatively involved in extracting higher-order sound features to implement temporally organized actions and allow for predictability in perception (21, 39).  #@NEW_LINE#@#  Listening to speech may not only entail activation of articulatory programs enabled through vPMC links but also engage a neural circuitin which dPMC is a crucial nodethat sets up temporal expectancies in understanding speech.  #@NEW_LINE#@#  Our findings suggest that musical training differentially affects the functional connectivity of dPMC and vPMC with auditory regions depending on the listening contexts.  #@NEW_LINE#@#  It is likely that musicians may have enhanced auditorymotor interplay in temporal prediction of speech signals regardless of the listening condition (40), which, however, needs to be tested in future.  #@NEW_LINE#@#  In parallel, musicians may have strengthened direct mapping of articulatory predictions to auditory inputs, and this function seems to reach its peak at mediate SNRs (e.g., 4 dB) when motoric representations excel auditory representations in specificity as shown before (22, 23).  #@NEW_LINE#@#  

Task_Difficulty_and_Hemispheric_Asymmetry  #@NEW_LINE#@#  
It is reported that the speech motor system is recruited to a greater extent at more challenging conditions (2224).  #@NEW_LINE#@#  Here training-related improvement on speech motoric and auditory representations interacted with task difficulty.  #@NEW_LINE#@#  Specifically, enhanced speech representations dominated musicians bilateral auditory regions (more prominent in the left hemisphere) at high SNRs (NoNoise  4 dB), whereas it targeted right motor regions at high SNRs but left motor regions at low SNRs (4  8 dB).  #@NEW_LINE#@#  This indicates that musicians may benefit from dynamic speech decoding strategies from relying on refined auditory cues to strengthened motor predictions as difficulty increases.  #@NEW_LINE#@#  Thus, musician advantage on speech in noise perception has different contributors from auditory ventral and dorsal streams depending on the listening context.  #@NEW_LINE#@#  
Another finding is that listeners recruit the right hemisphere to a different extent according to their experience.  #@NEW_LINE#@#  Compared with nonmusicians, musicians had increased activity of right auditory cortex and higher phoneme specificity in right auditory and motor regions, as well as stronger functional connectivity between right auditory and bilateral motor regions.  #@NEW_LINE#@#  This fits with previous findings that highlight greater training-related plasticity in right compared with left auditory cortex (1, 32).  #@NEW_LINE#@#  Such an increased contribution by right auditory cortex and the bilaterally organized auditorymotor integration network in musicians may provide superiority in processing speech in noise compared with the left-lateralized one in nonmusicians (20).  #@NEW_LINE#@#  
In sum, this study demonstrates musical training-related benefits of speech in noise perception, which is grounded on increased recruitment of auditory and motor cortices, enhanced auditory decoding and motor prediction, plus strengthened auditorymotor integration.  #@NEW_LINE#@#  Moreover, the auditory and motor contributions to the musician advantage are dynamically weighted according to the task difficulty.  #@NEW_LINE#@#  Musical training thereby has great potentials to set up the communicating brain for healthy aging and hearing disorders (2, 11).  #@NEW_LINE#@#  


Materials_and_Methods  #@NEW_LINE#@#  
Subjects  #@NEW_LINE#@#  
Fifteen musicians (21.4 ± 2.7 y, seven females) and 15 nonmusicians (22.1 ± 4.4 y, seven females) gave informed written consent approved by the McGill University Health Centre Research Ethics Board to participate in the study.  #@NEW_LINE#@#  All subjects were healthy right-handed native English speakers with normal pure-tone thresholds at both ears (less_than25 dB HL for 2508,000 Hz).  #@NEW_LINE#@#  Musicians had started training before age 7, had at least 10 y of musical training, and reported practicing consistently (3 times per week) over the past 3 y (Table S1).  #@NEW_LINE#@#  Nonmusicians reported less than 1 y of musical experience, which did not occur in the year before the experiment.  #@NEW_LINE#@#  

Stimuli_and_Task  #@NEW_LINE#@#  
The stimuli were four naturally produced American English consonantvowel syllables (/ba/, /ma/, /da/, and /ta/), spoken by a female talker.  #@NEW_LINE#@#  Each token was 500 ms in duration and matched for average root-mean-square sound pressure level (SPL).  #@NEW_LINE#@#  A 500-ms white noise segment (4-kHz low-pass, 10-ms risedecay envelope) starting and ending simultaneously with syllables was used as the masker.  #@NEW_LINE#@#  Sounds were played by a TDT (TuckerDavis Technologies) RX-6 real-time processor and presented via MRI-compatible Sensimetrics S14 insert earphones (Sensimetrics Corporation) with Comply foam tips, which maximally attenuate scanner noise by 40 dB.  #@NEW_LINE#@#  The syllables were fixed at 85-dB SPL; the noise level was adjusted to generate five SNRs (12, 8, 4, 0, and 8 dB) and the NoNoise condition.  #@NEW_LINE#@#  
During scanning, 96 stimuli (four trials per syllable per noise condition) were randomly presented in each block with an average interstimulus interval of 4 s (26 s, 0.5-s step), and five blocks were given in total.  #@NEW_LINE#@#  Subjects were asked to identify syllables as fast as possible by pressing one of four keys on a parallel four-button pad using their right hand (index to little fingers in response to /ba/, /da/, /ma/, and /ta/ sequentially).  #@NEW_LINE#@#  

Data_Acquisition_and_Preprocessing  #@NEW_LINE#@#  
Imaging data were collected using a 3.0-T MRI system (Siemens Magnetom Trio) with a 32-channel head coil.  #@NEW_LINE#@#  T1-weighted anatomical images were acquired using a magnetization-prepared rapid acquisition gradient echo sequence (sagittal orientation, 192 slices, repetition time (TR) = 2,300 ms, echo time (TE) = 2.98 ms, field of view (FOV) = 256 mm, voxel size = 1 × 1 × 1 mm).  #@NEW_LINE#@#  T2*-weighted functional images were acquired with a continuous multiband-accelerated echo planar imaging sequence (multiband factor = 4, 40 slices, TR = 636 ms, TE = 30 ms, flip angle = 90°, FOV = 192 mm, voxel size = 3 × 3 × 3 mm).  #@NEW_LINE#@#  The fMRI data were preprocessed using Analysis of Functional NeuroImages (AFNI) software, including slice timing correction, spatial alignment, image coregistration and normalization.  #@NEW_LINE#@#  The preprocessed images were then analyzed by General Linear Model (GLM) and MVPA.  #@NEW_LINE#@#  

General_Linear_Model_Analysis  #@NEW_LINE#@#  
Multiple-regression modeling was performed using the AFNI program 3dDeconvolve.  #@NEW_LINE#@#  Data were fit with different regressors for four syllables and six noise conditions.  #@NEW_LINE#@#  The predicted activation time course was modeled as a gamma function convolved with the canonical hemodynamic response function.  #@NEW_LINE#@#  For each SNR, the four syllables were grouped and contrasted against the baseline (no-stimulus intertrial intervals), as GLM revealed similar activity across syllables.  #@NEW_LINE#@#  Contrast maps were normalized to Talairach stereotaxic space and spatially smoothed using a Gaussian filter (FWHM = 6.0 mm).  #@NEW_LINE#@#  Individual maps at each SNR were then subjected to a mixed-effects ANOVA to test the random effects for each group and the main effect of group on BOLD activity.  #@NEW_LINE#@#  Multiple comparisons were corrected using 3dClustSim with 1,000 Monte Carlo simulations.  #@NEW_LINE#@#  This yielded a PFWE less_than 0.001 by using an uncorrected P less_than 0.001 and removing clusters less_than20 voxels for the group mean activation maps in both groups (Fig 1B).  #@NEW_LINE#@#  For the group difference map, this yielded a PFWE less_than 0.001, with an uncorrected P less_than 0.001, and cluster size 3 voxels (Fig 1C).  #@NEW_LINE#@#  Results were projected onto a cortical inflated surface template using surface mapping (SUMA) with AFNI.  #@NEW_LINE#@#  

Multivoxel_Pattern_Analysis  #@NEW_LINE#@#  
Given the likelihood of high intersubject anatomical variability and fine spatial scale of phoneme representations, pattern classifiers were trained to discriminate neural patterns associated with different phonemes and then tested on independent trials within anatomically defined ROIs.  #@NEW_LINE#@#  To do so, univariate trialwise  coefficients were first estimated using AFNI program 3dLSS (Least Square Sum regression; ref.  #@NEW_LINE#@#  41).  #@NEW_LINE#@#  Then Freesurfers automatic anatomical parcellation (aparc2009; ref.  #@NEW_LINE#@#  42) algorithm was used to define a set of 152 cortical and subcortical ROIs from individuals anatomical scan.  #@NEW_LINE#@#  STG was further divided into equational anterior and posterior portions, and preCG was divided into equational dorsal and ventral parts to dissociate their potentially different contributions to speech perception.  #@NEW_LINE#@#  Next, 21 left and 21 right ROIs sensitive to speech production and perception were selected (Fig 2J).  #@NEW_LINE#@#  This was done by intersecting a metaanalytic mask on Neurosynth (43) (search term: speech) with the Freesurfer mask defined in Montreal Neurological Institute space.  #@NEW_LINE#@#  MVPA were then carried out in the volumetric space within the 42 ROIs at each SNR, using shrinkage discriminant analysis (R package sda; ref.  #@NEW_LINE#@#  44) followed by fivefold cross-validation.  #@NEW_LINE#@#  A multiclass AUC measure computed as the average of all of the pairwise two-class AUC scores was used as an index of classification performance (SI Text).  #@NEW_LINE#@#  
Significance of classification was evaluated by one-sample t tests in each ROI at each SNR, where the null hypothesis assumed a theoretical chance AUC of 0.5.  #@NEW_LINE#@#  Multiple comparisons were corrected by using a FDR q = 0.05.  #@NEW_LINE#@#  AUC scores were also subjected to a mixed-effects ANOVA to evaluate the group difference in classification.  #@NEW_LINE#@#  Results were then projected on the parcellated cortical inflated map associated with the Freesurfer average template (fsaverage) using SUMA.  #@NEW_LINE#@#  

Psychophysiological_Interaction  #@NEW_LINE#@#  
gPPI analysis (28) was implemented to evaluate the SNR-dependent interaction of the functional connectivity between the auditory seeds and all other regions in the brain.  #@NEW_LINE#@#  The auditory seeds were defined by merging the PT and pSTG labels from Freesurfer anatomical parcellation and converted to the volumetric space.  #@NEW_LINE#@#  gPPI was conducted in the volumetric space, and the results were projected on the template surface for visualization.  #@NEW_LINE#@#  A mixed-effects ANOVA on gPPI estimates tested the main effect of group and the group × SNR interaction for each seed, separately.  #@NEW_LINE#@#  Multiple comparisons were corrected by 3dClustSim using an uncorrected P less_than 0.01, which yielded a PFWE less_than 0.01 by removing clusters less_than6 and 10 voxels for the group difference map for the left and right auditory seed, respectively (Fig 4A) and removing clusters less_than6 voxels for the group by SNR interaction map for the right auditory seed (Fig 4B).  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
This research was supported by grants from the National Natural Science Foundation of China (31671172) and the Thousand Young Talent Plan (to Y.D.)  #@NEW_LINE#@#  and the Canadian Institutes of Health Research (Foundation Grant) and an infrastructure grant from the Canada Fund for Innovation (to R.J.Z.  #@NEW_LINE#@#  ).  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  

Published under the PNAS license.  #@NEW_LINE#@#  

