article id="http://dx.doi.org/10.1073/pnas.1704785114"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Adaptive benefit of cross-modal plasticity following cochlear implantation in deaf adults  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Following sensory deprivation, the sensory brain regions can become colonized by the other intact sensory modalities.  #@NEW_LINE#@#  In deaf individuals, evidence suggests that visual language recruits auditory brain regions and may limit hearing restoration with a cochlear implant.  #@NEW_LINE#@#  This suggestion underpins current rehabilitative recommendations that deaf individuals undergoing cochlear implantation should avoid using visual language.  #@NEW_LINE#@#  However, here we show the opposite: Recruitment of auditory brain regions by visual speech after implantation is associated with better speech understanding with a cochlear implant.  #@NEW_LINE#@#  This suggests adaptive benefits of visual communication because visual speech may serve to optimize, rather than hinder, restoration of hearing following implantation.  #@NEW_LINE#@#  These findings have implications for both neuroscientific theory and the clinical rehabilitation of cochlear implant patients worldwide.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
It has been suggested that visual language is maladaptive for hearing restoration with a cochlear implant (CI) due to cross-modal recruitment of auditory brain regions.  #@NEW_LINE#@#  Rehabilitative guidelines therefore discourage the use of visual language.  #@NEW_LINE#@#  However, neuroscientific understanding of cross-modal plasticity following cochlear implantation has been restricted due to incompatibility between established neuroimaging techniques and the surgically implanted electronic and magnetic components of the CI.  #@NEW_LINE#@#  As a solution to this problem, here we used functional near-infrared spectroscopy (fNIRS), a noninvasive optical neuroimaging method that is fully compatible with a CI and safe for repeated testing.  #@NEW_LINE#@#  The aim of this study was to examine cross-modal activation of auditory brain regions by visual speech from before to after implantation and its relation to CI success.  #@NEW_LINE#@#  Using fNIRS, we examined activation of superior temporal cortex to visual speech in the same profoundly deaf adults both before and 6 mo after implantation.  #@NEW_LINE#@#  Patients ability to understand auditory speech with their CI was also measured following 6 mo of CI use.  #@NEW_LINE#@#  Contrary to existing theory, the results demonstrate that increased cross-modal activation of auditory brain regions by visual speech from before to after implantation is associated with better speech understanding with a CI.  #@NEW_LINE#@#  Furthermore, activation of auditory cortex by visual and auditory speech developed in synchrony after implantation.  #@NEW_LINE#@#  Together these findings suggest that cross-modal plasticity by visual speech does not exert previously assumed maladaptive effects on CI success, but instead provides adaptive benefits to the restoration of hearing after implantation through an audiovisual mechanism.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Cross-modal activation of auditory brain regions during a visual speech task (lip-reading) was measured in 15 profoundly deaf individuals before cochlear implantation (T0) and 6 mo after cochlear implantation (T1).  #@NEW_LINE#@#  Fig 1 displays the aggregate sensitivity profiles for our regions of interest (ROIs), illustrating the regions of bilateral superior temporal cortex (STC) to which our measurements were theoretically sensitive.  #@NEW_LINE#@#  
For each individual, we first examined how cross-modal activation of auditory brain regions by visual speech changed from preimplantation to postimplantation.  #@NEW_LINE#@#  The direction and magnitude of change in cross-modal activation varied across the group: nine CI users displayed a decrease in activation, whereas the remaining six displayed an increase.  #@NEW_LINE#@#  The change in cross-modal activation was negatively correlated with the duration of bilateral hearing loss (r = 0.58, P less_than 0.05, two-tailed; Fig S1), with more recently deafened individuals tending to show an increase in cross-modal activation from preimplantation to postimplantation and individuals with a longer duration of deafness tending to show a decrease.  #@NEW_LINE#@#  This suggests that an individuals clinical history of deafness may influence how the brain adapts following cochlear implantation.  #@NEW_LINE#@#  Perhaps unsurprisingly given this level of individual variability, there was no significant change in bilateral STC activation to visual speech at the group level from preimplantation to postimplantation (Fig 2A).  #@NEW_LINE#@#  
Linear mixed model analysis of the data show that (i) there was no significant change in bilateral STC activation to visual speech over time across both CI users and normal hearing (NH) controls (no main effect of time; F1,28.88 = 1.90, P = 0.18; Fig 2A), (ii) there was no significant difference in cortical activation between CI users and NH controls across time points (no main effect of group; F1,34.79 = 0.98, P = 0.33), and (iii) changes in activation to visual speech over time did not differ between the two groups (no grouptime interaction; F1,28.88 = 0.69, P = 0.41).  #@NEW_LINE#@#  
A significant reduction in cross-modal activation to visual speech has previously been documented from approximately 1 wk to 8 mo post-CI within anterior portions of the right superior temporal sulcus (9).  #@NEW_LINE#@#  Thus, we next examined changes in the amplitude of cross-modal activation to visual speech within the left and the right STC separately.  #@NEW_LINE#@#  Although there was no significant change in cross-modal activation of the left STC from preimplantation to postimplantation (no main effect of time; F1,31.07 = 0.09, P = 0.76; Fig 2B), a significant change in cross-modal activation over time was indeed observed within the right STC (main effect of time; F1,30.01 = 6.47, P less_than 0.05; Fig 2C).  #@NEW_LINE#@#  This indicates that the amplitude of cross-modal activation to visual speech within right STC decreased significantly over time when assessed across both groups combined.  #@NEW_LINE#@#  
Data pertaining to changes over time in activation of auditory brain regions by visual speech are not available from existing studies for both CI users and NH control subjects (9).  #@NEW_LINE#@#  We therefore asked whether the observed change over time in right STC activation to visual speech differed between CI users and NH controls.  #@NEW_LINE#@#  The analysis shows that pre- to post-CI changes in right STC activation did not significantly differ between the two groups (no significant main effect of group, F1,27.18 = 1.09, P = 0.31, nor a grouptime interaction, F1,30.01 = 0.49, P = 0.49).  #@NEW_LINE#@#  The absence of a significant grouptime interaction demonstrates that the observed change in activation of right STC to visual speech over time was not specific to the CI group and so cannot be attributed to the implantation process.  #@NEW_LINE#@#  However, the testretest reliability of fNIRS responses to visual speech has been shown to be relatively poor over a retest interval of 3 mo, particularly in the right hemisphere (12).  #@NEW_LINE#@#  Therefore, it is possible that modest testretest reliability prevented us from detecting a grouptime interaction effect.  #@NEW_LINE#@#  
Auditory speech understanding 6 mo after cochlear implantation ranged from 1 to 100% correct, with a mean performance of 71% correct (SD = 33.2).  #@NEW_LINE#@#  The large range of CI outcomes that we observed, as well as the mean performance, is consistent with previous reports from large-scale, international studies (1315), indicating that the CI outcomes observed in the present study may be considered representative of the wider CI population.  #@NEW_LINE#@#  
To identify whether a reduction in cross-modal activation of auditory brain regions by visual speech was necessary for a successful outcome following cochlear implantation, we performed a within-subject analysis to examine the relationship between change in STC activation from preimplantation to postimplantation and speech understanding with the CI.  #@NEW_LINE#@#  There was a strong positive correlation between change in bilateral STC activation to visual speech and speech understanding (r = 0.77, P less_than 0.01, two-tailed; Fig 3).  #@NEW_LINE#@#  Separate correlation analysis of the left and right STC confirmed that this relationship was not driven predominantly by one cerebral hemisphere (left STC: r = 0.63, P less_than 0.05; right STC: r = 0.73, P less_than 0.01, both two-tailed; Fig S2 A and B, respectively).  #@NEW_LINE#@#  Thus, contrary to expectations we found that the best performing CI users showed an increase in cross-modal activation by visual speech from preimplantation to postimplantation, whereas the poorest performing CI users showed a reduction in cross-modal activation over time.  #@NEW_LINE#@#  Because the change in bilateral STC activation to visual speech was associated with the duration of deafness (Fig S1), we also examined the relationship between cross-modal plasticity and CI outcome while controlling for duration of deafness.  #@NEW_LINE#@#  Partial correlation analysis indicated that the observed strong positive correlation between change in bilateral STC activation to visual speech from preimplantation to postimplantation and speech understanding with a CI remained after controlling for the effect of duration of deafness (r = 0.70, P less_than 0.01, two-tailed).  #@NEW_LINE#@#  
It has been assumed that visual language may compromise the ability of auditory brain regions to respond to sound after implantation (3, 16) and that maladaptive cross-modal plasticity must be reversed for CI success (4).  #@NEW_LINE#@#  To explore the mechanisms underlying hearing restoration, we examined whether an increase in responsiveness of auditory brain regions to auditory speech stimulation after implantation was dependent on a decrease in cross-modal activation to visual speech.  #@NEW_LINE#@#  Contrary to expectations, we found a positive correlation between change in bilateral STC activation to auditory speech and change in cross-modal activation to visual speech from T0 to T1 (r = 0.51, P less_than 0.05, two-tailed; Fig 4).  #@NEW_LINE#@#  This relationship between the auditory and visual modality did not exist in the NH control group (r = 0.09, P = 0.74, two-tailed; Fig S3).  #@NEW_LINE#@#  The positive relationship seen between the two sensory modalities in the CI group contradicts the popular, yet simplistic and unsubstantiated, theory of a visual-to-auditory sensory shift within auditory brain regions from preimplantation to postimplantation.  #@NEW_LINE#@#  Rather, they provide evidence of an audiovisual coupling, whereby the responsiveness of auditory brain regions to auditory speech increases in synchrony with their responsiveness to visual speech from preimplantation to postimplantation.  #@NEW_LINE#@#  

Discussion  #@NEW_LINE#@#  
Current CI rehabilitation strategies focus on hearing alone and often discourage the use of vision in the form of lip-reading (17) due to fear of an assumed adverse effect on hearing (18).  #@NEW_LINE#@#  Here we hypothesized that a decrease in cortical activation to visual speech after implantation would be linked to an increase in activation to auditory speech.  #@NEW_LINE#@#  However, the findings of this study do not support this hypothesis: longitudinal optical imaging of the human brain presented here reveals that increased cross-modal activation of auditory brain regions by lip-reading neither precludes an increase in cortical responsiveness to auditory speech, nor limits the recovery of speech understanding after implantation.  #@NEW_LINE#@#  Our findings in cochlear implanted adults parallel recent findings in an animal model showing that cross-modal plasticity within auditory brain regions does not preclude responsiveness to auditory stimulation with a CI and therefore should not be considered strictly maladaptive as traditionally thought (19).  #@NEW_LINE#@#  On the contrary, here we show that increased cross-modal activation after adult cochlear implantation is associated with increased auditory responsiveness and better speech understanding with a CI, indicating an adaptive benefit of cross-modal plasticity following implantation.  #@NEW_LINE#@#  
Previous postimplant imaging studies have identified subregions which differ in the direction and extent to which cross-modal STC activation to visual speech correlates with CI outcomes (8).  #@NEW_LINE#@#  Given the limited spatial resolution of fNIRS, it is not possible here to interrogate cortical activation in these individual subregions.  #@NEW_LINE#@#  Furthermore, given the large-scale averaging across millions of neurons that is inherent to all noninvasive neuroimaging techniques (and to fNIRS especially), it is not possible to classify whether it is the same population of neurons in the STC that is responding to the visual stimulus in the CI and NH groups, nor to characterize their precise nature.  #@NEW_LINE#@#  Therefore, although we use the term cross-modal to refer to putatively auditory brain regions being cross-activated by a different modality (vision), it is possible that this activation may be multimodal in its nature (i.e., reflects the activity of multisensory neurons that respond to both auditory and visual inputs).  #@NEW_LINE#@#  Nonetheless, despite greater spatial averaging, our findings show that changes from preimplantation to postimplantation in temporal lobe activation by visual speech are functionally relevant to CI outcome.  #@NEW_LINE#@#  
Our findings argue against the common view that visual language has a maladaptive effect on CI success due to cross-modal plasticity within auditory brain regions, indicating that the effects of cross-modal plasticity on sensory restoration are more complex than previously thought (5).  #@NEW_LINE#@#  Rather, our results provide evidence that increased cross-modal activation of auditory brain regions by visual speech may offer a facilitative link between the two modalities that promotes auditory recovery after cochlear implantation.  #@NEW_LINE#@#  Cross-modal activation of superior temporal cortex by visual speech may reflect processes such as inner speech and auditory imagery due to the inherent correspondence that exists between auditory and visual speech representations (20).  #@NEW_LINE#@#  In this way, an increase in STC activation to visual speech may reflect a stronger correspondence or synergy between the modalities that may facilitate auditory recovery.  #@NEW_LINE#@#  Indeed, multisensory integration of auditory and visual speech cues can enhance speech perception and is a skill shown to be enhanced in cochlear implant users compared with normal hearing individuals (21).  #@NEW_LINE#@#  Our finding of a synergistic link between the auditory and visual modality following cochlear implantation appears compatible with this suggestion that CI users are better multisensory integrators of auditory and visual speech cues (21).  #@NEW_LINE#@#  Furthermore, the regions of interest interrogated here include posterior regions of the STC, which are heavily implicated in audiovisual speech integration (22, 23).  #@NEW_LINE#@#  Therefore, the positive relationship observed between the two modalities here may reflect CI users continued reliance on visual speech cues and their integration with auditory information to decipher the degraded auditory signal provided by the implant (21, 24).  #@NEW_LINE#@#  
The underlying mechanisms responsible for yoking together the observed changes in responsiveness to auditory and visual stimulation within the CI group remain unclear.  #@NEW_LINE#@#  It has been proposed that vision may facilitate auditory perceptual learning by guiding top-down attention to auditory representations (25).  #@NEW_LINE#@#  As such, it is possible that changes in visual and auditory responsiveness of the STC over time may be linked through a mediating effect of top-down attention.  #@NEW_LINE#@#  It is also possible that the responses we measured from the STC may partly reflect generalized supramodal linguistic processing, for example, of phonological (26) or semantic information (27).  #@NEW_LINE#@#  Such supramodal linguistic networks may be increasingly activated by both audition and vision, as an individual CI patient learns to optimally integrate auditory and visual information to maximize language understanding.  #@NEW_LINE#@#  In an animal model, vision has been shown to play a facilitative role in restoring sound localization abilities after cochlear implantation (28).  #@NEW_LINE#@#  In parallel, our findings provide unique evidence in humans for a synergistic relationship between audition and vision within auditory brain regions, indicating a facilitative mechanism between the modalities that underlies the restoration of speech understanding following cochlear implantation.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
Participants  #@NEW_LINE#@#  
The study was approved by the Nottingham 1 Research Ethics Committee (reference: 12/EM/0016) and was sponsored by Nottingham University Hospitals National Health Service (NHS) Trust (Research & Innovation reference: 11IH007).  #@NEW_LINE#@#  All participants gave written informed consent before taking part.  #@NEW_LINE#@#  Common inclusion criteria across both groups were native English speakers, self-reported normal or corrected-to-normal vision, at least 18 y of age, and able to travel to and take part in all study assessments.  #@NEW_LINE#@#  Exclusion criteria were any known language, cognitive, or motor disorder or previous brain injury.  #@NEW_LINE#@#  

CI_Users  #@NEW_LINE#@#  
We recruited 17 adults with bilateral profound deafness who had consented to, but had not yet received, their CI device.  #@NEW_LINE#@#  The group included two prelingually, three perilingually, and 12 postlingually deaf individuals who were heterogeneous in their clinical characteristics (Table 1), as is typical of individuals presenting across CI clinics.  #@NEW_LINE#@#  All participants met United Kingdom national guidelines for cochlear implantation and had been deemed suitable CI candidates by the Nottingham Auditory Implant Program.  #@NEW_LINE#@#  All participants were implanted unilaterally with a Cochlear Nucleus 6 device with CP910 sound processor that used the advanced combination encoder stimulation strategy (SI Materials and Methods for further clinical information).  #@NEW_LINE#@#  One CI user was excluded from all analyses due to excessive motion and poor contact between fNIRS optodes and the scalp, resulting in poor data quality.  #@NEW_LINE#@#  Another CI user was withdrawn from the study at T1 for unrelated medical reasons.  #@NEW_LINE#@#  

Control_Subjects  #@NEW_LINE#@#  
Seventeen NH adults were recruited to serve as a control group.  #@NEW_LINE#@#  All participants had normal hearing thresholds, defined here as average pure-tone air conduction hearing thresholds of 20 decibels (dB) across frequencies 0.5, 1, 2, and 4 kHz in both ears.  #@NEW_LINE#@#  Audiometric testing was conducted at the beginning of each participants first study visit.  #@NEW_LINE#@#  The recruitment of control subjects was staggered in an attempt to approximately match the groups mean age (57 y ±16.8) to that of the CI users (58.2 y ±13.9).  #@NEW_LINE#@#  Due to attrition, one NH control subject did not complete testing at T1.  #@NEW_LINE#@#  

Experimental_Design  #@NEW_LINE#@#  
A longitudinal repeated-measures design was used.  #@NEW_LINE#@#  The same neuroimaging and behavioral tests were administered to all participants at two time points.  #@NEW_LINE#@#  For CI users, the first testing session (T0) took place at their earliest convenience after having consented to receive a CI but before undergoing surgery (preimplantation).  #@NEW_LINE#@#  At T0, CI users were tested in their best-aided condition, i.e., wearing their hearing aids if they used them in everyday conditions.  #@NEW_LINE#@#  The second testing session (T1) was conducted approximately 6 mo after activation of the CI (postimplantation, average duration of CI use = 6.1 mo, SD = 0.4).  #@NEW_LINE#@#  At T1, CI users were tested in their best-aided condition wearing their preferred listening devices (i.e., CI and optional contralateral hearing aid).  #@NEW_LINE#@#  The mean retest interval between T0 and T1 was 8.2 mo (SD = 1.2).  #@NEW_LINE#@#  
NH control subjects similarly underwent testing in two sessions.  #@NEW_LINE#@#  The T0T1 retest interval was set to mirror that of the CI group as closely as was pragmatically possible, given the variation in clinical waiting times for the CI operation and device activation.  #@NEW_LINE#@#  The mean retest interval between T0 and T1 was 8.1 mo (SD = 0.3).  #@NEW_LINE#@#  

Testing_Conditions  #@NEW_LINE#@#  
Testing was carried out in a double-walled sound-attenuated booth.  #@NEW_LINE#@#  Participants were seated in front of a visual display unit (VDU) at a viewing distance of 1 m. Visual components of the stimuli were presented on the VDU.  #@NEW_LINE#@#  To reflect the typical level of conversational speech, auditory components were presented through a centrally located loudspeaker at 65 dB sound pressure level (SPL; A-weighted root-mean-square level averaged over the duration of each sentence).  #@NEW_LINE#@#  See SI Materials and Methods for further information.  #@NEW_LINE#@#  

fNIRS_Scanning  #@NEW_LINE#@#  
In each testing session, cortical activation was measured using a continuous-wave fNIRS system (ETG-4000; Hitachi Medical Co.).  #@NEW_LINE#@#  The ETG-4000 is a commercial system that emits a continuous beam of light into the cortex and samples at a rate of 10 Hz.  #@NEW_LINE#@#  The system measures simultaneously at two wavelengths, 695 and 830 nm, to allow for the separate measurement of changes in oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR) concentrations.  #@NEW_LINE#@#  This specific choice of wavelengths has been shown to minimize cross-talk error between the two chromophores (29).  #@NEW_LINE#@#  

fNIRS_Stimuli  #@NEW_LINE#@#  
The Institute of Hearing Research (IHR) Number Sentences (20) were presented as speech stimuli during the acquisition of fNIRS measurements.  #@NEW_LINE#@#  The corpus comprised digital audiovisual recordings of 90 sentences, each spoken by both a male and female talker.  #@NEW_LINE#@#  Each of the sentences contained between four and seven words, three of which were designated keywords.  #@NEW_LINE#@#  For the purpose of this experiment, the speech material was presented in two stimulation conditions: (i) auditory-only (A-ONLY) where the auditory component was presented but the visual component was not shown and (ii) visual-only (i.e., lip-reading, V-ONLY) where the visual component of the recording was shown but the auditory component was muted.  #@NEW_LINE#@#  The speech material was also presented in an audiovisual condition (auditory and visual components presented congruently) for the purpose of a separate experiment to be reported elsewhere.  #@NEW_LINE#@#  In the A-ONLY condition the background remained uniform and a fixation cross was presented in place of the talkers mouth.  #@NEW_LINE#@#  Rest periods consisted of this uniform background and fixation cross only.  #@NEW_LINE#@#  

fNIRS_Paradigm  #@NEW_LINE#@#  
Thirty IHR number sentences were randomly selected without replacement for presentation in each of the conditions, with the restriction that an equal number were spoken by the male and female talker in each condition.  #@NEW_LINE#@#  The speech stimuli were presented in a block-design paradigm interleaved with rest periods.  #@NEW_LINE#@#  Each block comprised six concatenated sentences, evenly spaced to fill a 24-s block duration.  #@NEW_LINE#@#  Five blocks were presented for each stimulation condition.  #@NEW_LINE#@#  During these blocks, the participants were instructed to attend to the talker and to always try to understand what the talker was saying.  #@NEW_LINE#@#  To encourage sustained attention to the experimental stimuli, an attentional trial was presented after 2 of the 15 stimulation blocks.  #@NEW_LINE#@#  These blocks were chosen at random, and therefore, the attentional trials occurred at unpredictable positions within the experimental run.  #@NEW_LINE#@#  Two seconds after the cessation of a chosen block, two alternative words were presented on either side of the fixation cross; in a two-alternative forced-choice task, participants were asked to press one of two buttons to indicate which word had been spoken in the immediately preceding sentence.  #@NEW_LINE#@#  Following the participants response, an additional 5-s rest was added to the start of the ensuing rest period.  #@NEW_LINE#@#  Rest periods were included to allow the hemodynamic response elicited by the stimulation block to return to a baseline level.  #@NEW_LINE#@#  The durations of the rest periods were randomly varied between 20 and 40 s in 5-s increments.  #@NEW_LINE#@#  Before fNIRS scanning, participants first completed a short familiarization run to ensure that they understood the experimental procedure (SI Materials and Methods for further details).  #@NEW_LINE#@#  

Optode_Placement  #@NEW_LINE#@#  
Two 3 × 3 optode arrays were placed bilaterally over the subjects temporal lobes.  #@NEW_LINE#@#  The optode arrays were positioned on the participants head so as to ensure good coverage of the STC (Fig 1 and Fig S4).  #@NEW_LINE#@#  Optode positioning was guided by the International 1020 System (30) to promote consistency across participants and test sessions (SI Materials and Methods for further details).  #@NEW_LINE#@#  

Definition_of_ROI  #@NEW_LINE#@#  
To assess the sensitivity of our fNIRS measurements to the underlying cortical regions, using the AtlasViewer tool (31) a Monte Carlo code for simulating the probabilistic path of photon migration through the head (32) (tMCimg) was run with 1 × 107 simulated photons launched from each optode position.  #@NEW_LINE#@#  The resultant sensitivity profiles (Fig 1) suggested that channels 9, 10, and 12 (left hemisphere) and channels 20, 21, and 23 (right hemisphere) provided appropriate sensitivity to the posterior portion of STC.  #@NEW_LINE#@#  Therefore, these measurement channels were predefined as the left and right superior temporal ROIs.  #@NEW_LINE#@#  The left and right ROIs together formed the bilateral STC ROI.  #@NEW_LINE#@#  

Behavioral_Test_of_Speech_Understanding  #@NEW_LINE#@#  
The City University of New York (CUNY) Sentence Lists (33) were used to obtain a measure of speech understanding (SI Materials and Methods for further details).  #@NEW_LINE#@#  The CUNY Sentence Lists include 25 standardized lists each comprising 12 sentences that vary in length and topic.  #@NEW_LINE#@#  Each list contains between 101 and 103 words spoken by a male talker.  #@NEW_LINE#@#  
For the purpose of this experiment, two CUNY lists (i.e., 24 sentences) were randomly selected without replacement for presentation in the A-ONLY stimulation condition.  #@NEW_LINE#@#  Speech understanding in V-ONLY and audiovisual modalities was also tested for the purpose of a separate experiment to be reported elsewhere.  #@NEW_LINE#@#  The 24 sentences were presented in random order.  #@NEW_LINE#@#  After each sentence presentation, the participant was instructed to repeat back all words that they were able to identify.  #@NEW_LINE#@#  All words correctly reported by the participant were recorded by the researcher on a scoring laptop before initiation of the next trial.  #@NEW_LINE#@#  The scoring method ignored errors of case or declensions.  #@NEW_LINE#@#  Before commencement of speech understanding testing, all participants completed a short familiarization run (SI Materials and Methods).  #@NEW_LINE#@#  

Processing_of_fNIRS_Data  #@NEW_LINE#@#  
Raw fNIRS recordings were exported from the Hitachi ETG-4000 into MATLAB for use with routines provided in the HOMER2 package (34) and custom scripts.  #@NEW_LINE#@#  To prepare the recordings for subsequent analyses they were subjected to a set of preprocessing steps, including motion artifact correction, bandpass filtering, and hemodynamic signal separation.  #@NEW_LINE#@#  Full details of all preprocessing steps are provided in SI Materials and Methods.  #@NEW_LINE#@#  To quantify the level of cortical activation, the preprocessed fNIRS signal was subjected to an ordinary least squares (OLS) general linear model (GLM).  #@NEW_LINE#@#  The GLM design matrix included three boxcar regressors, one for each of the stimulation conditions.  #@NEW_LINE#@#  The two response periods following the two attentional trials were also modeled in the design matrix as isolated events occurring at the time the two words were presented on screen.  #@NEW_LINE#@#  These were convolved with the canonical hemodynamic response function provided in Statistical Parametric Mapping software version 8 (www.fil.ion.ucl.ac.uk/spm).  #@NEW_LINE#@#  After completing the first-stage OLS estimation at the single-subject level, we used the CochraneOrcutt procedure (35) to correct for serial correlation.  #@NEW_LINE#@#  Briefly, this involved fitting a first-order autoregressive process to the model residuals and transforming the original model according to the estimated autoregressive parameter (see ref.  #@NEW_LINE#@#  36).  #@NEW_LINE#@#  We then reestimated the beta weights based on the transformed model (second stage).  #@NEW_LINE#@#  
The beta weights of the canonical hemodynamic response function term were extracted at each measurement channel, for each stimulation condition, and for all participants.  #@NEW_LINE#@#  The hemodynamic signal separation method used here (37) (SI Materials and Methods) assumes a fixed linear relationship between HbO and HbR in the functional response.  #@NEW_LINE#@#  Therefore, the results of all statistical analyses are identical regardless of whether conducted on the beta weights extracted for the HbO or HbR parameter.  #@NEW_LINE#@#  For simplicity, only results pertaining to the beta estimates of the HbO parameter of the functional component are presented here.  #@NEW_LINE#@#  These beta weights were used to quantify the amplitude of cortical activation for each condition compared with rest.  #@NEW_LINE#@#  The resultant beta weights were averaged across the ROI measurement channels for each group and at each time point and were subjected to further statistical analysis as outlined below.  #@NEW_LINE#@#  

Processing_of_Behavioral_Data  #@NEW_LINE#@#  
Speech understanding, measured using the CUNY Sentence Lists, was quantified as the percentage of words reported correctly (% correct).  #@NEW_LINE#@#  To make the data more suitable for statistical analysis, the rationalized arcsine transform (38) was applied using MATLAB (SI Materials and Methods for details).  #@NEW_LINE#@#  Subsequently, the transformed scores [rationalized arcsine units (RAUs)] were subjected to statistical analysis.  #@NEW_LINE#@#  

Statistical_Analysis  #@NEW_LINE#@#  
Following the preprocessing of neuroimaging and behavioral data, resultant data were analyzed and figures were produced using IBM (International Business Machines Corporation) Statistical Package for the Social Sciences (SPSS) Statistics software (Release 22.0; IBM Corp.).  #@NEW_LINE#@#  Data and analysis scripts are publically available through the University of Nottinghams Research Data Management Repository (https://rdmc.nottingham.ac.uk).  #@NEW_LINE#@#  

Linear_Mixed_Model_Analysis  #@NEW_LINE#@#  
The ROI beta weights were analyzed separately for the bilateral, left, and right ROI using a linear mixed model (LMM; see SI Materials and Methods for further information).  #@NEW_LINE#@#  Each model included two fixed factors of group and time to estimate the fixed effect of experimental group (CI users versus NH controls) and time relative to implantation (T0, before implantation; T1, 6 mo after CI activation) on cross-modal activation.  #@NEW_LINE#@#  In addition, a grouptime interaction term was specified to understand whether an effect of time on cortical activation differed between the two groups.  #@NEW_LINE#@#  Specifically, if a grouptime interaction indicated that cross-modal activation changed over time in the CI group but remained comparatively stable in the NH group, this would suggest an effect specific to the CI process.  #@NEW_LINE#@#  

Correlational_Analysis  #@NEW_LINE#@#  
Change in amplitude of cross-modal activation from preimplantation to postimplantation was calculated as the difference between the amplitude (beta weight) of STC activation to visual speech measured at T0 and T1.  #@NEW_LINE#@#  Bivariate correlation analysis was conducted to examine the nature of the relationship between change in cross-modal activation ( beta weight) and speech understanding (RAU).  #@NEW_LINE#@#  Specifically, the parametric statistic Pearsons correlation coefficient (r) was used to estimate the direction and strength of the linear relationship.  #@NEW_LINE#@#  Similarly, Pearsons correlation was conducted to examine the direction and strength of the relationship between change in cross-modal activation and change in amplitude of STC activation to auditory speech (auditory responsiveness).  #@NEW_LINE#@#  


SI_Materials_and_Methods  #@NEW_LINE#@#  
CI_Users  #@NEW_LINE#@#  
None of the participants experienced any complications during their CI surgery, and no abnormalities were identified on postoperative X-ray.  #@NEW_LINE#@#  For all participants, all implantable electrodes were situated within the cochlea, and postoperative impedances were within normal range on all electrodes.  #@NEW_LINE#@#  For one participant, activation of the device was delayed by 1 wk due to an infection around the surgical site of incision.  #@NEW_LINE#@#  All participants were stimulated in monopolar configuration, and comfort and threshold levels were estimated for each electrode position by the clinical team according to standard clinical protocols.  #@NEW_LINE#@#  

Testing_Conditions  #@NEW_LINE#@#  
All stimuli were presented, and scored where appropriate, using the MATLAB computing environment (Release 2014b; The MathWorks).  #@NEW_LINE#@#  A centrally located Genelec 8030A loudspeaker was mounted immediately above and behind the visual display unit.  #@NEW_LINE#@#  The SPL of auditory stimuli was measured at the listening position with the participant absent using a Brüel & Kjær 2250 sound level meter and free-field microphone (Type 4189).  #@NEW_LINE#@#  A dense sound-absorbing screen was placed between the fNIRS equipment and the participant to attenuate the steady fan noise generated by the equipment.  #@NEW_LINE#@#  This resulted in a steady ambient noise level of 38 dB SPL (A-weighted).  #@NEW_LINE#@#  The fNIRS equipment remained switched on, although not recording, throughout behavioral testing.  #@NEW_LINE#@#  Before the commencement of each test, participants were provided with written instructions to ensure understanding and consistency of instructions given.  #@NEW_LINE#@#  

Familiarization_Runs  #@NEW_LINE#@#  
During the fNIRS familiarization session, one block of each of the conditions was presented.  #@NEW_LINE#@#  To avoid preexposure to the experimental stimuli, the familiarization blocks comprised speech material [BKB (BamfordKowalBench) sentences (39)] that were different from the material presented during the fNIRS measurements and the subsequent behavioral testing.  #@NEW_LINE#@#  Following each stimulation block, an example of the attentional control task was also presented.  #@NEW_LINE#@#  Similarly, BKB sentences were used during the behavioral familiarization run to avoid preexposure to the CUNY corpus.  #@NEW_LINE#@#  

Optode_Placement  #@NEW_LINE#@#  
Together, the optode arrays comprised 10 emitter and 8 detector optodes with a fixed interoptode distance of 30 mm, providing a penetration depth into the cortex of 15 mm (40).  #@NEW_LINE#@#  This resulted in a total of 24 measurement channels (12 per hemisphere).  #@NEW_LINE#@#  Optode positioning was guided by the International 1020 System (30) to promote consistency across participants and test sessions.  #@NEW_LINE#@#  Specifically, on each side, the lowermost source optode was placed as close as possible to the preauricular point, with the uppermost source optode aligned toward Cz.  #@NEW_LINE#@#  Consistency of optode positioning across test sessions at the individual level was further ensured by reference to photographs taken during the initial testing session.  #@NEW_LINE#@#  
To evaluate the consistency of optode positioning across individuals, the procedure was piloted on six adult volunteers who did not take part in the main experiment.  #@NEW_LINE#@#  After positioning the arrays as described above, the optode positions, plus anatomical surface landmarks, were recorded using the Hitachi ETG-4000s electromagnetic 3D Probe Positioning Unit.  #@NEW_LINE#@#  For each volunteer, the digitized optode positions were registered to a standard atlas brain (41) (Colin27) using the AtlasViewer tool (31), allowing their locations to be visualized relative to underlying cortical anatomy.  #@NEW_LINE#@#  The SD in the position of each optode was between 2.9 and 8.8 mm.  #@NEW_LINE#@#  Assessment of the mean optode positions suggested that the array provided good coverage of STC (Fig S4).  #@NEW_LINE#@#  

Behavioral_Test_of_Speech_Understanding  #@NEW_LINE#@#  
The CUNY corpus was used primarily due to its routine use as a clinical outcome measure by CI programs across the United Kingdom.  #@NEW_LINE#@#  Additionally, this corpus was not presented during fNIRS scanning, thus helping to limit training effects within and across testing sessions.  #@NEW_LINE#@#  

Processing_of_fNIRS_Data  #@NEW_LINE#@#  
Raw light intensity measurements were first converted to change in optical density (34).  #@NEW_LINE#@#  Wavelet motion correction was then performed to reduce the effect of motion artifacts on the fNIRS signal.  #@NEW_LINE#@#  Wavelet filtering can enhance data yield and has emerged as a favorable approach for use with fNIRS data (42).  #@NEW_LINE#@#  The HOMER2 hmrMotionCorrectWavelet function (based on ref.  #@NEW_LINE#@#  42) was used, which assumes that the wavelet coefficients have a Gaussian probability distribution and so applies a probability threshold to remove outlying wavelet coefficients that are assumed to correspond to motion artifacts.  #@NEW_LINE#@#  A probability threshold was set to exclude coefficients lying more than 1.5 interquartile ranges below the first quartile or above the third quartile.  #@NEW_LINE#@#  
Following motion artifact correction, a bandpass filter of 0.010.5 Hz was applied to reduce sources of physiological noise in the data including high-frequency cardiac oscillations and low-frequency respiration and blood pressure changes.  #@NEW_LINE#@#  The fNIRS signal was next converted into estimates of changes in HbO and HbR using the modified BeerLambert law with a default differential path length factor of six (34).  #@NEW_LINE#@#  Because bandpass filtering is unable to remove all physiological noise from fNIRS recordings (34), the hemodynamic signal separation method of (37) was also applied.  #@NEW_LINE#@#  This algorithm separates the fNIRS signal into estimates of the functional and systemic components, based on expected differences in the correlation between HbO and HbR in each component.  #@NEW_LINE#@#  Specifically, a positive correlation between changes in HbO and HbR is assumed in the systemic component, whereas a negative correlation is assumed in the functional component.  #@NEW_LINE#@#  The functional component of the signal was identified by the algorithm, extracted from the fNIRS signal and retained for further analysis.  #@NEW_LINE#@#  

Processing_of_Behavioral_Data  #@NEW_LINE#@#  
The arcsine transform (T) was applied as follows:T=arcsineXN+1+arcsineX+1N+1.The asin function in Matlab was used to return the inverse sine (arcsine) for each value of X, where X represents the total number of words reported correctly and N represents the total number of words presented.  #@NEW_LINE#@#  This was then transformed linearly:R=46.47324337T23,where R indicates the resulting rationalized arcsine-transformed score (RAU).  #@NEW_LINE#@#  This transformation extends the original percent correct scale outwards in both directions from 50%, creating bigger differences as the extremes of the range are approached.  #@NEW_LINE#@#  Consequently, this transformation makes the rationalized arcsine scale linear and additive in its proportions while producing values close to that of the original percentage scores (38).  #@NEW_LINE#@#  

Linear_Mixed_Model_Analysis  #@NEW_LINE#@#  
LMMs are able to give accurate estimates of the fixed effect of predictor variables while incorporating and accounting for the random effects that are often inherent in repeated-measures designs (43).  #@NEW_LINE#@#  Missing data were missing for reasons that were unrelated to the observed data values (i.e., attrition and unrelated medical condition).  #@NEW_LINE#@#  The LMMs were therefore conducted under the assumption that missing data were missing at random (43).  #@NEW_LINE#@#  Thus, the LMMs maximized the use of available data rather than entirely omitting subjects with missing data values from the analysis.  #@NEW_LINE#@#  
Due to the repeated measures design, participant was specified as a variable to incorporate within-participant covariance into the fixed effect estimation.  #@NEW_LINE#@#  Furthermore, a random intercept effect and random effect of time were also specified to facilitate the estimation of the fixed effect of group and time while accounting for possible between-subject variability in overall fNIRS response amplitude and the trajectory of change over time.  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
We thank the CI recipients and control subjects who took part in this study and the Nottingham Auditory Implant Programme for their help with recruitment.  #@NEW_LINE#@#  We thank Dr. Diane Lazard for her comments on an earlier version of the manuscript.  #@NEW_LINE#@#  This research was funded by the National Institute for Health Research (NIHR) Biomedical Research Unit Program, and was partly supported by funding from the Medical Research Council.  #@NEW_LINE#@#  C.A.A.  #@NEW_LINE#@#  was supported by an NIHR doctoral award and by an educational sponsorship from Cochlear Europe Ltd.  #@NEW_LINE#@#  The views expressed are those of the authors and not necessarily those of the NHS, the NIHR, or the Department of Health.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  



