article id="http://dx.doi.org/10.1073/pnas.1620350114"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Visual cortex entrains to sign language  #@NEW_LINE#@#  

Significance  #@NEW_LINE#@#  
Language comprehension is thought to rely on a combination of specialized and general-purpose neural mechanisms.  #@NEW_LINE#@#  When people listen to speech, low-frequency oscillations in cerebral cortex (less_than8 Hz) become entrained to quasirhythmic fluctuations in volume.  #@NEW_LINE#@#  Entrainment of auditory cortex to speech rhythms has been well documented, but its functional significance has remained unclear.  #@NEW_LINE#@#  By showing similar entrainment of visual cortex to sign language, we establish that this phase-locking is not due to specific properties of auditory cortex or of oral speech perception.  #@NEW_LINE#@#  Rather, low-frequency entrainment is a generalized cortical strategy for boosting perceptual sensitivity to informational peaks in time-varying signals.  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Despite immense variability across languages, people can learn to understand any human language, spoken or signed.  #@NEW_LINE#@#  What neural mechanisms allow people to comprehend language across sensory modalities?  #@NEW_LINE#@#  When people listen to speech, electrophysiological oscillations in auditory cortex entrain to slow (less_than8 Hz) fluctuations in the acoustic envelope.  #@NEW_LINE#@#  Entrainment to the speech envelope may reflect mechanisms specialized for auditory perception.  #@NEW_LINE#@#  Alternatively, flexible entrainment may be a general-purpose cortical mechanism that optimizes sensitivity to rhythmic information regardless of modality.  #@NEW_LINE#@#  Here, we test these proposals by examining cortical coherence to visual information in sign language.  #@NEW_LINE#@#  First, we develop a metric to quantify visual change over time.  #@NEW_LINE#@#  We find quasiperiodic fluctuations in sign language, characterized by lower frequencies than fluctuations in speech.  #@NEW_LINE#@#  Next, we test for entrainment of neural oscillations to visual change in sign language, using electroencephalography (EEG) in fluent speakers of American Sign Language (ASL) as they watch videos in ASL.  #@NEW_LINE#@#  We find significant cortical entrainment to visual oscillations in sign language less_than5 Hz, peaking at 1 Hz.  #@NEW_LINE#@#  Coherence to sign is strongest over occipital and parietal cortex, in contrast to speech, where coherence is strongest over the auditory cortex.  #@NEW_LINE#@#  Nonsigners also show coherence to sign language, but entrainment at frontal sites is reduced relative to fluent signers.  #@NEW_LINE#@#  These results demonstrate that flexible cortical entrainment to language does not depend on neural processes that are specific to auditory speech perception.  #@NEW_LINE#@#  Low-frequency oscillatory entrainment may reflect a general cortical mechanism that maximizes sensitivity to informational peaks in time-varying signals.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Developing_a_Metric_for_Quantifying_Visual_Change  #@NEW_LINE#@#  
To examine neural entrainment to visual rhythms in sign language, we must first quantify the amplitude envelope of a visual signal.  #@NEW_LINE#@#  The acoustic envelope is a highly reduced representation of sound, tracing extreme amplitude values in the time-varying signal.  #@NEW_LINE#@#  Oscillations in the envelope of speech depend on movements of various components of the vocal tract, including the rhythmic opening and closing of the mandible (22).  #@NEW_LINE#@#  Sign language, in contrast, does not involve consistent oscillatory movements by any single effector (23).  #@NEW_LINE#@#  However, quasiperiodic oscillations in sign language may arise from the coordinated movements of multiple effectors.  #@NEW_LINE#@#  
Here we present the Instantaneous Visual Change (IVC) as a metric that is conceptually similar to the acoustic envelope, summarizing the amplitude of change at each time point.  #@NEW_LINE#@#  The IVC is a time series of aggregated visual changes between frames (Fig 1 and Method and Materials).  #@NEW_LINE#@#  This algorithm provides an automatic, objective alternative to human-coded methods of studying temporal structure in sign (24).  #@NEW_LINE#@#  
The amplitude of the IVC indexes the amount of visual change between two video frames.  #@NEW_LINE#@#  The largest peaks in the IVC therefore occur during large, quick movements.  #@NEW_LINE#@#  In the videos we analyzed, these changes corresponded primarily to movements of the signers hands and arms, but may also reflect movements of the face, head, and torso.  #@NEW_LINE#@#  For example, a quick arm movement results in a larger number of pixels changing in each adjacent frameand a higher peak in the IVCthan a slow arm movement.  #@NEW_LINE#@#  The IVC thus offers a heuristic index of linguistic information in the visual signal.  #@NEW_LINE#@#  An example movie illustrating the IVC is included as Movie S1.  #@NEW_LINE#@#  

Characterizing_Temporal_Structure_in_Sign_Language  #@NEW_LINE#@#  
The IVC allows us to characterize one dimension of the temporal structure in sign language and to directly compare the spectral signatures of amplitude variability across sign and oral speech.  #@NEW_LINE#@#  Visual examination of the raw IVC of sign language reveals quasiperiodic oscillations with irregularly timed peaks (Fig 2A).  #@NEW_LINE#@#  To characterize variability within and across sign languages, we computed the power spectra of the IVC from samples of four different sign languages: American Sign Language (ASL), German Sign Language (Deutsche Geb√§rdensprache; DGS), British Sign Language (BSL), and Australian Sign Language (Auslan).  #@NEW_LINE#@#  These languages developed independently of the oral languages spoken in these countries and come from three genetically unrelated language families (BSL and Auslan from the British, Australian, and New Zealand Sign Language family; ASL from the French Sign Language family; and DGS from the German Sign Language family).  #@NEW_LINE#@#  In all four languages, power in the IVC decreases roughly monotonically with increasing frequency, without any pronounced peaks in the spectrum (Fig 2C).  #@NEW_LINE#@#  We tested for rhythmic components in the IVC by comparing these spectra against the 1/f spectrum characteristic of many signals in the natural world.  #@NEW_LINE#@#  Power that is higher than the 1/f function indicates periodicity at that frequency (22).  #@NEW_LINE#@#  The IVC of sign language showed elevated power at 28 Hz (Psless_than.01).  #@NEW_LINE#@#  Individual signs in sign languages tend to occur at 22.5 Hz (25, 26), on the lower end of the rhythmic components in the IVC.  #@NEW_LINE#@#  These analyses suggest that sign language involves weak, quasiperiodic rhythms with variable frequencies in the delta and theta range.  #@NEW_LINE#@#  
To explicitly compare temporal structure between sign and speech, we contrasted the IVC of sign with the broadband envelope of speech (22).  #@NEW_LINE#@#  We computed the broadband envelope of samples from nine spoken languages representing five language families: English, French, Portuguese, Dutch, German, Hungarian, Japanese, Arabic, and Mandarin.  #@NEW_LINE#@#  After resampling the broadband envelopes and IVC signals to a common frequency (30 Hz) and standardizing the amplitude of each recording by dividing out its standard deviation, we compared the average spectra of the IVC of sign and the broadband envelope of speech (Fig 2D).  #@NEW_LINE#@#  Spoken languages showed stronger modulations than sign languages at  greater than 2 Hz.  #@NEW_LINE#@#  This increased power may reflect modulation due to syllables in speech, which occur at 210 Hz (22, 27).  #@NEW_LINE#@#  Indeed, peaks from individual syllables are visible in the broadband envelope, and these peaks occur at 4 Hz (Fig 2B).  #@NEW_LINE#@#  These results indicate that visual motion in sign language is modulated at lower frequencies than auditory volume in spoken language.  #@NEW_LINE#@#  This difference is consistent with the slower movements in the articulators for sign (the hands) than in the articulators for speech (the vocal tract) (25, 26).  #@NEW_LINE#@#  

Cortical_Coherence_to_Visual_Rhythms_in_Sign_Language  #@NEW_LINE#@#  
We used electroencephalography (EEG) to examine cortical entrainment to quasirhythmic fluctuations in visual information in sign language.  #@NEW_LINE#@#  Fluent speakers of ASL watched videos of ASL stories against a static background.  #@NEW_LINE#@#  We tested for coherence of low-frequency electrophysiological oscillations to quasiperiodic oscillations in the IVC.  #@NEW_LINE#@#  
Coherence was calculated separately at each EEG channel in partially overlapping, logarithmically spaced bins centered over 0.516 Hz (Materials and Methods).  #@NEW_LINE#@#  Because coherence provides no intrinsic measure of chance performance, we created a null distribution of coherence using a randomization procedure.  #@NEW_LINE#@#  To obtain each value in the null distribution, we time-shifted the IVC to a randomly selected starting point, moving the portion of the IVC that remained after the final time point of the EEG signal to the beginning of the recording.  #@NEW_LINE#@#  This procedure preserves the spectral and temporal characteristics of the EEG and IVC recordings, but eliminates any relationship between these signals.  #@NEW_LINE#@#  Coherence was then computed between the EEG recordings and the randomly shifted IVC.  #@NEW_LINE#@#  
A cluster-based permutation test indicated that coherence between cortical oscillations and the IVC of sign was stronger than would be expected by chance (P=.0001).  #@NEW_LINE#@#  Averaging the coherence spectrum across every EEG channel, coherence was above chance from 0.4 to 5 Hz, peaking at 1 Hz (Fig 3A).  #@NEW_LINE#@#  Coherence emerged over a similar range of frequencies when we selected only occipital channels (0.85 Hz; P=.0001), primarily reflecting entrainment in visual cortex (Fig 3B).  #@NEW_LINE#@#  In frontal channels, above-chance coherence was present from 0.4 to 1.25 Hz (P=.0001; Fig 3C), revealing top-down control from frontal cortex.  #@NEW_LINE#@#  Examining the entire scalp distribution, cortical coherence to the IVC of sign language was strongest over central and occipital channels (Fig 3D).  #@NEW_LINE#@#  
To test whether cortical entrainment depends on linguistic knowledge, we examined coherence to sign language in people who did not know any ASL.  #@NEW_LINE#@#  Like signers, nonsigners showed significant coherence to videos of ASL storytelling (Pless_than.0005), with the strongest coherence over central and occipital channels from 0.8 to 3.5 Hz (Fig S1).  #@NEW_LINE#@#  We then separately analyzed effects of linguistic knowledge on entrainment in occipital and frontal cortex.  #@NEW_LINE#@#  Although coherence at occipital channels did not significantly differ between groups (Fig 4B), coherence at frontal channels was stronger in signers than in nonsigners, indicating differences in top-down control based on familiarity with ASL (Pless_than.05; Fig 4A and Fig S1).  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
Cortical_Coherence_to_Sign_Language  #@NEW_LINE#@#  
In this study, we find that electrophysiological oscillations in human cerebral cortex become entrained to quasiperiodic fluctuations of visual movement in sign language.  #@NEW_LINE#@#  In fluent signers, cortical entrainment to sign language was found between 0.4 and 5 Hz, peaking at 1 Hz, and emerged most robustly over occipital and central EEG channels.  #@NEW_LINE#@#  These results show that the human brain entrains to low-frequency variability in language whether it is perceived with the ears or eyes.  #@NEW_LINE#@#  Visual cortex flexibly phase-locks to visible changes in sign language, analogously to the way auditory cortex phase-locks to amplitude changes in oral speech.  #@NEW_LINE#@#  Our findings argue that flexible entrainment depends on mechanisms that are not specific to any given effector or sensory modality.  #@NEW_LINE#@#  
Prior results suggest that auditory and visual perception are differentially modulated by rhythms at different frequencies (16).  #@NEW_LINE#@#  Auditory sensitivity varies as a function of the power and phase of spontaneous 2- to 6-Hz rhythms (28), and these oscillations are entrained by sounds modulated at 3 Hz (29, 30).  #@NEW_LINE#@#  Visual sensitivity, by contrast, depends on the power and phase of spontaneous alpha rhythms (3133), and electrophysiological oscillations in visual cortex are robustly entrained by periodic stimulation at 10 Hz (19, 21).  #@NEW_LINE#@#  When humans watch a light flicker at frequencies from 1 to 100 Hz, visual cortex shows the strongest entrainment at 10 Hz (18).  #@NEW_LINE#@#  
Although these rhythmic preferences are not absolute [visual cortex also shows rhythmic oscillations in the delta/theta range (11, 17, 3437)], differences between the sensory modalities are apparent when different frequency bands are directly compared.  #@NEW_LINE#@#  Auditory detection sensitivity depends on the phase of underlying deltatheta, but not alpha, oscillations (28).  #@NEW_LINE#@#  In response to aperiodic stimulation, visual cortex oscillates in the alpha band (20), whereas auditory cortex does not show consistent oscillatory activity (38).  #@NEW_LINE#@#  
If sensory-specific oscillatory preferences determine the spectrum of entrainment, then peak coherence to sign language should be observed at the frequencies preferred by visual cortex: 10 Hz.  #@NEW_LINE#@#  Contrary to this prediction, we find that cortical coherence to sign language only emerges at less_than5 Hz.  #@NEW_LINE#@#  Cerebral cortex entrains to sign language around the frequencies of words and phrases in ASL.  #@NEW_LINE#@#  

Coherence_Across_Signers_and_Nonsigners  #@NEW_LINE#@#  
We find that cerebral cortex phase-locks to visual changes in ASL both in fluent signers and in people with no knowledge of sign language.  #@NEW_LINE#@#  In principle, coherence to ASL in nonsigners could emerge for two reasons.  #@NEW_LINE#@#  Coherence could be driven either bottom-up, by sensory stimulation, or top-down, by nonlinguistic temporal predictions.  #@NEW_LINE#@#  Human bodies move in predictable ways, and nonsigners could entrain to sign language based on these regularities in human movement.  #@NEW_LINE#@#  
In frontal areas, fluent signers showed stronger coherence than nonsigners.  #@NEW_LINE#@#  This difference in frontal coherence may reflect top-down sensory predictions based on knowledge of ASL.  #@NEW_LINE#@#  Alternatively, differences in coherence could reflect differences in attention to the videos.  #@NEW_LINE#@#  Cortical entrainment to oral speech decreases when people direct attention away from the speech stimulus (13, 39).  #@NEW_LINE#@#  Reduced coherence in nonsigners, therefore, would also be predicted if nonsigners do not attend to videos of ASL as strongly as fluent signers do.  #@NEW_LINE#@#  However, our findings at occipital channels argue against this possibility.  #@NEW_LINE#@#  If differences between groups were driven by attention, then occipital coherence should be stronger in signers than in nonsigners.  #@NEW_LINE#@#  However, we find no evidence that occipital coherence depends on linguistic knowledge.  #@NEW_LINE#@#  Together, these results suggest that, although linguistic knowledge is not necessary for entrainment, signers may leverage knowledge about ASL to sharpen temporal predictions during language comprehension.  #@NEW_LINE#@#  These sharpened predictions result in stronger entrainment in the frontal regions that exert top-down control over visual perception.  #@NEW_LINE#@#  

Specialization_for_Speech?  #@NEW_LINE#@#  
Syllables in oral speech occur at frequencies that largely overlap with cortical entrainment to the volume envelope.  #@NEW_LINE#@#  This overlap could be interpreted as evidence for a specialized oscillatory mechanism for speech comprehension.  #@NEW_LINE#@#  This type of speech-specific mechanism could evolve in at least two ways.  #@NEW_LINE#@#  First, as Giraud and Poeppel state: The articulatory motor system [may have] structured its output to match those rhythms the auditory system can best apprehend (6).  #@NEW_LINE#@#  Second, auditory mechanisms may have developed to comprehend speech based on the timing of preexisting oral behaviors.  #@NEW_LINE#@#  Nonhuman primates create vocalizations and facial displays that fluctuate at frequencies similar to human speech syllables (40), and their attention is preferentially captured by faces that move at these frequencies (41); perhaps auditory processing evolved to fit the timing profile of these behaviors.  #@NEW_LINE#@#  
The data we report here, however, suggest that entrainment may not have any close evolutionary link to oral speech.  #@NEW_LINE#@#  Instead, a more general process may underlie cortical phase-locking to variability in language.  #@NEW_LINE#@#  Previous results are consistent with this interpretation as well.  #@NEW_LINE#@#  When participants watch videos of speech, entrainment emerges not only in auditory cortex, but also in visual cortex (11, 35, 36).  #@NEW_LINE#@#  Furthermore, cortical rhythms entrain to rhythms in music (42) and to other rhythmic stimuli in audition (29, 30) and vision (19, 43).  #@NEW_LINE#@#  These examples of low-frequency cortical entrainment to a broad range of stimuli across sensory modalities suggest that the cortical mechanisms supporting entrainment to the volume envelope of speech may be a specialized case of a general predictive process.  #@NEW_LINE#@#  

Neural_Mechanisms_of_Language_Comprehension_Across_Sensory_Modalities  #@NEW_LINE#@#  
Previous studies have shown that the functional neuroanatomy of speech largely overlaps with that of sign (44).  #@NEW_LINE#@#  At the coarsest level of anatomical specificity, the left hemisphere is specialized for spoken language.  #@NEW_LINE#@#  The left hemisphere is also asymmetrically active during sign language perception (45) and production, regardless of which hand people use to sign (46, 47).  #@NEW_LINE#@#  Left hemisphere damage, furthermore, results in linguistic deficits in signing patients (48).  #@NEW_LINE#@#  
Specific regions within the left hemisphere show similar involvement in processing both speech and sign.  #@NEW_LINE#@#  Across signed and spoken language, bloodflow increases to the left inferior frontal gyrus (LIFG) and left inferior parietal lobe (IPL) during phonemic discrimination (49, 50) and morphosyntactic processing (51).  #@NEW_LINE#@#  Similarly, word production in both signed and spoken languages activates LIFG, left IPL, and left temporal areas (52).  #@NEW_LINE#@#  
Differences in the cortical areas involved in sign and speech can often be attributed to differences in the form of these languages.  #@NEW_LINE#@#  For example, comprehension of sign language activates primary visual, but not primary auditory, cortex (45).  #@NEW_LINE#@#  Consistent with the fact that sign language relies on spatial contrasts, inferior and superior parietal cortex is more strongly active during signed than during spoken language production (52) and perception (49).  #@NEW_LINE#@#  
Our findings go beyond functional neuroanatomy to examine neurophysiological processes that can arise in multiple cortical areas.  #@NEW_LINE#@#  We show that oscillatory entrainment to low-frequency variability in the stimulus occurs, regardless of whether language is being processed using auditory cortex or visual cortex.  #@NEW_LINE#@#  
Our results differ from previous studies on entrainment to speech primarily in the scalp topography of coherence.  #@NEW_LINE#@#  Entrainment to auditory speech is strongest over auditory cortex (2, 8, 11, 35) and central frontal sites (14).  #@NEW_LINE#@#  By contrast, our results show that entrainment to sign language is strongest at occipital and parietal channels, consistent with greater parietal activation during sign compared with speech (53).  #@NEW_LINE#@#  This difference likely reflects increased visual and spatial demands of perceiving sign language.  #@NEW_LINE#@#  

The_IVC_Quantifies_Temporal_Structure_in_Visual_Perception  #@NEW_LINE#@#  
The IVC provides a method for examining gross temporal structure in natural visual stimuli.  #@NEW_LINE#@#  Analogously to the way the broadband envelope summarizes early stages of auditory processing, the IVC provides a first approximation of the magnitude of information available to the earliest stages of visual processing.  #@NEW_LINE#@#  At the first stage of auditory transduction, hair cells in the cochlea extract the narrowband envelope of sounds.  #@NEW_LINE#@#  Summing these narrowband envelopes together yields the overall auditory responses over time: the broadband envelope.  #@NEW_LINE#@#  In the retina, center-surround retinal ganglion cells respond to changes in the brightness of specific wavelengths of light.  #@NEW_LINE#@#  Summing the responses from these cells yields the overall visual responses over time, approximated by the IVC.  #@NEW_LINE#@#  
The IVC provides a coarse index of visual information in sign language, just as the broadband envelope provides a coarse index of information in speech.  #@NEW_LINE#@#  For example, the volume envelope does not reflect small spectral differences that are crucial for discriminating vowels.  #@NEW_LINE#@#  The IVC, analogously, does not preserve information about which effectors are moving or their trajectories.  #@NEW_LINE#@#  Nevertheless, sign language comprehenders may use the IVC of sign heuristically, as listeners use the acoustic envelope of speech, to anticipate when important information is likely to appear.  #@NEW_LINE#@#  
In the present study, we use the IVC to characterize temporal structure in sign language and to examine responses of the human brain to that temporal structure.  #@NEW_LINE#@#  The IVC could also be applied to study temporal structure in other domains, such as gesture, biological motion, and movement in natural scenes.  #@NEW_LINE#@#  

The_Functional_Role_of_Entrainment_to_Language  #@NEW_LINE#@#  
Oscillatory entrainment to language may be a specific case of a general cortical mechanism.  #@NEW_LINE#@#  In primates, spiking probability varies with the phase of low-frequency oscillations: Neurons are most likely to fire at specific points in the phase of ongoing oscillations (54, 55).  #@NEW_LINE#@#  Perhaps the cortex strategically resets the phase of ongoing neural oscillations to ensure that perceptual neurons are in an excitable state when new information is likely to appear (57, 14).  #@NEW_LINE#@#  Oscillatory entrainment may constitute a cortical strategy to boost perceptual sensitivity at informational peaks in language.  #@NEW_LINE#@#  Our findings suggest that the brain can flexibly entrain to linguistic information regardless of the modality in which language is produced or perceived.  #@NEW_LINE#@#  


Materials_and_Methods  #@NEW_LINE#@#  
Participants watched 20 min of naturalistic storytelling in ASL while EEG was recorded.  #@NEW_LINE#@#  Participants were instructed to watch the videos and remain still and relaxed.  #@NEW_LINE#@#  All procedures were approved by the Institutional Review Board of the University of Chicago.  #@NEW_LINE#@#  Detailed methods and analyses are available in SI Materials and Methods.  #@NEW_LINE#@#  
Participants  #@NEW_LINE#@#  
Participants had corrected-to-normal vision and reported no history of epilepsy, brain surgery, or traumatic brain injuries.  #@NEW_LINE#@#  Informed consent was obtained before beginning the experiment, and participants were paid $20/h for their participation.  #@NEW_LINE#@#  We recorded EEG from 16 fluent signers of ASL.  #@NEW_LINE#@#  Data from two participants were excluded before analysis due to excessive EEG artifacts, and data from one participant were lost due to experimenter error.  #@NEW_LINE#@#  All participants retained in the analyses began learning ASL by 5 y of age (N=13; 3 female, 10 male; age 2444; mean age of acquisition 1.1 y).  #@NEW_LINE#@#  Participants who used hearing aids or cochlear implants removed the devices before beginning the experiment.  #@NEW_LINE#@#  A fluent speaker of ASL (J.L.)  #@NEW_LINE#@#  answered participants questions about the study.  #@NEW_LINE#@#  We recorded EEG from an additional 16 participants who had no prior exposure to ASL.  #@NEW_LINE#@#  These participants were recruited from the University of Chicago community through online postings.  #@NEW_LINE#@#  One participant who was currently learning ASL was excluded before analyses, leaving N=15 nonsigning participants (10 female, 5 male; ages 1831).  #@NEW_LINE#@#  

IVC  #@NEW_LINE#@#  
The IVC represents a time-series of aggregated visual change between frames (Fig 1) and is computed as the sum of squared differences in each pixel across sequential frames of video:IVC(t)=i[xi(t)xi(t1)]2,where x is the grayscale value of pixel i at time t. Python code for the IVC is available at https://github.com/gbrookshire/ivc.  #@NEW_LINE#@#  

EEG_Analysis  #@NEW_LINE#@#  
See SI Materials and Methods for details about EEG acquisition and preprocessing.  #@NEW_LINE#@#  To compute coherence, IVC and EEG data were filtered into overlapping log-spaced frequency bins by using phase-preserving forward-reverse Butterworth bandpass filters.  #@NEW_LINE#@#  Bins were centered on values from 0.5 to 16 Hz and included frequencies in the range (0.8f,1.25f), where f is the center frequency f=2n for n{1,0.5,0,,4}.  #@NEW_LINE#@#  Instantaneous phase and power were determined with the Hilbert transform.  #@NEW_LINE#@#  Power was computed as the absolute value of the analytic signal, and phase as the angle of the analytic signal.  #@NEW_LINE#@#  These power and phase estimates were then used to calculate coherence:Coh=|t(eitPC,tPV,t)|t(PC,tPV,t),where t is the time point,  is the phase difference between the IVC and EEG, PV is power in the IVC, and PC is power in the EEG recording (9).  #@NEW_LINE#@#  Statistical significance of coherence was determined by a two-stage randomization procedure.  #@NEW_LINE#@#  First, the IVC was randomly shifted to obtain a null distribution of coherence between the two signals.  #@NEW_LINE#@#  Second, statistical significance was determined by using cluster-based permutation tests (56) (SI Materials and Methods).  #@NEW_LINE#@#  


SI_Materials_and_Methods  #@NEW_LINE#@#  
Spectral_Analysis_of_Speech_and_Sign  #@NEW_LINE#@#  
We analyzed temporal structure in multiple spoken and signed languages.  #@NEW_LINE#@#  These analyses were performed by using custom software written in Python.  #@NEW_LINE#@#  
Spoken-language samples comprised audio recordings of stories in nine languages from five languages families (n=12 samples, total duration 1:07:28; Table S1).  #@NEW_LINE#@#  Before computing the spectra, these recordings were trimmed to a maximum of 6 min (mean 5:37, SD 0:52), mixed down to 1 output channel, and downsampled to 22,050 Hz.  #@NEW_LINE#@#  
Sign-language samples were chosen to be as similar as possible to the spoken samples (n=14, total duration 1:10:22, mean 5:02, SD 3:19; Table S2), representing four languages from three families.  #@NEW_LINE#@#  All samples comprised forward-facing views of a single speaker.  #@NEW_LINE#@#  In 12 videos, the speaker told a story; in the remaining 2 videos, the speaker gave an informational speech.  #@NEW_LINE#@#  The IVC was calculated for each video.  #@NEW_LINE#@#  
All IVC and broadband envelope recordings were resampled to a common frequency of 30 Hz, and power was normalized by dividing out the SD of each signal.  #@NEW_LINE#@#  Power spectra were computed by using Welchs method.  #@NEW_LINE#@#  Signals were split into 2.13-s-long segments (26 samples) that overlapped by 1.07 s (25 samples).  #@NEW_LINE#@#  A Hanning window was applied to each segment, and the linear trend was removed.  #@NEW_LINE#@#  Fast Fourier transforms were then computed for each segment.  #@NEW_LINE#@#  The spectrum for each signal was obtained by averaging the spectra in all segments.  #@NEW_LINE#@#  
We fit sign language IVC spectra to a 1/f function using least-squares regression by transforming the equation Y=Af into the linear form logY=logAlogf, with power Y and frequency f (22).  #@NEW_LINE#@#  We tested for deviations from the 1/f trend using one-sample t tests.  #@NEW_LINE#@#  

Broadband_Envelope  #@NEW_LINE#@#  
The broadband envelope was computed over samples of oral speech by adapting methods from previous studies (22).  #@NEW_LINE#@#  First, the raw waveform was bandpass-filtered into 25 logarithmically spaced frequency bands from 0.1 to 10 kHz using least-squares filters with 501 taps.  #@NEW_LINE#@#  The narrowband envelope of each filtered signal was then calculated as the absolute value of the Hilbert transform.  #@NEW_LINE#@#  The narrowband envelopes were summed to obtain the broadband envelope for each recording.  #@NEW_LINE#@#  

Stimuli  #@NEW_LINE#@#  
Stimuli comprised two videos of ASL storytelling: Kondima and Little Feet from the Rosa Lee Show (57).  #@NEW_LINE#@#  Each movie was 10 min long with a 30-Hz frame rate and depicted a native speaker of ASL telling a story against a static background.  #@NEW_LINE#@#  To ensure that the timing of the movie was accurately matched with EEG recordings, a small white square flashed in the corner of the display once every 30 frames of video (out of view of the participant) and was registered by a photodiode connected to the EEG amplifier.  #@NEW_LINE#@#  In total, the session lasted 6090 min.  #@NEW_LINE#@#  

EEG_Acquisition_and_Preprocessing  #@NEW_LINE#@#  
EEG was recorded at 250 Hz by using a 128-channel net (Electrical Geodesics).  #@NEW_LINE#@#  Impedances were reduced to less_than50 k before participants watched each story.  #@NEW_LINE#@#  EEG analyses were performed in Matlab by using custom software and the FieldTrip package (58).  #@NEW_LINE#@#  Recordings from electrodes along the face, beneath the ears, and at the base of the neck were excluded before any analysis, leaving 103 channels in all analyses.  #@NEW_LINE#@#  Electrode movement artifacts were manually identified and rejected by replacing the tagged region with zeros and applying a 4,000-ms half-Hann taper to each side of the artifact.  #@NEW_LINE#@#  Artifacts from blinks and eye-movements were identified and removed by using independent component analysis.  #@NEW_LINE#@#  To ensure that the IVC of each story was accurately matched to the EEG recordings, we used cubic spline interpolation to warp the IVC to the time stamps registered by the photodiode.  #@NEW_LINE#@#  This process simultaneously resampled the IVC from 30 to 250 Hz.  #@NEW_LINE#@#  Before computing coherence, EEG signals were rereferenced to the average mastoids.  #@NEW_LINE#@#  

EEG_Statistical_Testing  #@NEW_LINE#@#  
Statistical significance of EEG coherence to the IVC was determined by a two-stage randomization procedure.  #@NEW_LINE#@#  To obtain a null distribution of coherence, the onset of the IVC was circularly shifted to a randomly selected starting point, and coherence was computed between EEG signals and the shifted IVC.  #@NEW_LINE#@#  This procedure preserves the spectrotemporal characteristics of both signals, but eliminates any relationship between them.  #@NEW_LINE#@#  For each subject, we computed 100 randomly shifted baselines.  #@NEW_LINE#@#  Next, we tested for significant differences between the empirical and randomly shifted coherence using a cluster-based nonparametric permutation test (56).  #@NEW_LINE#@#  This test looked for a difference in coherence between the empirical and randomly shifted data, contrasted with n=10,000 permutations in which the empirical data were randomly selected from the group of empirical and randomly shifted traces.  #@NEW_LINE#@#  For each frequency and each channel, a t statistic was computed on the difference between empirical and randomly shifted data using a dependent-samples regression.  #@NEW_LINE#@#  The test statistic was computed as the maximum cluster size in each permutation (cluster threshold: =.01, two-tailed).  #@NEW_LINE#@#  The P value was calculated as the proportion of permuted cluster statistics that were more extreme than the empirical value.  #@NEW_LINE#@#  
In previous studies, cortical entrainment to speech is often strongest over auditory cortex (2, 8, 11, 35) and frontal cortex (14).  #@NEW_LINE#@#  We tested coherence to sign language in two analogous regions of interest (ROIs): at occipital channels and at frontal channels.  #@NEW_LINE#@#  
To test whether knowledge of ASL influenced the strength of entrainment, we performed cluster-permutation analyses on the difference in coherence between signers and nonsigners at the occipital and frontal ROIs.  #@NEW_LINE#@#  For each participant, we normalized the data by computing the Z score of their empirical coherence against the randomly shifted baseline coherence values.  #@NEW_LINE#@#  Z-scored empirical coherence was then compared between groups using cluster permutation tests.  #@NEW_LINE#@#  

Data_Deposition  #@NEW_LINE#@#  
All EEG data and samples of speech and sign language are available on an Open Science Framework repository (https://osf.io/5pktz/?view_only=d4a9eb7180824a5d824a83b9b7f9ef47).  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
This work was supported by a William Orr Dingwall Neurolinguistics Fellowship (to G.B.  #@NEW_LINE#@#  ); a research grant from the University of Chicago Center for Gesture, Sign, and Language (to G.B.  #@NEW_LINE#@#  and J.L.  #@NEW_LINE#@#  ); National Science Foundation Graduate Research Fellowship Program Grant DGE-1144082 (to J.L.  #@NEW_LINE#@#  ); the Institute of Education Sciences; U.S. Department of Education Grant R305B140048 (to J.L.  #@NEW_LINE#@#  ); McDonnell Scholar Award 220020236 (to D.C.); National Science Foundation Grant BCS-1257101 (to D.C.); and National Science Foundation Grant BCS-0116293 (to H.C.N.).  #@NEW_LINE#@#  Images from The Rosa Lee Show are used courtesy of Rosa Lee Timm.  #@NEW_LINE#@#  

Footnotes  #@NEW_LINE#@#  




