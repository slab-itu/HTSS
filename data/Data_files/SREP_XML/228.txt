article id="http://dx.doi.org/10.1038/srep25890"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Fast machine-learning online optimization of ultra-cold-atom experiments  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
We apply an online optimization process based on machine learning to the production of Bose-Einstein condensates (BEC).  #@NEW_LINE#@#  BEC is typically created with an exponential evaporation ramp that is optimal for ergodic dynamics with two-body s-wave interactions and no other loss rates, but likely sub-optimal for real experiments.  #@NEW_LINE#@#  Through repeated machine-controlled scientific experimentation and observations our learner discovers an optimal evaporation ramp for BEC production.  #@NEW_LINE#@#  In contrast to previous work, our learner uses a Gaussian process to develop a statistical model of the relationship between the parameters it controls and the quality of the BEC produced.  #@NEW_LINE#@#  We demonstrate that the Gaussian process machine learner is able to discover a ramp that produces high quality BECs in 10 times fewer iterations than a previously used online optimization technique.  #@NEW_LINE#@#  Furthermore, we show the internal model developed can be used to determine which parameters are essential in BEC creation and which are unimportant, providing insight into the optimization process of the system.  #@NEW_LINE#@#  

Introduction  #@NEW_LINE#@#  
Experimental research into quantum phenomena often requires the optimization of resources or processes in the face of complex underlying dynamics and shifting environments.  #@NEW_LINE#@#  For example, creating large Bose-Einstein condensates (BECs) with short duty cycles is one of the keys to improving the sensitivity of cold-atom based sensors1 or for performing scientific investigation into condensed matter phases2, many-body physics3 and non-equilibrium dynamics4.  #@NEW_LINE#@#  The standard process of BEC production is evaporative cooling5.  #@NEW_LINE#@#  Microscopic semi-classical theory exists to describe this process6, but it can oversimplify the dynamics and miss more complex and effective methods of performing evaporation.  #@NEW_LINE#@#  For example, Shobu et al.7 found circumventing higher order inelastic collisions can produce large condensates.  #@NEW_LINE#@#  Tricks like this are likely to exist for other species with complicated scattering processes8, but discovery is only possible by experimentation.  #@NEW_LINE#@#  We automate this process of discovery with machine-learning online optimization (MLOO).  #@NEW_LINE#@#  What distinguishes our approach from previous methods for optimization is that we seek to develop a statistical model of the relationship between parameters and the outcome of the experiment.  #@NEW_LINE#@#  We demonstrate that MLOO can discover condensation with less experiments than a competing optimization method and provide insight into which parameters are important in achieving condensation.  #@NEW_LINE#@#  
Online optimization (OO), with mostly genetic9,10,11,12,13,14,15,16,17,18,19,20,21,22 but also gradient23 and hybrid solvers24,25, has been used to enhance a variety of quantum experiments.  #@NEW_LINE#@#  Here we use online to mean optimization that is performed in real time with the experiment.  #@NEW_LINE#@#  What distinguishes our approach is that it does not only seek to optimize the experiment, but also creates an internal model that is able to predict the performance of future experiments given any set of parameters.  #@NEW_LINE#@#  This is achieved by modeling the experiment using a Gaussian process (GP)26.  #@NEW_LINE#@#  Our algorithm both fits the model to previous observations, and chooses to do future experiments that will best refine its model, making it an automation of scientific method.  #@NEW_LINE#@#  Online machine learning (OML) with GPs26,27,28,29,30 has been applied in a variety of areas including robotics31,32, vision33, industrial chemistry34,35 and biochemistry36.  #@NEW_LINE#@#  However, in all of these cases, the focus was not on optimization.  #@NEW_LINE#@#  Rather, the goal was the development of an accurate model.  #@NEW_LINE#@#  We combine the advantages of OML with the motivation of OO.  #@NEW_LINE#@#  The resultant MLOO algorithm has the following advantages: every experimental observation is used to improve the GP model, and uncertainties in the measurements are correctly accounted for; our algorithm will find global minima, but exploration is not random, new parameters are picked with knowledge of where the learner is most uncertain; the learner provides a visualization of the resources quality as function of the parameters that can inform experimentalists on how to best develop future optimization experiments.  #@NEW_LINE#@#  

Methods  #@NEW_LINE#@#  
Experiment  #@NEW_LINE#@#  
The experimental apparatus is described in detail in37.  #@NEW_LINE#@#  Initially 87Rb atoms are cooled in a combined 2D and 3D MOT system and subsequently cooled further by RF (radio frequency) evaporation.  #@NEW_LINE#@#  The cloud is then loaded into a cross beam optical dipole trap for the final evaporation stage.  #@NEW_LINE#@#  It is this stage that is the subject of the optimization process.  #@NEW_LINE#@#  At this point, the sample contains 4×107 atoms at a temperature of ~5K with a phase space density of ~0.05.  #@NEW_LINE#@#  The cross dipole trap is formed from two intersecting 1090nm and 1064nm lasers with approximate waists of 350m and 300m respectively producing a trap with frequencies 185×185×40Hz.  #@NEW_LINE#@#  The depth of the cross trap is determined by the intensity of the two beams and is found to be approximately 70K.  #@NEW_LINE#@#  The 1064nm beam is controlled by varying the current to the laser, while the 1090nm beam is controlled using the current and a waveplate rotation stage combined with a polarizing beamsplitter to provide additional power attenuation while maintaining mode stability.  #@NEW_LINE#@#  A diagram of the experimental set up is shown in Fig 1.  #@NEW_LINE#@#  Normally the power to these beams is ramped down over time, thereby lowering the walls of the trap and allowing the higher energy atoms to leak out.  #@NEW_LINE#@#  The remaining atoms rethermalize to a lower temperature, enabling cooling.  #@NEW_LINE#@#  Once the gas has been cooled to temperatures on the order of nK, a phase transition occurs, and a macroscopic number of atoms start to occupy the same quantum state.  #@NEW_LINE#@#  This transition is called Bose-Einstein condensation38.  #@NEW_LINE#@#  We hand over control of these ramps to the MLOO.  #@NEW_LINE#@#  We consider two parameterizations: one simple, where we only control the start and end points of a linear interpolation; and one complex, where we add variable quadratic, cubic and quartic corrections to the simple case (see Supplemental Equations).  #@NEW_LINE#@#  

Performance_Measure  #@NEW_LINE#@#  
The approach we propose is a form of supervised learning, meaning that we provide the learner with a number that quantifies the quality of the resource produced or in optimization terminology a cost that must be minimized.  #@NEW_LINE#@#  Naïvely one might try to use a measure based on temperature and particle number.  #@NEW_LINE#@#  However determining these quantities accurately near condensation is difficult when constrained to very few runs per parameter set.  #@NEW_LINE#@#  Instead, a technique was created to measure the width of the edges of the cloud.  #@NEW_LINE#@#  For thermal clouds this edge is broad, but as the sample cools and condenses these edges become sharper.  #@NEW_LINE#@#  To quantify this, an absorption image of the final state of the quantum gas is taken after a 30ms expansion of the cloud, with the image providing the optical depth as a function of space.  #@NEW_LINE#@#  This absorption image is taken at resonance, resulting in saturation of the image (see Fig 2).  #@NEW_LINE#@#  Whilst this makes determining peak density difficult, it ensures that the edges of the cloud are accurately determined.  #@NEW_LINE#@#  The cost is then calculated from all data between a lower and upper threshold optical depth.  #@NEW_LINE#@#  The lower threshold is determined by the noise in the system.  #@NEW_LINE#@#  The upper threshold is set slightly lower than the saturation level of the image.  #@NEW_LINE#@#  Only data from between the bounds is used and the cost is simply the average of these values.  #@NEW_LINE#@#  In practice this means the sharper the edges of the cloud, the lower the cost.  #@NEW_LINE#@#  Indeed, low quality thermal clouds have broad edges, whereas the ideal BEC has much sharper edges.  #@NEW_LINE#@#  Each parameter set is tested twice with the average of the two runs used for the cost.  #@NEW_LINE#@#  Tests of the variation in cost for a set of parameters run-to-run indicate they obey a Gaussian distribution.  #@NEW_LINE#@#  As such we are able to estimate the uncertainty from two runs as twice the range.  #@NEW_LINE#@#  In doing so, the chance we have underestimated the uncertainty will be 27%.  #@NEW_LINE#@#  We therefore also apply bounds to the uncertainty to eliminate outliers overly affecting the modeling process.  #@NEW_LINE#@#  The cost function can be evaluated as long as some atoms are present at the end of the evaporation run.  #@NEW_LINE#@#  In cases where the evaporation parameters produced no cloud twice for a set of parameters, we set the cost to a default high value.  #@NEW_LINE#@#  

Algorithm  #@NEW_LINE#@#  
We treat the experiment as a stochastic process  which is dependent on the parameters X=(x1,, xM).  #@NEW_LINE#@#  When we make a measurement and determine a cost, we interpret this as a sample of this process C(X) with some associated uncertainty U(X).  #@NEW_LINE#@#  We define the set of all parameters, costs and uncertainty previously measured as ,  and  respectively and collectively refer to these sets as our observations .  #@NEW_LINE#@#  The aim of OO is to use previous observations  to plan future experiments in order to find a set of parameters that minimize the mean cost of the stochastic process .  #@NEW_LINE#@#  Unique to the MLOO approach, we first make an estimate of the stochastic process given our observations , which is then used to determine what parameters to try next.  #@NEW_LINE#@#  
We model  as a GPa distribution over functionswith constant mean function and covariance defined by a squared exponential correlation function  where H=(h1, , hM) is a set of correlation lengths for each of the parameters.  #@NEW_LINE#@#  The mean function conditional on the observations  and correlation lengths H of our GP is: , which is evaluated through a set of matrix operations26 (see Supplemental Equations).  #@NEW_LINE#@#  As we are using a GP, we can also get the variance of the functions conditioned on  and H: 26.  #@NEW_LINE#@#  Both of these estimates depend on the correlation lengths H, normally referred to as the hyperparameters of our estimate.  #@NEW_LINE#@#  We assume that H is not known a priori and needs to be fitted online.  #@NEW_LINE#@#  
The correlation lengths H control the sensitivity of the model to each of the parameters, and relates to how much a parameter needs to be changed before it has a significant effect on the cost (see Fig 1).  #@NEW_LINE#@#  A standard approach to fit H is maximum likelihood estimation26.  #@NEW_LINE#@#  Here, the hyperparameters are globally optimized over the likelihood of the parameters H given our observations , or 26 (see Supplemental Equations).  #@NEW_LINE#@#  However, when the data set is small there will often be multiple local optima for the hyperparameters whose likelihoods are comparable to the maximum.  #@NEW_LINE#@#  We term these hyperparameters the hypothesis set  with corresponding likelihood set .  #@NEW_LINE#@#  
To produce our final estimates for the mean function and variance we treat each hypothesis as a particle30, and perform a weighted average over .  #@NEW_LINE#@#  The weighted mean function is now defined as  and weighted variance of the functions is  , where  are the relative weights for the hyperparameters.  #@NEW_LINE#@#  Now that we have our final estimate for , we need to determine an optimization strategy for picking the next set of parameters to test.  #@NEW_LINE#@#  
Consider the following two strategies: We could always test parameters that are predicted to minimize , making our learner act as an optimizer.  #@NEW_LINE#@#  But this learner could get trapped in local minima and re-enforce its ignorance; Or we could test parameters that maximize  (i.e.  #@NEW_LINE#@#  where we are most uncertain), this would provide us with experimental data that helps us best refine our model and discriminate between the hypotheses, making our learner act like a scientist.  #@NEW_LINE#@#  But this learner may require a large number of trials to map the space and would not prioritize refinement of the global minima.  #@NEW_LINE#@#  We chose to implement a balanced strategy that repeatably sweeps between these two extremes by minimizing a biased cost function: , where the value for b is linearly increased from 0 to 1 in a cycle of length Q.  #@NEW_LINE#@#  During testing with synthetic data, we found sweeping the learner between acting like a scientist (b=0) and an optimizer (b=1) was more robust and efficient than fixing the learner to one strategy.  #@NEW_LINE#@#  When we minimized  we also put bounds, set to 20% of the parameters maximum-minimum values, on the search relative to the last best measured X.  #@NEW_LINE#@#  We call these bounds a leash, as it restricts how fast the learner could change the parameters but did not stop it from exploring the full space (similar to trust-regions39,40).  #@NEW_LINE#@#  This was a technical requirement for our experiment: when a set of parameters was tested that was very different from the last set, the experiment almost always produced no atoms, meaning we had to assign a default cost that did not provide meaningful gradient information to the learner.  #@NEW_LINE#@#  Once the next set of parameters is determined they are sent to the experiment to be tested.  #@NEW_LINE#@#  After the resultant cost is measured this is then added to the observation set  with NN+1 and the entire process is repeated.  #@NEW_LINE#@#  
As a benchmark for comparison, we also performed OO using a Nelder-Mead solver41, which has previously been used to optimize quantum gates25.  #@NEW_LINE#@#  


Results  #@NEW_LINE#@#  
We demonstrate the performance of machine learning online optimization in comparison to the Nelder-Mead optimizer in Fig 2.  #@NEW_LINE#@#  Here we used the complex parameterization for all 3 ramps, and added an extra parameter that controlled the total time of the ramps, resulting in 16 parameters.  #@NEW_LINE#@#  If we were to perform a brute force search and optimize the parameters to within a 10% accuracy of the parameters maximum-minimum bounds, the number of runs required would be 1016.  #@NEW_LINE#@#  The Nelder-Mead algorithm is able to find BEC much faster than this, in only 145 runs.  #@NEW_LINE#@#  The machine learning algorithm, on the other hand, is much faster.  #@NEW_LINE#@#  After the first 20 training runs, where the machine learning and Nelder-Mead algorithm use a common set of parameters, the machine learning algorithm converges in only 10 experiments.  #@NEW_LINE#@#  
The learner used in Fig 2 only used the best hypothesis set when picking the next parameters, in other words we set P=1.  #@NEW_LINE#@#  Evaluating multiple GPs is computationally expensive with so many parameters, so to save time we made this restriction.  #@NEW_LINE#@#  In spite of this, the learner discovered ramps that produced BEC in very few iterations.  #@NEW_LINE#@#  This is because the learner consistently fitted the correlation lengths of the 3 most important parametersthe end points of the rampsvery quickly.  #@NEW_LINE#@#  However, we found the other correlation lengths were not estimated well and would not converge, even after a BEC was found.  #@NEW_LINE#@#  This meant that we were unable to make useful predictions about the cost landscape and we could not reliably determine what parameters were least important.  #@NEW_LINE#@#  The final optimized parameters produced a condensate with 5×105 atoms.  #@NEW_LINE#@#  
Gramacy et al.30 have suggested that making good online estimation of the GP correlation lengths requires multiple particles.  #@NEW_LINE#@#  We considered achieving this goal in a different experiment as shown in Fig 3.  #@NEW_LINE#@#  Here we used a learner with many particles P=16, but had to use the simple parameterization for the ramps to save computational time.  #@NEW_LINE#@#  This resulted in a total of 7 parameters.  #@NEW_LINE#@#  We can see again the overall trend for the machine learner is still faster than Nelder-Mead, but less pronounced.  #@NEW_LINE#@#  More carefully estimating the correlation lengths has hindered the convergence rate compared to the 16 parameter case.  #@NEW_LINE#@#  Nevertheless, as we now have a more reliable estimate of the correlation lengths we can take advantage of a different feature of the learner.  #@NEW_LINE#@#  
In Fig 4(a) we show estimates of the cost landscape as 1D cross sections about the best measured point.  #@NEW_LINE#@#  We plot the two most sensitive parameters and the least.  #@NEW_LINE#@#  We can see the least sensitive parameter appears to have no effect on the production of a BEC.  #@NEW_LINE#@#  This parameter corresponds to an intentionally added 7th parameter of the system that controls nothing in the experiment.  #@NEW_LINE#@#  Figure 4(a) shows the learner successfully identified this, even with such a small data set.  #@NEW_LINE#@#  After making this observation we can then reconsider the design of the optimization process and eliminate this parameter from the experiment.  #@NEW_LINE#@#  
In Fig 3 we plot the machine learner optimization run with P=16 but now with only 6 parameters.  #@NEW_LINE#@#  We can see the learner converges more rapidly than the 7 parameter case, and even produces a higher quality BEC.  #@NEW_LINE#@#  As the learner no longer takes extra runs to determine the importance of the useless 7th parameter, it achieves BEC rapidly.  #@NEW_LINE#@#  We plot a 2D cross section of the landscape against the two most sensitive parameters in Fig 4(b) generated from the 6 parameter machine learning run.  #@NEW_LINE#@#  We can see there is a very sharp transition to BEC, as it exists in a very deep valley of the landscape.  #@NEW_LINE#@#  
The optimum values for each parameter of the 16 parameter MLOO run are shown in a table in the supplementary material.  #@NEW_LINE#@#  Plots of the optimal ramps for each of the five optimization runs discussed are also shown and display cases where the optimum ramp is non-monotonically decreasing.  #@NEW_LINE#@#  The experimental controls adjust the shape of the trapping beams in a non-trivial manner, so this does not ensure that the trap depth experienced by the BEC was also non-monotonically decreasing.  #@NEW_LINE#@#  A key strength of the MLOO process is that we were able to find the ramps for the control that maximized the BEC without performing a lengthy characterization of relationship between the controls and the potential.  #@NEW_LINE#@#  Instead, the MLOO directly characterizes the relationship between the controls and the quality of the outcome.  #@NEW_LINE#@#  This significantly decreases the system identification and analysis overhead when optimizing an experiment.  #@NEW_LINE#@#  
The topology of optimization landscapes has been suggested as an explanation for the profound discrepancy between the number of experiments required to do a brute force search, and the number of experiments required in practice when using OO.  #@NEW_LINE#@#  Specifically, in laser-aided quantum chemistry, under the assumption of controllability it has been proven that landscapes are trap-free42 (there has been further refinement43,44,45,46,47 and debate48,49,50 on the generality of the result).  #@NEW_LINE#@#  In our experiment we observed Nelder-Mead always found condensation, albeit slower than MLOO, even though it is a local solver susceptible to being trapped.  #@NEW_LINE#@#  This suggests our landscape is also trap-free, and perhaps there is a universal principle for all quantum OO systems.  #@NEW_LINE#@#  
The MLOO algorithm we developed is available online51 (it uses52 to evaluate the GPs); it can be immediately applied to experiments that have previously used OO: quantum chemistry9, femtosecond physics13, and quantum computing25.  #@NEW_LINE#@#  Indeed, any automated experiment with a resource of measurable quality can be enhanced using MLOO.  #@NEW_LINE#@#  



