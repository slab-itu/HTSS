article id="http://dx.doi.org/10.1038/srep43293"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Measuring speakerlistener neural coupling with functional near infrared spectroscopy  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
The present study investigates brain-to-brain coupling, defined as inter-subject correlations in the hemodynamic response, during natural verbal communication.  #@NEW_LINE#@#  We used functional near-infrared spectroscopy (fNIRS) to record brain activity of 3 speakers telling stories and 15 listeners comprehending audio recordings of these stories.  #@NEW_LINE#@#  Listeners brain activity was significantly correlated with speakers with a delay.  #@NEW_LINE#@#  This between-brain correlation disappeared when verbal communication failed.  #@NEW_LINE#@#  We further compared the fNIRS and functional Magnetic Resonance Imaging (fMRI) recordings of listeners comprehending the same story and found a significant relationship between the fNIRS oxygenated-hemoglobin concentration changes and the fMRI BOLD in brain areas associated with speech comprehension.  #@NEW_LINE#@#  This correlation between fNIRS and fMRI was only present when data from the same story were compared between the two modalities and vanished when data from different stories were compared; this cross-modality consistency further highlights the reliability of the spatiotemporal brain activation pattern as a measure of story comprehension.  #@NEW_LINE#@#  Our findings suggest that fNIRS can be used for investigating brain-to-brain coupling during verbal communication in natural settings.  #@NEW_LINE#@#  

Introduction  #@NEW_LINE#@#  
Verbal communication involves the relaying of information between individuals through the use of sound patterns within a structure of language.  #@NEW_LINE#@#  For decades, neuroimaging technologies have been applied to study the neural mechanisms underlying the production and comprehension of language.  #@NEW_LINE#@#  Multiple brain areas have been identified to be involved with verbal communication using Positron Emission Tomography (PET) and functional Magnetic Resonance Imaging (fMRI)1, whereas the timing of auditory processing was studied with the aid of Electroencephalogram (EEG) and Magnetoencephalogram (MEG)2,3,4,5,6,7.  #@NEW_LINE#@#  Although important findings have been discovered using these technologies, there are two limitations in traditional neurolinguistic studies.  #@NEW_LINE#@#  First, these studies are mostly concerned with the cognitive process of either speech production or speech comprehension and confine the analysis to be within individual brains.  #@NEW_LINE#@#  Verbal communication, however, is an interactive process between speaker and listener.  #@NEW_LINE#@#  As pointed out by Hasson and others8, a complete understanding of the cognitive processes involved cannot be achieved without examining and understanding the interaction of neural activity among individuals.  #@NEW_LINE#@#  Second, cognitive functions are traditionally studied in a controlled laboratory environment.  #@NEW_LINE#@#  While this practice helps to isolate various factors (e.g.  #@NEW_LINE#@#  syntactical transformations or the representation of isolated lexical items), the ecological validity of the findings is not clear until tested in a real-life context.  #@NEW_LINE#@#  In addition, many studies confined the auditory stimuli to short lengths, often using isolated words or sentences for experimental control1,7,9.  #@NEW_LINE#@#  As a result, questions regarding the brains ability to accumulate information over longer time scales cannot be effectively investigated8,10,11.  #@NEW_LINE#@#  
With the recent advances in neuroimaging systems and methodology, researchers can now address which brain processes are involved in social interaction.  #@NEW_LINE#@#  Stephens et al.  #@NEW_LINE#@#  investigated the alignment (correlation) of neural activity between speaker and listener during natural verbal communication using fMRI12.  #@NEW_LINE#@#  In the study, brain activity was recorded when a speaker was telling a real-life story and later when listeners were listening to the audio recording of the story.  #@NEW_LINE#@#  Listeners brain activity was found to be coupled with the speakers brain activity with a delay, although for certain brain areas, listeners were ahead of the speaker in time, possibly due to a predictive anticipatory effect.  #@NEW_LINE#@#  Remarkably, higher coupling was found to be associated with better understanding of the story.  #@NEW_LINE#@#  This neural coupling between speaker and listener was further supported by a recent EEG study in which the coordination between the brain activity of speakers and listeners was investigated with canonical correlation analysis13.  #@NEW_LINE#@#  Additional findings in the same EEG study suggest that this speaker-listener neural coupling might not be restricted to homologous brain areas.  #@NEW_LINE#@#  In another study using fMRI, Lerner et al.  #@NEW_LINE#@#  recorded the Blood Oxygenation Level Dependent (BOLD) response of participants listening to a real-life story scrambled at the time scales of words, sentences and paragraphs.  #@NEW_LINE#@#  Inter-subject correlation analyses were employed to estimate the reliability of neural responses across subjects, and striking topography differences in brain activation were found at the different time scales14.  #@NEW_LINE#@#  
Although novel findings have been discovered using fMRI and EEG to address the aforementioned challenges, certain limitations of the two neuroimaging technologies have hindered the investigation of neural coupling during natural verbal communication.  #@NEW_LINE#@#  fMRI, for example, requires subjects to lie down motionlessly in a noisy scanning environment.  #@NEW_LINE#@#  Simultaneous scanning of multiple individuals engaged in a face-to-face communication is impractical for fMRI based setups.  #@NEW_LINE#@#  EEG, on the other hand, is able to provide a more naturalistic environment.  #@NEW_LINE#@#  However, EEG is susceptible to muscle induced artifacts during vocalization, and is therefore less suitable for studying speaker-listener interactions15.  #@NEW_LINE#@#  Furthermore, the localization of sources from the EEG signal requires higher-density recordings and additional computation to solve the inverse problem16,17,18.  #@NEW_LINE#@#  
In this study, we propose using functional near-infrared spectroscopy (fNIRS) to investigate speaker-listener coupling as an effective complement to the existing studies.  #@NEW_LINE#@#  fNIRS is an optical brain imaging technology for monitoring the concentration changes of oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR) in the cortex.  #@NEW_LINE#@#  By utilizing portable and wearable sensors, fNIRS provides an imaging solution with high ecological validity for studying cortical hemodynamic changes in real-life contexts19.  #@NEW_LINE#@#  Furthermore, fNIRS has been used in natural environments, including on mobile individuals outdoors20, consistent with the neuroergonomic and mobile brain/body imaging approaches21,22.  #@NEW_LINE#@#  fNIRS has been adopted to study brain-to-brain coupling during a cooperation-competition game23 and a finger-tapping imitation task24.  #@NEW_LINE#@#  fNIRS has demonstrated usefulness for studying single brain activations during social interactions in a natural setting with traditional block design25.  #@NEW_LINE#@#  At present, studying brain-to-brain coupling during natural verbal communication using fNIRS has not been demonstrated.  #@NEW_LINE#@#  
The main objective of this study is to evaluate speaker-listener brain-to-brain coupling in natural everyday settings.  #@NEW_LINE#@#  To achieve this objective, we designed an fNIRS experiment to replicate the speaker-listener neural coupling results from a previous fMRI study12.  #@NEW_LINE#@#  A native English speaker and two native Turkish speakers told an unrehearsed real-life story in their native language.  #@NEW_LINE#@#  An additional real-life English story E2 (Pie-man, recorded at The Moth, a live storytelling event in New York City) used in recent fMRI studies of natural verbal communication14,26,27, was also used here.  #@NEW_LINE#@#  The resulting two English stories (E1 and E2) and two Turkish stories (T1 and T2, the control conditions) were played to listeners who only understand English.  #@NEW_LINE#@#  We hypothesized that: (1) the listeners brain activity will demonstrate inter-subject correlation only when listening to a story they understand; (2) the English speakers brain activity during production of E1 will be coupled with the activity of the listeners during comprehension.  #@NEW_LINE#@#  We have targeted both prefrontal and parietal cortices, as these include cognitive and higher order extralinguistic areas that are known to be involved in social information processing crucial for successful communication including, among others, the capacity to discern the beliefs, desires, and goals of others12,28,29.  #@NEW_LINE#@#  
A second objective of this study is to compare the fNIRS recorded in this study with the fMRI BOLD response recorded in the previous fMRI study14.  #@NEW_LINE#@#  It has been known that fNIRS and fMRI BOLD signals, both of which are based on the neurovascular coupling phenomenon, are correlated in various cognitive tasks30,31,32.  #@NEW_LINE#@#  However, previous studies primarily adopted block designs with simple stimulation to investigate the mean activity induced by trigger-averaging a condition over time.  #@NEW_LINE#@#  The relationship between fNIRS and BOLD during natural verbal communication is yet to be shown.  #@NEW_LINE#@#  To this end, we hypothesized that fNIRS biomarkers (HbO and HbR) of our listeners are correlated with the fMRI BOLD responses of the previously tested listeners14 during the comprehension of only the same English story E2 (which all of these listeners heard) and not for the other English story E1 and Turkish stories T1 and T2 (which the previously tested fMRI participants did not hear).  #@NEW_LINE#@#  The convergence of fNIRS and fMRI for comprehension of the same story serves as a further validation that fNIRS can be used to investigate brain-to-brain coupling during natural verbal communication.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Natural communication unfolds through collective participation of speaker and listener: speakers construct grammatical sentences based on thoughts, convert these to motor plans, and execute the plans to produce vocal sounds; listeners analyze the sounds, build phonemes into words and sentences, and ultimately decode sound patterns into meaning.  #@NEW_LINE#@#  In our study, we observed significant speakerlistener temporal coupling only during successful verbal communication.  #@NEW_LINE#@#  When communication was blocked (i.e., during listening to a foreign, incomprehensible language), the synchronization was lost.  #@NEW_LINE#@#  As expected, this synchronization was present in a temporally shifted time course of the speakers brain activity relative to the moment of vocalization.  #@NEW_LINE#@#  Hence, listeners brain activity mirrors the speakers brain activity with a delay.  #@NEW_LINE#@#  These lagged responses suggest that on average, the speakers production-based processes precede and likely induce the mirrored activity observed in the listeners brains during comprehension.  #@NEW_LINE#@#  Furthermore, synchronization was also present between listeners with zero shift, representing temporally aligned listener-to-listener inter-subject coupling.  #@NEW_LINE#@#  Again, this coupling was only present during successful communication and disappeared when communication was blocked.  #@NEW_LINE#@#  Finally, we found strong consistencies between the evidence from fNIRS and fMRI for the presence of coupling during successful communication.  #@NEW_LINE#@#  
Listener-listener_fNIRS_inter-subject_correlation  #@NEW_LINE#@#  
For each story, significantly coupled optodes were identified using multilevel general linear model (GLM) and the results are shown in Fig 1.  #@NEW_LINE#@#  As expected, significant results were found only for the English conditions E1 and E2 (false discovery rate33 [FDR] qless_than0.01), indicating that neural coupling only emerges during successful communication (i.e.  #@NEW_LINE#@#  when subjects understand the story content).  #@NEW_LINE#@#  HbO shows a much stronger coupling effect than HbR.  #@NEW_LINE#@#  A contrast for listening comprehension using successful communication stories (E1 and E2) minus control conditions (T1 and T2) was further included in Fig 1 as E1+E2T1T2.  #@NEW_LINE#@#  Consistent with previous fMRI studies, superior frontal gyrus, inferior parietal and angular gyrus were robustly correlated across listeners during speech comprehension34.  #@NEW_LINE#@#  Although the function of these regions is far from clear, they have been associated with various linguistic functionsin particular, semantic processing and comprehension35,36.  #@NEW_LINE#@#  

Speaker-listener_fNIRS_coupling  #@NEW_LINE#@#  
The neural coupling between speaker and listener may not be restricted to homologous brain areas13.  #@NEW_LINE#@#  Previous studies also showed that the neural responses of listeners can lag behind12,13 or precede12 those of the speaker, facilitating comprehension and anticipation, respectively.  #@NEW_LINE#@#  To investigate these effects, multilevel GLM has been adopted to evaluate the coupling between all permutations of (speaker optode, listener optode) pairs with the speakers time course shifted with respect to those of the listeners from 20s to 20s in 0.5s increments, where a positive shift represents the speaker preceding (listener lagging), and the results are shown in Fig 2.  #@NEW_LINE#@#  For the English story E1, the listeners fNIRS signals were found to be significantly coupled with the speakers signal with a 57s time delay, and the number of significantly coupled optodes peaked at 5s (Fig 2a).  #@NEW_LINE#@#  As expected, no temporal asymmetry has been found for the listener-listener case and alignment is coupled to the incoming auditory input (i.e.  #@NEW_LINE#@#  lag 0, moment of vocalization) (Fig 2b).  #@NEW_LINE#@#  The speaker-listener lagged correlation replicated the speaker-listener lagged correlation observed with fMRI12.  #@NEW_LINE#@#  Significant couplings can mainly be found between prefrontal of speaker and parietal of listeners in the medial prefrontal and left parietal areas for HbO and no results was significant at FDR qless_than0.01 level for HbR (Fig 2c).  #@NEW_LINE#@#  The relationships between significantly coupled anatomical locations in speaker and listener brains are further listed in Figure S1.  #@NEW_LINE#@#  These areas include many of the sensory, classic linguistic-related, and extralinguistic-related brain areas, demonstrating that areas involved in speech comprehension (listenerlistener coupling) are also aligned during communication (speakerlistener coupling).  #@NEW_LINE#@#  No significant speaker-listener coupling was found for either of the Turkish stories (stories T1 and T2).  #@NEW_LINE#@#  Using identical analysis methods and statistical thresholds, we found no significant coupling between the speaker and the listeners or among the listeners during these control conditions (see Fig 1, T1 and T2).  #@NEW_LINE#@#  
To further investigate the temporal asymmetry of coupling, the average t-statistics for all significantly coupled speaker-listener optode-pairs were assessed across time shifts between the speaker and listener time series, as shown in Fig 3 (red curve).  #@NEW_LINE#@#  The peak of the curve is centered at 5seconds, which shows that, on average, listeners time courses lagged behind the speakers.  #@NEW_LINE#@#  In comparison, the time courses of the listeners were synchronized (with each other) at 0sec (Fig 3, blue curve).  #@NEW_LINE#@#  

Listener-listener_BOLD_coupling  #@NEW_LINE#@#  
As a verification of the fNIRS approach, we reanalyzed an fMRI dataset of 17 subjects listening to story E2 (the Pie-man story), which was recorded and used in a previous study14,27.  #@NEW_LINE#@#  To compare with the fNIRS results, we considered only voxels from the outer layer of the cortex in the neighboring regions of prefrontal and parietal sites.  #@NEW_LINE#@#  The coupling results estimated using the multilevel GLM model are shown in Fig 4.  #@NEW_LINE#@#  Of the 994 investigated voxels, 551 showed significant listener-listener coupling (FDR qless_than0.01.  #@NEW_LINE#@#  This result replicates published result14 and demonstrate a nice convergence across fNIRS and fMRI methods.  #@NEW_LINE#@#  

BOLD_and_HbO_are_correlated_during_comprehension_of_the_same_story  #@NEW_LINE#@#  
Previous studies have shown that fNIRS and fMRI signals are highly correlated across multiple cognitive tasks31,32,38,39.  #@NEW_LINE#@#  In our study, two groups of subjects, the brain activity of one group measured with fNIRS and the other with fMRI, were engaged in the same task of listening to the E2 story (Pie-man).  #@NEW_LINE#@#  We hypothesize that the BOLD and fNIRS signals share common information even though they were measured from different subjects and in different recording environments.  #@NEW_LINE#@#  To directly compare the signals across fMRI and fNIRS, we estimated correlations between spatially overlapping voxel-optode pairs while subjects listened to the exact same story.  #@NEW_LINE#@#  Widespread significant correlations can be found between BOLD and HbO only when the participants were listening to the same story (i.e.  #@NEW_LINE#@#  E2) as shown in Fig 5a.  #@NEW_LINE#@#  For control stories T1, T2 (Turkish) and E1 (English), we observed no coupling effect.  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
During social interaction, the brains of individuals become coupled as those individuals send and receive signals (light, sound, etc.)  #@NEW_LINE#@#  through the environment, analogous to a wireless communication system8.  #@NEW_LINE#@#  This brain-to-brain coupling relies on the stimulus-to-brain coupling which reflects the brains ability to be coupled with the physical world in order to represent it, veridically and dynamically.  #@NEW_LINE#@#  In this study, we identified the brain-to-brain coupling between a speaker telling an unrehearsed real-life story and a group of listeners listening to the story, with the aid of fNIRS.  #@NEW_LINE#@#  We have demonstrated for the first time that it is feasible to study neural coupling during natural verbal communication with fNIRS.  #@NEW_LINE#@#  While there is a growing literature using fMRI and EEG to study brain-to-brain coupling during social interaction40, the application of fNIRS in the field is still rare.  #@NEW_LINE#@#  Cui et al.  #@NEW_LINE#@#  in 2012 first adopted fNIRS to investigate neural coupling between pairs of subjects playing a simple cooperation and competition game23.  #@NEW_LINE#@#  In the game, participants were asked to press a response key after a go signal, either in synchrony with (cooperation mode) or faster than (competition mode) their partner.  #@NEW_LINE#@#  An increase in neural correlation between the members of a pair was found only during cooperation and was associated with better cooperation performance.  #@NEW_LINE#@#  Holper et al.  #@NEW_LINE#@#  in 2012 investigated neural coupling between a model and an imitator during a finger tapping task24.  #@NEW_LINE#@#  A stronger increase in neural coupling was found during the imitation condition compared with a control condition in which the imitator no longer needed to follow the models tapping pace.  #@NEW_LINE#@#  These two studies, however, involved only simple stimuli and contexts.  #@NEW_LINE#@#  
Our results demonstrated that: (1) the brain activation recorded by fNIRS was coupled between a speaker telling a real-life story and listeners listening to the story (speaker-listener coupling); (2) on average, the listeners brain activity lagged behind that of the speaker (5-s delay); (3) the brain activity evoked by the same story was reliable across the listeners (listener-to-listener coupling); and (4) coupling was not present when listeners heard stories in a language incomprehensible to them.  #@NEW_LINE#@#  These findings are consistent with previous work using fMRI12 and EEG13 and demonstrate that fNIRS can be used to study brain-to-brain coupling during social interaction in a natural communicative context.  #@NEW_LINE#@#  
During brain-to-brain coupling, activity in areas of prefrontal and parietal cortex previously reported to be involved in sentence comprehension were robustly correlated across subjects, as revealed in the inter-subject correlation analysis34.  #@NEW_LINE#@#  As these are task-related (active listening) activation periods (not resting, etc.  #@NEW_LINE#@#  ), the correlations reflect modulation of these regions by the time-varying content of the narratives, and comprise linguistic, conceptual and affective processing.  #@NEW_LINE#@#  As expected, and in agreement with previous work12, the activity among listeners is time locked to the moment of vocalization.  #@NEW_LINE#@#  In contrast, the activity in the listeners brain lagged behind the activity in the speakers brain.  #@NEW_LINE#@#  Though the sluggish hemodynamic response measures of fMRI and fNIRS have lower temporal resolution and could mask the exact temporal speakerlistener coupling, the delay (5-sec) we have identified in this study is consistent with previous work on the multiple timescales of linguistic processing10.  #@NEW_LINE#@#  In particular, analyses integrating single-unit, ECoG, and fMRI data have uncovered a hierarchy of timescales over which natural communication unfolds in the brain, ranging from hundreds of milliseconds (single words) to a few seconds (sentences) to tens of seconds (paragraphs)10.  #@NEW_LINE#@#  These previous findings indicate that each brain area along this hierarchical pathway accumulates and integrates information at a preferred timescale (e.g., early auditory areas integrate phonemes to detect words, while later areas integrate words to form a sentence, etc.)  #@NEW_LINE#@#  and transmits the chunked information upstream to the next processing level.  #@NEW_LINE#@#  The relatively long (5-sec) lag between speaker and listener seems to match the processing timescale of natural discourse, which unfolds at the sentence level.  #@NEW_LINE#@#  In agreement with such an observation, the speaker-listener neural coupling emerges in areas with relatively long processing timescales.  #@NEW_LINE#@#  
Another interesting observation is that significant inter-subject correlations were found primarily between prefrontal areas in the speaker and parietal areas in the listeners.  #@NEW_LINE#@#  This result supports a previous EEG study in which the coupling between speaker and listener was found to be mainly between different channels13.  #@NEW_LINE#@#  In that study, listeners watched the video playback of speakers who were telling either a fairytale or the plot of their favorite movie or book.  #@NEW_LINE#@#  A canonical correlation analysis between the EEG of the speakers and listeners showed that coupling was mainly limited to non-homologous channels although source localization was not applied.  #@NEW_LINE#@#  To our best knowledge, our study presents the first fNIRS-based evidence that the brain-to-brain coupling between speaker and listener was mainly between non-homologous brain areas.  #@NEW_LINE#@#  
In the current study, we further compared the fMRI and fNIRS signal time courses when two groups of subjects were listening to the same audio recording of a real-life story.  #@NEW_LINE#@#  The neural activation of one group was recorded with fNIRS.  #@NEW_LINE#@#  The neural activation of the other group (the fMRI group) was recorded with fMRI in a previous study14.  #@NEW_LINE#@#  We first analyzed the two datasets separately with inter-subject multilevel GLM and found similar patterns of coupling between listeners in the two modalities.  #@NEW_LINE#@#  We then investigated the correlation between fMRI and fNIRS signals and found that HbO and BOLD were significantly correlated, despite the fact that they were collected from different subjects in different recording environments, and with different techniques (fMRI vs. fNIRS).  #@NEW_LINE#@#  Furthermore, the fMRI voxels that were significantly correlated with fNIRS optodes were not randomly distributed but came from brain areas usually considered to be related to listening comprehension1.  #@NEW_LINE#@#  When the fNIRS and fMRI signals corresponding to different stories were compared for control purposes, no significant correlation was found, as expected.  #@NEW_LINE#@#  Significant BOLD-HbR correlations were found but to a much lesser extent compared to HbO (Fig 5a).  #@NEW_LINE#@#  One possible explanation is the superior reliability of HbO across the listeners compared to HbR as shown in Fig 1.  #@NEW_LINE#@#  
For many years, researchers have been interested in comparing the fNIRS and fMRI responses during cognitive tasks.  #@NEW_LINE#@#  Strangman et al.  #@NEW_LINE#@#  simultaneously recorded fNIRS and fMRI in a finger flexion/extension task30.  #@NEW_LINE#@#  Although the authors expected the HbR-BOLD correlation to be strongest due to the causal relation between the HbR and BOLD signals, the results suggested stronger BOLD-HbO correlations.  #@NEW_LINE#@#  The authors suspected this might be due to a higher signal-to-noise ratio (SNR) of HbO in response to the task.  #@NEW_LINE#@#  Cui et al.  #@NEW_LINE#@#  simultaneously recorded fNIRS and fMRI for the same group of subjects during 4 tasks: finger tapping, go/no-go, judgment of line orientation and visuospatial n-back31.  #@NEW_LINE#@#  They found that both HbR and HbO were correlated with BOLD, despite differences in SNR, and the type of task did not significantly affect the correlations.  #@NEW_LINE#@#  HbO-BOLD correlations were found to be slightly but significantly higher than HbR-BOLD.  #@NEW_LINE#@#  Noah et al.  #@NEW_LINE#@#  compared fMRI and fNIRS measurements in a naturalistic task in which participants played the video game Dance Dance Revolution and rested alternately for 30second blocks, and the results suggested a high HbO-BOLD correlation within the same measurement areas32.  #@NEW_LINE#@#  All of the studies above compared the mean triggered average activity induced by averaging a condition over time.  #@NEW_LINE#@#  However, Ben-Yakov et al.  #@NEW_LINE#@#  demonstrated the shortcoming of the triggered averaging method for detecting an event-specific responses which are locked to the structure of each particular exemplar26.  #@NEW_LINE#@#  In our study, each event in the story is unique and singular and cannot be averaged with the responses evoked by other events in the story.  #@NEW_LINE#@#  Our findings provide evidence that the response dynamics to a sequence of events in the story are robust and reliable, and can be detected both with fMRI and fNIRS, by measuring the reliability (correlation) of responses to the story within and between the two methods in real-life complex settings that unfolds over several minutes.  #@NEW_LINE#@#  
The current study, however, is only the first step toward studying brain coupling during natural communication using fNIRS.  #@NEW_LINE#@#  While fNIRS has obvious disadvantages relative to fMRI, which include coarser spatial resolution and an inability to measure signal beneath the cortical surface, it also has some advantages over fMRI.  #@NEW_LINE#@#  The advantages of fNIRS over fMRI include: (1) lower cost; (2) easier setup; (3) higher temporal resolution, (4) greater ecological validity, which can allow face-to-face communication (in fMRI setups subjects cannot see each other); (5) greater ease of connecting two systems simultaneously to test bi-directional dialogue-based communication; and (6) ease of use in real-life clinical communicative contexts.  #@NEW_LINE#@#  In recent studies, fNIRS has been used in extreme settings such as in aerospace applications41,42 and with mobile participants walking outdoors20.  #@NEW_LINE#@#  In the context of the current study, the true advantages of fNIRS can be exploited when the neural activation of two or more subjects is studied during face-to-face conversations in a natural context such as a classroom.  #@NEW_LINE#@#  
Despite these promising results, our study was limited in certain aspects.  #@NEW_LINE#@#  First, for the fNIRS-fMRI comparison, the spatial resolution and coverage of our fNIRS system were limited, so fMRI-fNIRS correlations were estimated between all possible voxel-optode pairs within large cortical areas or the whole brain.  #@NEW_LINE#@#  Second, our data from fNIRS and fMRI were not recorded simultaneously and involved different participants, so any possible between-subject differences should be taken into account when interpreting the results.  #@NEW_LINE#@#  Future studies, preferably with concurrent fNIRS and fMRI, can validate our findings and deepen our understanding of the fNIRS-fMRI relationship for complex natural stimuli.  #@NEW_LINE#@#  Potential future uses of this approach would include everyday settings such as classrooms, where we could investigate communication between a teacher and students, or in business meetings, across a speaker and attendees.  #@NEW_LINE#@#  However, due to the nature of the hemodynamic response that fNIRS measures, rapid communication may be difficult to analyze at short timescales (e.g., at the word-level).  #@NEW_LINE#@#  Future studies can investigate the temporal and spatial limits and requirements of fNIRS-based speaker-listener coupling.  #@NEW_LINE#@#  
In summary, our results showed that: (1) A speakers and listeners brain activity as measured with fNIRS were coupled only when the listeners understood the story; (2) listeners brain activity mirrored the speakers brain activity with a delay, (3) only during listening to the same real-life story, common brain activation patterns were evoked across listeners that were independent of the imaging technology (fNIRS or fMRI) and recording environment (quiet or noisy; sitting down or lying down); and (4) fNIRS and fMRI signals were correlated during the comprehension of the same real-life story.  #@NEW_LINE#@#  These results support fNIRS as a viable future tool to study brain-to-brain coupling during social interaction, in real-life and clinical settings such as a classroom, a business meeting, a political rally, or a doctors office.  #@NEW_LINE#@#  

Materials_and_Methods  #@NEW_LINE#@#  
Participants  #@NEW_LINE#@#  
Three speakers (one male native English speaker, two male native Turkish speakers) and 15 native English listeners (8 females) volunteered to participate in the study and were included in the analysis.  #@NEW_LINE#@#  An additional six subjects participated in the study but were excluded from analysis due to technical issues during recording or the excessive motion artifact (detected both visually and using an automatic algorithm43) presented in the large sensor array data that covers both prefrontal and parietal cortices.  #@NEW_LINE#@#  Subjects were all right-handed (mean LQ=74.5, SD=24.1) based on Edinburgh Handedness Inventory44 and ages 1835 years.  #@NEW_LINE#@#  All subjects had normal or corrected-to-normal vision.  #@NEW_LINE#@#  Participants did not have any history of neurological/mental disorder and were not taking any medication known to affect alertness or brain activity.  #@NEW_LINE#@#  None of the listeners understood Turkish.  #@NEW_LINE#@#  The protocol used in the study was reviewed and approved by the Institutional Review Board (IRB) of the Drexel University (DU).  #@NEW_LINE#@#  The methods were carried out in accordance with approved guidelines and participants gave written informed consent approved by the IRB of DU.  #@NEW_LINE#@#  

Experimental_Procedure  #@NEW_LINE#@#  
All participants were seated comfortably in front of a computer screen throughout the experiment.  #@NEW_LINE#@#  fNIRS data of the three speakers were recorded while they told an unrehearsed real-life story in their native language (either English or Turkish).  #@NEW_LINE#@#  Audio of the stories was recorded using a microphone.  #@NEW_LINE#@#  The resulting one English story (E1) and two Turkish stories (T1 and T2) were played to the listeners later.  #@NEW_LINE#@#  An additional real-life English story E2 (Pie-man, recorded at The Moth, a live storytelling event in New York City) used in several recent fMRI studies of natural verbal communication14,26,27, was also played to the listeners.  #@NEW_LINE#@#  
fNIRS data were recorded from the listeners throughout the audio playbacks.  #@NEW_LINE#@#  The playback sequence always began with E2 (English story, Pie-man), and order of the remaining stories (E1, T1, and T2) was counterbalanced across subjects.  #@NEW_LINE#@#  Before each story playback, short samples of scrambled audio were played to the subjects so they could adjust the volume of the headphones they were wearing.  #@NEW_LINE#@#  Before the start and after the end of the audio story playback, there was a 15-s fixation period for stabilizing the signal.  #@NEW_LINE#@#  Immediately after each playback, subjects were asked to write a detailed report of the story they just heard to verify if they understood the story.  #@NEW_LINE#@#  Figure 6, below, shows the timeline of a story session.  #@NEW_LINE#@#  

Data_Acquisition  #@NEW_LINE#@#  
Two continuous wave optical brain imaging devices were used simultaneously on each participant to record brain activity from prefrontal cortex (PFC) and parietal cortex (PL) using 40 measurement locations (optodes).  #@NEW_LINE#@#  Prefrontal and parietal regions were selected based on the significant areas found in the previous fMRI-based speaker-listener neural coupling study by Stephens, et al.12.  #@NEW_LINE#@#  Anterior prefrontal cortex was recorded by a 16-optode continuous wave fNIRS system (fNIR Imager Model 1100; fNIR Devices, LLC) first described by Chance et al.45 and developed in our lab at Drexel University46,47.  #@NEW_LINE#@#  The sensor was positioned based on the anatomical landmarks as described before in Ayaz et al.47.  #@NEW_LINE#@#  Briefly, the center of the sensor was aligned to the midline and the bottom of the sensor was touching the participants eyebrow so that the center point of the sensor was approximately at Fpz according to the 1020 international system (see Fig 7).  #@NEW_LINE#@#  The sensor has a source-detector distance of 2.5cm and the sampling rate was 2Hz.  #@NEW_LINE#@#  Parietal cortex was recorded using a 24-optode continuous wave Hitachi fNIRS system (ETG 4000; Hitachi Medical Systems).  #@NEW_LINE#@#  Two 3×3 measurement patches were attached to a cap that was customized for the measurement of the parietal cortex.  #@NEW_LINE#@#  For each subject, the center of the two patches was placed at Pz, which was located using a measuring tape.  #@NEW_LINE#@#  Sensors from each patch measured the fNIRS signal of one hemisphere from 12 channels.  #@NEW_LINE#@#  The sensor has a source-detector distance of 3cm and the sampling rate was 10Hz.  #@NEW_LINE#@#  Figure 7 shows the complete sensor setup and optode configuration.  #@NEW_LINE#@#  
Time synchronization markers (triggers) were sent from the stimulus presentation computer to both fNIRS acquisition computers for registration of audio playback start and end times on both fNIRS devices and for temporal aligning recorded data for all subjects.  #@NEW_LINE#@#  
The approximate projection of the channel locations onto the cortical surface in MNI space was estimated using a virtual spatial registration approach48,49.  #@NEW_LINE#@#  In this approach, the sensor patches are virtually placed on an ideal scalp and the projected Montreal Neurological Institute (MNI) coordinates on the cortical surface and the standard deviation of displacement were estimated from the magnetic resonance (MR) images of 17 individuals that were obtained from a publicly available dataset50,51.  #@NEW_LINE#@#  The results are shown in Fig 8.  #@NEW_LINE#@#  The optodes covered regions in the frontopolar area, orbitofrontal area, dorsolateral prefrontal cortex, primary somatosensory cortex, somatosensory association cortex, supramarginal gyrus and angular gyrus.  #@NEW_LINE#@#  Detailed lists of anatomical locations and Brodmann areas corresponding to each optode are included in Table S1 and Table S2, respectively.  #@NEW_LINE#@#  

fNIRS_Data_Preprocessing  #@NEW_LINE#@#  
fNIRS raw light intensity signals were converted to changes in oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR) concentrations using the modified Beer-Lambert law52.  #@NEW_LINE#@#  The raw signal and hemoglobin concentration changes were inspected both visually and also using the automated SMAR algorithm53, which uses a coefficient-of-variation based approach to assess signal quality and reject problematic channels with bad contact or saturated raw light intensity.  #@NEW_LINE#@#  Two optodes, 1 and 15, were over the hairline for most participants and hence were rejected from the study.  #@NEW_LINE#@#  A total of 11.7% were rejected by visual inspection and SMAR.  #@NEW_LINE#@#  Next, the HbO and HbR time series for each optode and participant were band-pass filtered (0.010.5Hz) and the recordings from parietal sites were down-sampled to 2Hz to match the sampling rate of prefrontal sites.  #@NEW_LINE#@#  In the fNIRS literature, various filtering settings have been adopted to reduce physiological artifacts31,54,55,56,57,58.  #@NEW_LINE#@#  The high cutoff of a band-pass filter, for example, ranges from 0.11Hz31,54,55,56.  #@NEW_LINE#@#  We chose 0.010.5Hz cutoffs to reduce slow signal drift and physiological artifacts from cardiac activities but maximally preserve activities related to listening comprehension.  #@NEW_LINE#@#  We considered only the period from 15 to 399seconds, with respect to story start, in the signal time courses from each audio story.  #@NEW_LINE#@#  The first 15s were rejected to account for subjects initial period of adjustment to the listening comprehension task, and 399s is the duration of the shortest story.  #@NEW_LINE#@#  Prior to subsequent analysis, the signal time courses were standardized optode-wise using a Z-score transform.  #@NEW_LINE#@#  

fNIRS_Analysis  #@NEW_LINE#@#  
We first evaluated the reliability of the correlation between listeners brain activity using an inter-subject multilevel GLM similar to the one adopted by Stephens et al.12 for each of the four conditions: E1, E2, T1 and T2.  #@NEW_LINE#@#  We expected neural coupling between listeners to emerge only for the English story conditions E1 and E2, as none of the subjects understood Turkish.  #@NEW_LINE#@#  
At the individual subject level, a GLM with AR(1) (first-order autoregressive) error model was estimated using the average time course of the listeners as the independent variable and the time course of an individual listener as the dependent variable as follows:  #@NEW_LINE#@#  
where  is the time course of a channel from listener k,  is the average time course of a channel from all the other listeners except for listener k,  is the autocorrelation coefficient and .  #@NEW_LINE#@#  AR(1) error model has been frequently adopted in event-related fNIRS analysis to model auto-correlated noise caused by low-frequency drift and physiological processes such as cardio-respiratory and blood pressure changes57.  #@NEW_LINE#@#  
At the group level, we tested the hypothesis using a one-tailed one-sample t-test evaluated on the slopes  () estimated at the individual level.  #@NEW_LINE#@#  We used the BenjaminiHochberg procedure33 to control FDR among 80 statistical tests (2[HbO/HbR]×40[optodes]) with .  #@NEW_LINE#@#  
We evaluated the coupling between optode  of speaker and optode  of the listeners for all permutations of ( ().  #@NEW_LINE#@#  The multilevel GLM was applied as it was for listener-listener coupling except that the time course of optode  of a speaker was used as independent variable  and the time course of optode j of each individual listener  was used as dependent variable .  #@NEW_LINE#@#  
The speaker-listener coupling analysis was repeated by shifting the speakers time course with respect to those of the listeners from 20s to 20s in 0.5s increments.  #@NEW_LINE#@#  At each time shift, FDR was controlled among 3200 statistical tests (2[HbO/HbR]×(40×40)[optode pairs]) with .  #@NEW_LINE#@#  As a comparison, listener-listener coupling was re-evaluated by shifting the average listener time series with respect to that of each individual listener.  #@NEW_LINE#@#  

fMRI_Analysis  #@NEW_LINE#@#  
The fMRI dataset included 17 subjects listening to story E2 (the Pie-man story), which was recorded and used in a previous study14.  #@NEW_LINE#@#  To compare with the fNIRS results, we considered only voxels from the outer layer of the cortex in the neighboring regions of prefrontal and parietal sites as shown in Fig 8.  #@NEW_LINE#@#  The neighboring regions are defined as voxels within a radius of 2.7 standard deviations from the center of each optode projection.  #@NEW_LINE#@#  A total of 994voxels were chosen in this manner.  #@NEW_LINE#@#  The voxel time courses were high-pass filtered at 0.01Hz (for comparison with fNIRS signals) and trimmed to only include 15399seconds (with respect to story start), and the inter-subject multilevel GLM described in section 0 was employed for model analysis.  #@NEW_LINE#@#  FDR was controlled among the 994voxels with .  #@NEW_LINE#@#  

fMRI-fNIRS_Correlation  #@NEW_LINE#@#  
To estimate the correlation between BOLD and fNIRS signals, both signals were high-pass filtered at 0.01Hz.  #@NEW_LINE#@#  The BOLD time courses were z-scored normalized for each voxel and then averaged across the 17 subjects.  #@NEW_LINE#@#  fNIRS time courses were down-sampled to Hz to match the fMRI sampling rate.  #@NEW_LINE#@#  The inter-subject multilevel GLM approach was then applied with the time course of optode  of an fNIRS subject  as independent variable  and the time course of voxel  averaged across fMRI subjects as dependent variable .  #@NEW_LINE#@#  
The aforementioned procedure was applied first to estimate all possible correlations between channels and their corresponding voxels in the left prefrontal (8 optodes×131 voxels), right prefrontal (8optodes×144voxels), left parietal (12optodes×376voxels) and right parietal (12optodes×343voxels) areas.  #@NEW_LINE#@#  The voxels were selected as described in section 0.  #@NEW_LINE#@#  To correct for multiple comparisons, FDR was controlled with a threshold of 0.05.  #@NEW_LINE#@#  




