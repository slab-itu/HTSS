article id="http://dx.doi.org/10.1371/journal.pone.0202344"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Machine learning models in electronic health records can outperform conventional survival models for predicting patient mortality in coronary artery disease  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Prognostic modelling is important in clinical practice and epidemiology for patient management and research.  #@NEW_LINE#@#  Electronic health records (EHR) provide large quantities of data for such models, but conventional epidemiological approaches require significant researcher time to implement.  #@NEW_LINE#@#  Expert selection of variables, fine-tuning of variable transformations and interactions, and imputing missing values are time-consuming and could bias subsequent analysis, particularly given that missingness in EHR is both high, and may carry meaning.  #@NEW_LINE#@#  Using a cohort of 80,000 patients from the CALIBER programme, we compared traditional modelling and machine-learning approaches in EHR.  #@NEW_LINE#@#  First, we used Cox models and random survival forests with and without imputation on 27 expert-selected, preprocessed variables to predict all-cause mortality.  #@NEW_LINE#@#  We then used Cox models, random forests and elastic net regression on an extended dataset with 586 variables to build prognostic models and identify novel prognostic factors without prior expert input.  #@NEW_LINE#@#  We observed that data-driven models used on an extended dataset can outperform conventional models for prognosis, without data preprocessing or imputing missing values.  #@NEW_LINE#@#  An elastic net Cox regression based with 586 unimputed variables with continuous values discretised achieved a C-index of 0.801 (bootstrapped 95% CI 0.799 to 0.802), compared to 0.793 (0.791 to 0.794) for a traditional Cox model comprising 27 expert-selected variables with imputation for missing values.  #@NEW_LINE#@#  We also found that data-driven models allow identification of novel prognostic variables; that the absence of values for particular variables carries meaning, and can have significant implications for prognosis; and that variables often have a nonlinear association with mortality, which discretised Cox models and random forests can elucidate.  #@NEW_LINE#@#  This demonstrates that machine-learning approaches applied to raw EHR data can be used to build models for use in research and clinical practice, and identify novel predictive variables and their effects to inform future research.  #@NEW_LINE#@#  

Citation: Steele AJ, Denaxas SC, Shah AD, Hemingway H, Luscombe NM (2018) Machine learning models in electronic health records can outperform conventional survival models for predicting patient mortality in coronary artery disease.  #@NEW_LINE#@#  PLoS ONE 13(8):  #@NEW_LINE#@#  
           e0202344.  #@NEW_LINE#@#  

        https://doi.org/10.1371/journal.pone.0202344  #@NEW_LINE#@#  
Editor: Tiratha Raj Singh,  #@NEW_LINE#@#  
Jaypee University of Information Technology, INDIA  #@NEW_LINE#@#  

Received: February 5, 2018; Accepted: July 30, 2018; Published:  August 31, 2018  #@NEW_LINE#@#  
Copyright:  Â© 2018 Steele et al.  #@NEW_LINE#@#  This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.  #@NEW_LINE#@#  
Data Availability: While our data does not contain any personal sensitive identifiers, its deemed as sensitive as it contains sufficient clinical information about patients such as dates of clinical events for there to be a potential risk of patient re-identification.  #@NEW_LINE#@#  This restriction has been imposed by the data owner (CPRD/MHRA) and the data sharing agreements between UCL and the CPRD/MHRA.  #@NEW_LINE#@#  Access to data may be requested via the Clinical Practice Research Datalink (CPRD) and applying to the CPRDs Independent Scientific Advisory Committee (https://www.cprd.com/researcher/.  #@NEW_LINE#@#  
Funding: This work was supported by the Francis Crick Institute which receives its core funding from Cancer Research UK (FC001110), the UK Medical Research Council (FC001110), and the Wellcome Trust (FC001110).  #@NEW_LINE#@#  NML and HH were supported by the Medical Research Council Medical Bioinformatics Award eMedLab (grant number MR/L016311/1).  #@NEW_LINE#@#  The CALIBER programme was supported by the National Institute for Health Research (RP-PG-0407-10314, PI HH); Wellcome Trust (WT 086091/Z/08/Z, PI HH); the Medical Research Prognosis Research Strategy Partnership (G0902393/99558, PI HH) and the Farr Institute of Health Informatics Research, funded by the Medical Research Council (K006584/1, PI HH), in partnership with Arthritis Research UK, the British Heart Foundation, Cancer Research UK, the Economic and Social Research Council, the Engineering and Physical Sciences Research Council, the National Institute of Health Research, the National Institute for Social Care and Health Research (Welsh Assembly Government), the Chief Scientist Office (Scottish Government Health Directorates) and the Wellcome Trust.  #@NEW_LINE#@#  
Competing interests:  The authors have declared that no competing interests exist.  #@NEW_LINE#@#  
Introduction  #@NEW_LINE#@#  
Advances in precision medicine will require increasingly individualised prognostic assessments for patients in order to guide appropriate therapy.  #@NEW_LINE#@#  Conventional statistical methods for prognostic modelling require significant human involvement in selection of prognostic variables (based on an understanding of disease aetiology), variable transformation and imputation, and model optimisation on a per-condition basis.  #@NEW_LINE#@#  
Electronic health records (EHR) contain large amounts of information about patients medical history including symptoms, examination findings, test results, prescriptions and procedures.  #@NEW_LINE#@#  The increasing quantity of data in EHR means that they are becoming more valuable for research [15].  #@NEW_LINE#@#  Although EHR are a rich data source, many of the data items are collected in a non-systematic manner according to clinical need, so missingness is often high [6, 7].  #@NEW_LINE#@#  The population of patients with missing data may be systematically different depending on the reason that the data are missing [8]: tests may be omitted if the clinician judges they are not necessary [9], the patient refuses [10], or the patient fails to attend.  #@NEW_LINE#@#  Multiple imputation to handle missing data can be computationally intensive, and needs to include sufficient information about the reason for missingness to avoid bias [11].  #@NEW_LINE#@#  
Conventional_versus_data-driven_approaches_to_prognostic_modelling  #@NEW_LINE#@#  
Conventional statistical models, with a priori expert selection of predictor variables [1, 12], have a number of potential shortcomings.  #@NEW_LINE#@#  First, they may be time-consuming to fit and require expert knowledge of the aetiology of a given condition [13].  #@NEW_LINE#@#  Second, such models are unable to utilise the richness of EHR data; a recent meta-analysis [1] found that EHR studies used a median of just 27 variables, despite thousands potentially being available [5].  #@NEW_LINE#@#  Third, parametric models rely on assumptions which may not be borne out in practice.  #@NEW_LINE#@#  For example Cox proportional hazards models assume that a change in a predictor variable is associated with a multiplicative response in the baseline hazard that is constant over time.  #@NEW_LINE#@#  Nonlinearity and interactions need to be built into models explicitly based on prior clinical knowledge.  #@NEW_LINE#@#  Finally, missing data need to be handled using a separate process such as multiple imputation.  #@NEW_LINE#@#  Given that most imputation techniques assume that data are missing at random [11], this may mean that their results are unreliable in this context.  #@NEW_LINE#@#  Imputation can also be time-consuming both for researchers, and computationally given the large cohorts available in EHR data.  #@NEW_LINE#@#  Categorisation of continuous variables can accommodate nonlinear relationships and allow the inclusion of missing data as an additional category [14], but may substantially increase the number of parameters in the model.  #@NEW_LINE#@#  
Machine-learning approaches include automatic variable selection techniques and non-parametric regression methods which can handle large numbers of predictors and may not require as many assumptions about the relationship between particular variables and outcomes of interest [15].  #@NEW_LINE#@#  These have the potential to reduce the amount of human intervention required in fitting prognostic models [1618].  #@NEW_LINE#@#  For example, random survival forests [1921] can accommodate nonlinearities and interactions between variables, and are not restricted to a common baseline hazard for all patients, avoiding the assumptions inherent in Cox proportional hazards models.  #@NEW_LINE#@#  
Previous studies have used machine learning on EHR data for tasks such as patient classification and diagnosis [2224] or predicting future hospitalisation [25], but we are unaware of any systematic comparison of machine learning methods for predicting all-cause mortality in a large, richly characterised EHR cohort of patients with stable coronary artery disease.  #@NEW_LINE#@#  
In this study we compared different analytic approaches for predicting mortality, using a Cox model with 27 expert-selected variables as the reference [13].  #@NEW_LINE#@#  We aimed to:  #@NEW_LINE#@#  


Methods  #@NEW_LINE#@#  
Data_sources  #@NEW_LINE#@#  
We used a cohort of over 80,000 patients from the CALIBER programme [26].  #@NEW_LINE#@#  CALIBER links 4 sources of electronic health data in England: primary care health records (coded diagnoses, clinical measurements, and prescriptions) from 244 general practices contributing to the Clinical Practice Research Datalink (CPRD); coded hospital discharges (Hospital Episode Statistics, HES); the Myocardial Ischemia National Audit Project (MINAP); and death registrations (ONS).  #@NEW_LINE#@#  CALIBER includes about 4% of the population of England [27] and is representative in terms of age, sex, ethnicity, and mortality [26].  #@NEW_LINE#@#  

Patient_population  #@NEW_LINE#@#  
Patients were eligible to enter the cohort after being registered with the general practice for a year.  #@NEW_LINE#@#  If they had pre-existing coronary artery disease (myocardial infarction [MI], unstable angina or stable angina) they entered the cohort on their eligibility date, otherwise they entered on the date of the first stable angina diagnosis after eligibility, or six months after the first acute coronary syndrome diagnosis (MI or unstable angina) after eligibility.  #@NEW_LINE#@#  The six-month delay was chosen to differentiate long-term prognosis from the high-risk period that typically follows acute coronary syndromes.  #@NEW_LINE#@#  For patients whose index diagnosis was MI, we used information in CPRD and MINAP to attempt to classify the type of MI as ST-elevation myocardial infarction (STEMI) or nonST-elevation myocardial infarction (NSTEMI).  #@NEW_LINE#@#  
Diagnoses were identified in CPRD, HES, or MINAP records according to definitions in the CALIBER data portal (www.caliberresearch.org/portal).  #@NEW_LINE#@#  Stable angina was defined by angina diagnoses in CPRD (Read codes) and HES (ICD-10 codes), repeat prescriptions for nitrates, coronary revascularisation (Read codes in CPRD or OPCS-4 codes in HES), or ischaemia test results (CPRD).  #@NEW_LINE#@#  Acute coronary syndromes were defined in MINAP or diagnoses in CPRD and HES.  #@NEW_LINE#@#  

Follow-up_and_endpoints  #@NEW_LINE#@#  
The primary endpoint was death from any cause.  #@NEW_LINE#@#  Patients were followed up until they died (as identified in ONS or CPRD) or transferred out of practice, or until the last data collection date of the practice.  #@NEW_LINE#@#  

Prognostic_factors  #@NEW_LINE#@#  
Expert-selected predictors [28] were extracted from primary care (CPRD) and coded hospital discharges (HES).  #@NEW_LINE#@#  These included cardiovascular risk factors (e.g.  #@NEW_LINE#@#  hypertension, diabetes, smoking, lipid profile), laboratory values, pre-existing diagnoses and prescribed medication.  #@NEW_LINE#@#  Small-area index of multiple deprivation (IMD) score was derived from the patients postcode.  #@NEW_LINE#@#  Table 1 shows all the expert-selected predictors used.  #@NEW_LINE#@#  
We also generated an extended set of predictor variables for the data-driven models, derived from rich data prior to the index date in CALIBER: primary care (CPRD) tables of coded diagnoses, clinical measurements and prescriptions; and hospital care (HES) tables of coded diagnoses and procedures.  #@NEW_LINE#@#  From each of these sources, we generated new binary variables for the presence or absence of a coded diagnosis or procedure, and numeric variables for laboratory or clinical measurements.  #@NEW_LINE#@#  We selected the 100 least missing variables from each source table, and the 100 least missing of both clinical history and measurements from the primary care data.  #@NEW_LINE#@#  After combining with the pre-selected predictors and removing duplicate and erroneous variables, there were 586 variables per patient in the extended dataset.  #@NEW_LINE#@#  

Ethics  #@NEW_LINE#@#  
Approval was granted by the Independent Scientific Advisory Committee (ISAC) of the Medicines and Healthcare Products Regulatory Agency (protocol 14_107).  #@NEW_LINE#@#  Approval from ISAC is equivalent to approval from an institutional review board.  #@NEW_LINE#@#  All data were fully, irreversibly anonymised before access was provided to the authors.  #@NEW_LINE#@#  

Statistical_methods  #@NEW_LINE#@#  


Results  #@NEW_LINE#@#  
Patient_population  #@NEW_LINE#@#  
A summary of the patient population used in this study is shown in Table 2.  #@NEW_LINE#@#  We initially identified 115,305 patients with coronary disease in CALIBER and, after excluding patients based on criteria relating the timing of the diagnosis and follow-up, 82,197 patients remained in the cohort.  #@NEW_LINE#@#  Imputation models included all 115,305 patients in order to increase precision, with the exclusion flag as an auxiliary variable in imputation models.  #@NEW_LINE#@#  

Age-based_baseline  #@NEW_LINE#@#  
A baseline model with age as the only predictor attained a C-index of 0.74.  #@NEW_LINE#@#  To calculate the risk and assess calibration, the KaplanMeier estimator for patients of age x years in (a bootstrap sample of) the training set is used, resulting in a well-calibrated model with a calibration score of 0.935 (95% confidence interval: 0.924 to 0.944).  #@NEW_LINE#@#  

Calibration_and_discrimination_of_models  #@NEW_LINE#@#  
A summary of the performance results obtained for modelling time to all-cause mortality using various statistical methods and data is shown in Fig 1.  #@NEW_LINE#@#  
Comparing between models, the discrimination performance of random forests is similar to that of the best Cox models, but they performed worse in calibration, underperforming the Cox model with imputed data by 0.098 (0.105 to 0.091, 95% confidence interval on distribution of differences between bootstrap replicates).  #@NEW_LINE#@#  A calibration curve for this model is shown in Fig 2(A).  #@NEW_LINE#@#  In view of this unexpectedly poor calibration performance, we also attempted to fit five-year survival with a classification forest.  #@NEW_LINE#@#  The binary outcome of being dead or alive at five years is essentially a classification problem, and it was possible that metrics for node purity during classification may be more reliable than those used in survival modelling.  #@NEW_LINE#@#  However, this proved to be both worse calibrated and worse discriminating than survival-based models, perhaps due to loss of the large number of censored patients in the dataset.  #@NEW_LINE#@#  A random survival forest implicitly takes censored patients into account by splitting based on logrank scores of survival curves, whereas a simple classification forest must discard them, perhaps losing valuable information.  #@NEW_LINE#@#  
Imputing missing values has some effect on model performance.  #@NEW_LINE#@#  Comparing the two continuous Cox models, column 1s model includes missingness indicators and column 3s model imputes missing values.  #@NEW_LINE#@#  Discrimination is very slightly improved in the imputed model, but calibration performance remains the same.  #@NEW_LINE#@#  Discretising continuous values without imputing has a similar effect, improving C-index slightly with little effect on calibration score.  #@NEW_LINE#@#  
To assess the impact of imputation on performance model-agnostically, we performed an empirical test by fitting a random forest model to the imputed dataset.  #@NEW_LINE#@#  The C-index remained similar, but calibration improves dramatically, almost achieving the same performance as the Cox model.  #@NEW_LINE#@#  This is suggestive but not decisive evidence that some information may leak between training and test sets, or from future observations, during imputation.  #@NEW_LINE#@#  
On the extended dataset, random forests fitted with all variables performed badly, even when cross-validated to optimise mtry.  #@NEW_LINE#@#  After trying the different techniques discussed in Methods to rank important variables, by far the best random forest performance was achieved when variables were ranked by within-variable logrank test.  #@NEW_LINE#@#  This gave rise to a model using 98 of the variables, with a C-index of 0.797 (0.796 to 0.798).  #@NEW_LINE#@#  Unfortunately, even on the extended dataset random forests are similarly poorly calibrated to those fitted to the expert-selected dataset, under-performing the Cox model with imputed data by 0.086 (0.078 to 0.093).  #@NEW_LINE#@#  
A Cox model was also fitted to the extended dataset, with all continuous variables discretised into deciles.  #@NEW_LINE#@#  Variables were again ranked by within-variable logrank tests, and cross-validated to give a model comprising 155 variables.  #@NEW_LINE#@#  Its C-index performance was comparable to other models, and also has the highest median calibration performance of 0.966 (0.961 to 0.970).  #@NEW_LINE#@#  
Finally, an elastic net model was fitted to the discretised extended dataset.  #@NEW_LINE#@#  The optimal value of the hyperparameter  = 0.90 was determined by grid-search, and fixed for the remainder of the analysis.  #@NEW_LINE#@#  The hyperparameter  was selected by ten-fold cross-validation on the full training dataset, returning nonzero coefficients for 270 variables.  #@NEW_LINE#@#  Bootstrapped replicates were again used to assess model performance, giving a calibration score of 0.075 (0.0679 to 0.0812), and the highest discrimination score of all models with a C-index of 0.801 (0.799 to 0.802).  #@NEW_LINE#@#  A calibration plot for this model is shown in Fig 2(B).  #@NEW_LINE#@#  

Effect_of_different_methods_for_missing_data  #@NEW_LINE#@#  

Fig 3(A) shows coefficients associated with variables compared between a model based on imputation, and models based on accounting for missing values with missingness indicators, and discretisation.  #@NEW_LINE#@#  Most of the risks have similar values between models, showing that different methods of dealing with missing data preserve the relationships between variables and outcome.  #@NEW_LINE#@#  
Explicitly coding missing data as a missingness indicator allows us to associate a risk with that variable having no value in a patients record.  #@NEW_LINE#@#  These are examined in Fig 3(B), compared to the range of risks implied by the range of values for a given variable.  #@NEW_LINE#@#  Having a missing value is associated with very different risk depending on which variable is being observed, indicating that missingness carries information about a patients prognosis.  #@NEW_LINE#@#  Where risks fall outside the range of risks associated with values found in the dataset, we validated these associations by plotting survival curves comparing patients with and without a particular value missing.  #@NEW_LINE#@#  These curves, shown in Fig 3(C), agree with the coefficients shown in Fig 3(B): variables whose missingness carries a higher risk than the normal range of values show a worse survival curve for patients with that value missing, and vice-versa.  #@NEW_LINE#@#  

Variable_and_missing_value_effects  #@NEW_LINE#@#  
Next, we examined variable effects between models.  #@NEW_LINE#@#  These are shown in Fig 4, with row (A) showing the continuous and discrete Cox models on unimputed data, and row (B) showing partial effects plots derived from the random survival forest models.  #@NEW_LINE#@#  
The partial effects of these variables show varying levels of agreement depending on model used.  #@NEW_LINE#@#  Risk of death rises exponentially with age, with the discrete Cox models response having a very similar gradient to the continuous model.  #@NEW_LINE#@#  Haemoglobin and total white blood cell count are in close agreement between these models too, with only slight deviations from linearity in the discrete case.  #@NEW_LINE#@#  Creatinine shows a pronounced increase in mortality at both low and high levels in the discrete model, which the linear model is unable to capture.  #@NEW_LINE#@#  The risks associated with having a missing value show broad agreement between the Cox models.  #@NEW_LINE#@#  
Random forest variable effect plots differ somewhat from the equivalent Cox model coefficients.  #@NEW_LINE#@#  The sign of the overall relationship is always the same, but all display quite pronounced deviations from linearity, especially at extreme values.  #@NEW_LINE#@#  

Variable_selection_and_importance_in_data-driven_models  #@NEW_LINE#@#  

Fig 5 shows the permutation variable importances for the 20 most important variables in the random forest and discrete Cox models fitted to the large dataset, after variable selection.  #@NEW_LINE#@#  All three models identify several variables which are also present in the expert-selected dataset, including the two most important (age and smoking status).  #@NEW_LINE#@#  There is also significant agreement within the 20 selected variables between models, with the top three appearing in the same order in the random forest and Cox model, and all appearing in the top five for the elastic net.  #@NEW_LINE#@#  
When performing cross-validation to select variables for the Cox and random forest models, we observed large performance differences between ranking methods.  #@NEW_LINE#@#  Random forest variable importance on the full dataset performed poorly; it was a noisy measure of a variables usefulness, with significantly different variable rankings were returned on successive runs of the algorithm.  #@NEW_LINE#@#  Ranking variables by missingness performed better, but still significantly below the models using expert-selected variables.  #@NEW_LINE#@#  By far the best performance was achieved when variables were ranked by within-variable logrank test, which was used for both the random forest and discrete Cox models fitted here.  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
We compared Cox modelling and data-driven approaches for all-cause mortality prediction in a cohort of over 80,000 patients in an electronic health record dataset.  #@NEW_LINE#@#  All models tested had good performance.  #@NEW_LINE#@#  This suggests that, quantified by discrimination and calibration performance, there is no need to impute EHR data before fitting predictive models, nor to select variables manually.  #@NEW_LINE#@#  It also invites the use of data-driven models in other contexts where important predictor variables are not known.  #@NEW_LINE#@#  
Advantages_of_data-driven_approaches  #@NEW_LINE#@#  
Data-driven modelling has both practical and theoretical advantages over conventional survival modelling [5, 24].  #@NEW_LINE#@#  It may require less user input, through automated variable selection.  #@NEW_LINE#@#  Further, both our method of converting continuous to categorical data (by quantiles), and random forest modelling are robust to arbitrary monotonic transformations of a variable, removing the need to manually normalise or transform data before analysis.  #@NEW_LINE#@#  
The ability to analyse data without the need for prior imputation also saves researcher time, and allows the effect of missingness in particular variables to be examined.  #@NEW_LINE#@#  It also has the potential to improve model performance in settings where missingness carries meaning.  #@NEW_LINE#@#  It is also possible for new patients to be assessed without the need for a clinician to impute, or take additional measurements, in order to obtain a risk score.  #@NEW_LINE#@#  
Discretisation of continuous values can accommodate nonlinear and nonmonotonic responses to variables, without requiring time-consuming choice and tuning of these relationships [47].  #@NEW_LINE#@#  
Machine learning techniques can also make use of more data, which in other datasets or settings may give rise to further improved performance over conventional models.  #@NEW_LINE#@#  Variable selection allows its use in contexts where risk factors are unknown.  #@NEW_LINE#@#  

Comparison_of_modelling_methods  #@NEW_LINE#@#  
We found that random forests did not outperform Cox models despite their inherent ability to accommodate nonlinearities and interactions [48, 49].  #@NEW_LINE#@#  Random forests have a number of shortcomings which may explain this.  #@NEW_LINE#@#  First, only a random subset of variables (mtry) are tried at each split, so datasets that contain a large proportion of uninformative noise variables may cause informative variables to be overlooked by chance at many splits.  #@NEW_LINE#@#  Increasing mtry can improve performance, but often at a large cost in computation time.  #@NEW_LINE#@#  Second, when random forests are used for prediction, the predictions are a weighted average of a subset of the data, and are biased away from the extremes [50].  #@NEW_LINE#@#  This may partly explain their poor calibration.  #@NEW_LINE#@#  
We did not find that discretisation of continuous variables improved model performance, probably because the majority of these variables had associations with prognosis that were close to linear, and the small improvement in fit was offset by the large increase in the number of model parameters.  #@NEW_LINE#@#  
Elastic nets achieved the highest discrimination performance of any model tested, demonstrating the ability of regularisation to select relevant variables and optimise model coefficients in an EHR context.  #@NEW_LINE#@#  

Missing_data  #@NEW_LINE#@#  
We also found that missing values may be associated with greater or lesser risk than any of the measured values depending on the potential reasons for a values presence or absence.  #@NEW_LINE#@#  For example, serum creatinine needs to be monitored in people on certain medication, and is less likely to be measured in healthy people.  #@NEW_LINE#@#  Conversely, missing high-density lipoprotein (HDL) or total cholesterol (TC), used for cardiovascular risk prediction in preventive care, was associated with worse outcomes.  #@NEW_LINE#@#  
There was little difference in model performance with or without imputation of missing values.  #@NEW_LINE#@#  It is possible that this did not have a large effect in the expert-selected dataset because only six variables carried missing values and, of these, only three had risks significantly different from the range of measured values.  #@NEW_LINE#@#  The majority of the variables were Boolean (e.g.  #@NEW_LINE#@#  presence of diagnoses) and were assumed to be completely recorded, where absence of a record was interpreted as absence of the condition.  #@NEW_LINE#@#  
One shortcoming of the use of imputation in our analysis is that, to our knowledge, no software is able to build an imputation model on a training set and then apply that model to new data.  #@NEW_LINE#@#  As a consequence, we performed imputation across the whole dataset, but this violates the principle of keeping training and test data entirely separate to prevent information leaking between them.  #@NEW_LINE#@#  Further, because future values are used in the imputation process, this adds additional potential for introducing bias to models using such data.  #@NEW_LINE#@#  We investigated this empirically by fitting a random forest model to the imputed dataset (see Results) and found some evidence that bias may be introduced.  #@NEW_LINE#@#  If this is the case, the benefits of imputation to model performance may be less than suggested here.  #@NEW_LINE#@#  

Variable_selection  #@NEW_LINE#@#  
Finally, we found that data-driven modelling with large numbers of variables is sensitive to the modelling and variable selection techniques used.  #@NEW_LINE#@#  Random forests without variable selection performed poorly, which is evident from both the poor performance of fitted models, and the lack of utility of random forest variable importance as a measure by which to rank variables during selection.  #@NEW_LINE#@#  
Variable selection using the least missing data performed better, but not as well as expert selection.  #@NEW_LINE#@#  This may be because many of the variables with low missingness were uninformative.  #@NEW_LINE#@#  Variable selection using univariate logrank tests was far more successful, allowing models with slightly higher performance than expert selection, and discrete Cox models based on this displayed the best calibration performance.  #@NEW_LINE#@#  
Elastic net regression offered the best C-index performance, by fitting a Cox model with implicit variable selection.  #@NEW_LINE#@#  Since this selection process operates simultaneously across coefficients for all variables, it is possible that this explains its improved performance over ranking by univariate measures applied to random forests.  #@NEW_LINE#@#  
High-ranking variables in our final model (Fig 5) seem plausible.  #@NEW_LINE#@#  Well-known prognostic factors such as age and smoking status were strongly associated with mortality, as expected.  #@NEW_LINE#@#  Many of the other variables identified, such as prescriptions for cardiovascular medications, are proxy indicators of the presence and severity of cardiovascular problems.  #@NEW_LINE#@#  Finally, some variables are proxies for generalised frailty, such as laxative prescriptions, home visits etc.  #@NEW_LINE#@#  These may be unlikely to considered for a prognostic model constructed by experts as they are not obviously related to cardiovascular mortality, and this demonstrates a potential benefit of data-driven modelling in EHR to both identify these novel variables, and improve model performance by incorporating them.  #@NEW_LINE#@#  
The important caveat is that these correlative relationships are not necessarily causal, and may not generalise beyond the study population or EHR system in which they were originally derived [1].  #@NEW_LINE#@#  For example, our finding that home visits are a highly ranked variable is likely to be indicative of frailty, but its contribution will change depending on the criteria for such visits and the way they are recorded, which may vary between healthcare systems or over time.  #@NEW_LINE#@#  

Disadvantages_of_data-driven_approaches  #@NEW_LINE#@#  
Data-driven and machine learning based techniques come with several disadvantages.  #@NEW_LINE#@#  Firstly, the degree of automation in these methods is not yet complete; random forests worked showed poor performance on the full 586-variable dataset without variable selection.  #@NEW_LINE#@#  Use with other datasets or endpoints currently requires some researcher effort to identify optimal algorithms and variable selection methods.  #@NEW_LINE#@#  It would be useful to develop tools which would automate this, and test them across a multitude of different EHR systems and endpoints.  #@NEW_LINE#@#  
Secondly, it can be difficult to interpret the final models.  #@NEW_LINE#@#  Whilst it is possible to use variable effect plots to understand the relationship between a few variables and an outcome, this is prohibitively complex with high-dimensional models.  #@NEW_LINE#@#  In addition, not imputing missing values makes it difficult to interpret coefficients for partially observed variables, as they include a component of the reason for missingness.  #@NEW_LINE#@#  When prognostic models are used in clinical practice, it is important to be able to trust that the data are incorporated appropriately into the model so that clinicians can justify decisions based on the models.  #@NEW_LINE#@#  
Conventional statistical modelling techniques retain advantages and disadvantages which are the converse of these: models are more readily interpretable, and may generalise better, but at the expense of requiring significant expert input to construct, potentially not making use of the richness of available data, and only being applicable to complete data.  #@NEW_LINE#@#  

Limitations_of_this_study  #@NEW_LINE#@#  
These methods were compared in a single study dataset, and replication in other settings would be needed to demonstrate the generalisability of the findings.  #@NEW_LINE#@#  
The simple approach of using the top 100 least-missing variables from each table for development of the data-driven models is unlikely to be the optimal approach.  #@NEW_LINE#@#  In particular, some highly present variables are administrative and have little prognostic value, while significant but rare variables could be overlooked.  #@NEW_LINE#@#  
There are many types of model we did not consider in this study, including support vector machines and neural networks, which may provide improved prognostic performance.  #@NEW_LINE#@#  However, machine learning libraries for survival data are not well-developed, limiting the model types which can be tested without significant time devoted to software and model development.  #@NEW_LINE#@#  

Conclusion  #@NEW_LINE#@#  
We have demonstrated that machine learning approaches on routine EHR data can achieve comparable or better performance than expert-selected, imputed data in manually optimised models for risk prediction.  #@NEW_LINE#@#  Our comparison of a range of machine learning algorithms found that elastic net regression performed best, with cross-validated variable selection based on logrank tests enabling Cox models and random forests to achieve comparable performance.  #@NEW_LINE#@#  
Data-driven methods have the potential to simplify model construction for researchers and allow novel epidemiologically relevant predictors to be identified, but achieving good performance with machine learning requires careful testing of the methods used.  #@NEW_LINE#@#  These approaches are also disease-agnostic, which invites their further use in conditions with less well-understood aetiology.  #@NEW_LINE#@#  
Eventually, machine learning approaches combined with EHR may make it feasible to produce fine-tuned, individualised prognostic models, which will be particularly valuable in patients with conditions or combinations of conditions which would be very difficult for conventional modelling approaches to capture.  #@NEW_LINE#@#  


Code_and_data  #@NEW_LINE#@#  
The scripts used for analysis in this paper are available at https://github.com/luslab/MLehealth.  #@NEW_LINE#@#  
While our data does not contain any sensitive personal identifiers, it is deemed as sensitive as it contains sufficient clinical information about patients such as dates of clinical events for there to be a potential risk of patient re-identification.  #@NEW_LINE#@#  This restriction has been imposed by the data owner (CPRD/MHRA) the data sharing agreements between UCL and the CPRD/MHRA.  #@NEW_LINE#@#  Access to data may be requested via the Clinical Practice Research Datalink (CPRD) and applying to the CPRDs Independent Scientific Advisory Committee (https://www.cprd.com/researcher/).  #@NEW_LINE#@#  

Acknowledgments  #@NEW_LINE#@#  
The authors thank Sera Aylin Cakiroglu for performing the imputation with MICE, and for her help with the data curation and preparation and the design of the study.  #@NEW_LINE#@#  This work was supported by the Francis Crick Institute which receives its core funding from Cancer Research UK (FC001110), the UK Medical Research Council (FC001110), and the Wellcome Trust (FC001110).  #@NEW_LINE#@#  NML and HH were supported by the Medical Research Council Medical Bioinformatics Award eMedLab (MR/L016311/1).  #@NEW_LINE#@#  The CALIBER programme was supported by the National Institute for Health Research (RP-PG-0407-10314, PI HH); Wellcome Trust (WT 086091/Z/08/Z, PI HH); the Medical Research Prognosis Research Strategy Partnership (G0902393/99558, PI HH) and the Farr Institute of Health Informatics Research, funded by the Medical Research Council (K006584/1, PI HH), in partnership with Arthritis Research UK, the British Heart Foundation, Cancer Research UK, the Economic and Social Research Council, the Engineering and Physical Sciences Research Council, the National Institute of Health Research, the National Institute for Social Care and Health Research (Welsh Assembly Government), the Chief Scientist Office (Scottish Government Health Directorates) and the Wellcome Trust.  #@NEW_LINE#@#  

References  #@NEW_LINE#@#  



