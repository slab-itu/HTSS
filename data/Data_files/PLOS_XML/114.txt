article id="http://dx.doi.org/10.1371/journal.pbio.1002295"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Public Data Archiving in Ecology and Evolution: How Well Are We Doing?  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Policies that mandate public data archiving (PDA) successfully increase accessibility to data underlying scientific publications.  #@NEW_LINE#@#  However, is the data quality sufficient to allow reuse and reanalysis?  #@NEW_LINE#@#  We surveyed 100 datasets associated with nonmolecular studies in journals that commonly publish ecological and evolutionary research and have a strong PDA policy.  #@NEW_LINE#@#  Out of these datasets, 56% were incomplete, and 64% were archived in a way that partially or entirely prevented reuse.  #@NEW_LINE#@#  We suggest that cultural shifts facilitating clearer benefits to authors are necessary to achieve high-quality PDA and highlight key guidelines to help authors increase their datas reuse potential and compliance with journal data policies.  #@NEW_LINE#@#  

Citation: Roche DG, Kruuk LEB, Lanfear R, Binning SA (2015) Public Data Archiving in Ecology and Evolution: How Well Are We Doing?  #@NEW_LINE#@#  PLoS Biol 13(11):  #@NEW_LINE#@#  
           e1002295.  #@NEW_LINE#@#  

        https://doi.org/10.1371/journal.pbio.1002295  #@NEW_LINE#@#  
Published:  November 10, 2015  #@NEW_LINE#@#  
Copyright:  © 2015 Roche et al.  #@NEW_LINE#@#  This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited  #@NEW_LINE#@#  
Funding: DGR and SAB were supported by grants from the Australian National University (http://biology.anu.edu.au), the Natural Science and Engineering Research Council of Canada (http://www.nserc-crsng.gc.ca/index_eng.asp) and the Fonds de Recherche du Québec Nature et Technologies (http://www.frqnt.gouv.qc.ca/accueil).  #@NEW_LINE#@#  LEBK and RL were supported by Australian Research Council Future Fellowships (http://www.arc.gov.au/ncgp/futurefel/future_default.htm).  #@NEW_LINE#@#  The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.  #@NEW_LINE#@#  
Competing interests:  The authors have declared that no competing interests exist.  #@NEW_LINE#@#  
Abbreviations:  #@NEW_LINE#@#  
          CC0,  #@NEW_LINE#@#  
            Creative Commons Zero licence; DORA,  #@NEW_LINE#@#  
            Declaration on Research Assessment; E&E,  #@NEW_LINE#@#  
            ecology and evolution; GBIF,  #@NEW_LINE#@#  
            Global Biodiversity Information Facility; JDAP,  #@NEW_LINE#@#  
            Joint Data Archiving Policy; KNB,  #@NEW_LINE#@#  
            Knowledge Network for Biocomplexity; PDA,  #@NEW_LINE#@#  
            public data archiving  #@NEW_LINE#@#  
The expectation of PDA that exists in genetics and molecular biology is rapidly permeating throughout ecology and evolution.  #@NEW_LINE#@#  With the advent of data archiving policies and integrated data repositories, journals and funders now have effective means of mandating PDA.  #@NEW_LINE#@#  However, the quality of publicly archived data associated with experimental and observational (nonmolecular) studies in ecology and evolution is highly variable.  #@NEW_LINE#@#  Illustration by Ainsley Seago.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pbio.1002295.g001  #@NEW_LINE#@#  
At the time of data deposition in the repository, journals had either a strong PDA policy or adhered to the Joint Data Archiving Policy (JDAP), both of which require that data necessary to replicate a studys results be archived in a public repository.  #@NEW_LINE#@#  Datasets were examined to assess completeness and reusability.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pbio.1002295.t001  #@NEW_LINE#@#  
Scoring system and criteria used to assess data completeness and reusability of 100 studies with data archived in the public repository Dryad.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pbio.1002295.t002  #@NEW_LINE#@#  
How_Well_Are_We_Doing?  #@NEW_LINE#@#  
We found considerable variation in the quality of publicly archived data from the 100 studies surveyed, even though all were published either in JDAP journals or journals with a strong PDA policy.  #@NEW_LINE#@#  In most studies (56%), the archived datasets were incomplete, either because of missing data or insufficient metadata, resulting in a completeness score of 3 or less (Figs 1 and 2A).  #@NEW_LINE#@#  Therefore, these studies do not comply with the PDA policy of the journal in which they were published (Fig 2A), as strong policies (JDAP or other) require all the data supporting a papers results to be available in a public repository.  #@NEW_LINE#@#  Secondly, datasets for 64% of studies were archived in a way that either partially or fully prevented reuse (Fig 2B), either because they lacked essential metadata, because the data were presented in processed rather than raw form, or because inadequate file formats were used (e.g., non-machine-readable file formats, such as pdf, that require specialized software to read) (Fig 2B).  #@NEW_LINE#@#  Thus, even if these datasets could in theory be used to reproduce a studys results, their value is questionable.  #@NEW_LINE#@#  Finally, there was a strong correlation between the completeness and reusability scores (Fig 3; R = 0.59 ± 0.07 SE, p less_than 0.001; see S3 Text for further details).  #@NEW_LINE#@#  In 22% of studies, some or all of the archived data were presented as electronic supplementary material.  #@NEW_LINE#@#  This is not ideal since, unlike files archived on Dryad, there are no standards for organizing supplementary data both within and across journals [37], and such data are often not readily discoverable or openly accessible (to those without a relevant journal subscription, for example) [33].  #@NEW_LINE#@#  
Frequency distribution of public data archiving (PDA) scores for (A) completeness and (B) reusability across 100 studies in 2012 (light blue bars) and 2013 (dark blue bars).  #@NEW_LINE#@#  A score of 5 indicates exemplary archiving, and a score of 1 indicates poor archiving (see Table 2).  #@NEW_LINE#@#  Studies with completeness scores of 3 or lower (left of the red dashed line in panel A) do not comply with their journal's PDA policy.  #@NEW_LINE#@#  Studies to the left of the red dashed line in panel B have a reusability score between average (score of 3) and very poor (score of 1).  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pbio.1002295.g002  #@NEW_LINE#@#  
Empty circles are individual data points (offset to avoid overlap).  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pbio.1002295.g003  #@NEW_LINE#@#  
These findings are concerning given that (1) the studies were published in journals that enforce PDA, (2) our completeness score likely underestimates the number of irreproducible results since we did not attempt to replicate each studys statistical analyses (see [9]), and (3) one key objective of PDA beyond increasing reproducibility is to accelerate scientific progress by facilitating data reuse [2,5,7].  #@NEW_LINE#@#  Recent enforcement of PDA policies has had a positive effect on data deposition rates [22,23].  #@NEW_LINE#@#  However, most journals do not verify the quality of archived data beyond basic checks such as ensuring that a data availability statement and a valid DOI are provided in the manuscript [3840].  #@NEW_LINE#@#  Therefore, datasets can contain involuntary errors and omissions [38]; we ourselves acknowledge errors made and possible improvements to past archived datasets.  #@NEW_LINE#@#  
Almost 40% of the 56 non-JDAP or non-journal policy compliant studies lacked only small amounts of data (completeness score of 3; Fig 2A).  #@NEW_LINE#@#  This suggests that many of these omissions are unintended and can be avoided with some slight improvements to data archiving practices.  #@NEW_LINE#@#  It is important to note, however, that authors concerned about potential individual costs of PDA (see [1,4144]) can deliberately archive data to make them difficult or impossible for a third party to reuse (e.g., by archiving incomplete data or data in unusable formats) [12,17,41,4547].  #@NEW_LINE#@#  Notable examples have recently been pointed out on Twitter and other social media [41,4850].  #@NEW_LINE#@#  
Many authors willingly participate in PDA because they believe in sharing data from publicly funded research, they wish to contribute to science beyond their own publications, and/or because they see individual benefits in doing so (e.g., increased citation rate [51], opportunities for coauthorship and new collaborations [1,2,7,20]).  #@NEW_LINE#@#  Despite these motivations, we uncovered a suite of problems that made understanding and assessing data difficult: omission of data necessary to reproduce results, nonexistent or insufficient data descriptors (e.g., no unit specifications or explanations of abbreviations and column headings in tables), inflexible file formats (e.g., .sav files that required the proprietary software SPSS Statistics to open), nonstandard data formats (e.g., colour coding of cells in tables, unspecified column headings), poor data organization (e.g., unclear tab labels for Excel documents with multiple spreadsheets, mismatches between column headings and variable labels in the associated paper, variable labels in a language other than English), and inclusion of poorly identified data unrelated to the paper (e.g., unspecified subsets of the data used for the analyses).  #@NEW_LINE#@#  The most common pitfalls that affected data reusability were inadequate metadata, the use of proprietary and non-machine-readable file formats (e.g., data tables archived as PDF and word documents; S1 Table, S2 Table), and failure to archive raw data (S3 Table).  #@NEW_LINE#@#  
Ecologists and evolutionary biologists receive little or no training in data management and may be unfamiliar with the best practices for proper data archiving (Table 3) [12,30,52].  #@NEW_LINE#@#  The fact that a datasets completeness score was generally higher than its reusability score suggests that authors understand their obligation to share data but struggle to do this effectively (Fig 3, S3 Text).  #@NEW_LINE#@#  Small, simple improvements can dramatically increase the reusability of archived data with minimal time or monetary investments (e.g., [53,54]).  #@NEW_LINE#@#  We summarise key recommendations in Table 3.  #@NEW_LINE#@#  Based on our assessment of articles, we found that the datasets that had the highest completeness and reusability scores were often those in which the authors explicitly linked the archived data to figures and analyses in the paper.  #@NEW_LINE#@#  This simple practice greatly enhances the organization and interpretability of the data, enabling both authors and third parties to verify that all data points are present.  #@NEW_LINE#@#  
References listed provide specific details and more extensive discussion on these topics.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pbio.1002295.t003  #@NEW_LINE#@#  

Which_Way_Forward?  #@NEW_LINE#@#  
Participation in PDA is on the rise, but its benefits require that authors archive complete and reusable datasets.  #@NEW_LINE#@#  Suggestions to improve acceptance of PDA policies are diverse and include treating data associated with journal articles as formal publications (i.e., publish data papers) [6,20,40,61,62], providing incentives for best practices so that authors voluntarily archive high-quality, reusable data [2,7,28,53], and allowing reasonable embargoes for researchers who have planned further uses for their data [1,19,21,36].  #@NEW_LINE#@#  Obviously, increased policing of publicly archived datasets by journals and/or archive curators (i.e., reviewing archived data) should also increase the quality of archived data [22,24,38,45,63].  #@NEW_LINE#@#  All of these recommendations have merit, but it is unlikely that there is one ideal solution.  #@NEW_LINE#@#  
From a practical point of view, enforcing PDA on unwilling authors is largely ineffective because cheating is easytrying to reproduce the results of every submitted manuscript is virtually impossible.  #@NEW_LINE#@#  Publishing data papers is a valid solution for large, important datasets with a high reuse potential [40,64], but there are good reasons to think that this model is both impractical and unlikely to succeed for data that underlie most publications [62], namely because many datasets are limited in their size, scope, and/or novelty, which might not warrant publication in a data journal [40,61].  #@NEW_LINE#@#  Reviewers and editors are also already overloaded with article peer reviews, almost always without compensation from publishers.  #@NEW_LINE#@#  Therefore, additional requests to police data associated with traditional papers could be perceived as unreasonable [6].  #@NEW_LINE#@#  Finally, data repositories currently lack the funding to perform thorough technical reviews to verify that datasets and metadata are complete and concordant with the information in a paper [6,36].  #@NEW_LINE#@#  For example, Dryad is currently forced to charge archiving fees to operate [60] but only has enough curators to perform basic checks on data submissions such as verifying that files can be opened and are free of viruses [65].  #@NEW_LINE#@#  
Rather than punishing researchers who do not share their data, there are strong arguments for rewarding those who do [1,66,67].  #@NEW_LINE#@#  This idea is in line with recent calls for a culture shift towards more collaboration in science [68,69], in which the value and importance of PDA is emphasized and greater benefits given to active participants [1,12,31,33,63].  #@NEW_LINE#@#  These benefits can take many forms, including credit from hiring or promotion committees and funding agencies [12], as well as prizes from departments, societies, and publishers for most reusable or reused dataset, best data paper, or most reproducible results [63].  #@NEW_LINE#@#  An important move in this direction was the 2013 San Francisco Declaration on Research Assessment (DORA), which recommends considering datasets and other types of scientific contributions (e.g., software, training) when scientists research outputs are evaluated [70].  #@NEW_LINE#@#  
Importantly, sociological studies (both experimental and theoretical) point to the fact that both sticks and carrots are necessary to improve cooperation [71,72].  #@NEW_LINE#@#  A recent theoretical study of a public good game, a standard framework for cooperation in groups, showed that the policy first carrot, then stick is highly successful at promoting cooperation because it combines the effectiveness of rewarding to establish cooperation with the effectiveness of punishing to maintain it [72].  #@NEW_LINE#@#  Those who comply must first be rewarded, and, once compliance has become the norm, it can become mandatory and enforced by a penalty for noncompliance [72].  #@NEW_LINE#@#  This strategy has major advantages for PDA in that offering carrots can shift the culture to the point at which authors publicly archive their data even when they are not required to do so [12].  #@NEW_LINE#@#  

Conclusion  #@NEW_LINE#@#  
Our results suggest that at least some parts of public data archives are being used to maintain datasets in E&E that are of little use for reproducing existing studies or carrying out new ones.  #@NEW_LINE#@#  These findings, combined with those of the few other studies that have also explored this issue [9,27], suggest that the problem is ubiquitous, touching both molecular and nonmolecular fields of biology.  #@NEW_LINE#@#  Clearly, improvements to current PDA practices are necessary.  #@NEW_LINE#@#  Solutions might not be straightforward, but they may have to include strategies combining enforcement, reward, and flexibility [1].  #@NEW_LINE#@#  Importantly, PDA is quite new for ecologists and evolutionary biologists, and our results indicate that substantial improvements to its value can be made with relatively little effort.  #@NEW_LINE#@#  
Data_Availability  #@NEW_LINE#@#  
The data and code for this study are available on the repository figshare: http://dx.doi.org/10.6084/m9.figshare.1393269.  #@NEW_LINE#@#  

Data_Reuse  #@NEW_LINE#@#  
The list of publications with associated data archived in Dryad from inception to 20 Sep 2013 was kindly compiled and publicly archived by Vision et al.  #@NEW_LINE#@#  [73].  #@NEW_LINE#@#  




