article id="http://dx.doi.org/10.1371/journal.pone.0141965"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
A Bayesian Developmental Approach to Robotic Goal-Based Imitation Learning  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
A fundamental challenge in robotics today is building robots that can learn new skills by observing humans and imitating human actions.  #@NEW_LINE#@#  We propose a new Bayesian approach to robotic learning by imitation inspired by the developmental hypothesis that children use self-experience to bootstrap the process of intention recognition and goal-based imitation.  #@NEW_LINE#@#  Our approach allows an autonomous agent to: (i) learn probabilistic models of actions through self-discovery and experience, (ii) utilize these learned models for inferring the goals of human actions, and (iii) perform goal-based imitation for robotic learning and human-robot collaboration.  #@NEW_LINE#@#  Such an approach allows a robot to leverage its increasing repertoire of learned behaviors to interpret increasingly complex human actions and use the inferred goals for imitation, even when the robot has very different actuators from humans.  #@NEW_LINE#@#  We demonstrate our approach using two different scenarios: (i) a simulated robot that learns human-like gaze following behavior, and (ii) a robot that learns to imitate human actions in a tabletop organization task.  #@NEW_LINE#@#  In both cases, the agent learns a probabilistic model of its own actions, and uses this model for goal inference and goal-based imitation.  #@NEW_LINE#@#  We also show that the robotic agent can use its probabilistic model to seek human assistance when it recognizes that its inferred actions are too uncertain, risky, or impossible to perform, thereby opening the door to human-robot collaboration.  #@NEW_LINE#@#  

Citation: Chung MJ-Y, Friesen AL, Fox D, Meltzoff AN, Rao RPN (2015) A Bayesian Developmental Approach to Robotic Goal-Based Imitation Learning.  #@NEW_LINE#@#  PLoS ONE 10(11):  #@NEW_LINE#@#  
           e0141965.  #@NEW_LINE#@#  

        https://doi.org/10.1371/journal.pone.0141965  #@NEW_LINE#@#  
Editor: Rachel L. Kendal,  #@NEW_LINE#@#  
Centre for Coevolution of Biology & Culture, University of Durham, UNITED KINGDOM  #@NEW_LINE#@#  

Received: January 22, 2015; Accepted: October 15, 2015; Published:  November 4, 2015  #@NEW_LINE#@#  
Copyright:  Â© 2015 Chung et al.  #@NEW_LINE#@#  This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited  #@NEW_LINE#@#  
Data Availability: The data and code for this paper are now available on github as follows: Robot data: https://github.com/mjyc/chung2015plosone_robot_data.  #@NEW_LINE#@#  Matlab code: https://github.com/afriesen/gpgaze.  #@NEW_LINE#@#  
Funding: This work was supported by Office of Naval Research (ONR) Science of Autonomy, grant no.  #@NEW_LINE#@#  N000141310817, http://www.onr.navy.mil/; Office of Naval Research (ONR) Cognitive Science program, grant no.  #@NEW_LINE#@#  N000140910097, http://www.onr.navy.mil/; National Science Foundation (NSF), NSF grant no.  #@NEW_LINE#@#  SMA-1540619 and NSF grant no.  #@NEW_LINE#@#  IIS-1318733, http://www.nsf.gov/; Intel Science and Technology Center (ISTC), Seattle; and (NSERC) fellowship to A. Friesen, http://www.nserc-crsng.gc.ca/index_eng.asp.  #@NEW_LINE#@#  The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.  #@NEW_LINE#@#  
Competing interests:  The Intel Science and Technology Center, Seattle, is included in the list of acknowledgments due to their assistance with the project.  #@NEW_LINE#@#  This does not alter the authors adherence to PLOS ONE policies on sharing data and materials.  #@NEW_LINE#@#  
Introduction  #@NEW_LINE#@#  
Considerable progress has been made in robotics in recent years, particularly in the area of human-robot interaction.  #@NEW_LINE#@#  Techniques have been proposed to impart new skills to robots via programming by demonstration [1] and imitation learning [2].  #@NEW_LINE#@#  An important remaining challenge is endowing a robot with the ability to infer the intentions of humans and to learn new skills not by naively following demonstrated action trajectories (trajectory-based or action imitation) but through goal-based imitation [3, 4].  #@NEW_LINE#@#  
Developmental scientists have shown that infants can infer the goal of an adults actions.  #@NEW_LINE#@#  In one set of experiments with 18-month old infants [5], an adult actor demonstrated an act in which the goal-state was not achieved; infants were able to read through the literal body movements to infer the underlying goal and execute the intended act.  #@NEW_LINE#@#  In other experiments, children employed different means to achieve the same goal as adults.  #@NEW_LINE#@#  When given a barbell-shaped object, adults used their hands to pull apart the object, but children who could not grasp the end of an oversized barbell used alternative means (e.g., holding one end in both hands and clasping the other end between their knees) to pull it apart.  #@NEW_LINE#@#  The children thus not only inferred the adults goal but could also use novel alternative means to achieve the goal.  #@NEW_LINE#@#  
Clues to how children acquire the ability to infer goals have come from studies on gaze following, the ability of humans to follow the line of regard of another human.  #@NEW_LINE#@#  A naive interpretation of gaze following is that it is simply the imitation of head movement.  #@NEW_LINE#@#  However, Meltzoff and Brooks [6] controlled for head movements and showed that infants tendency to follow gaze varied as a function of whether or not the actors view was blocked by an intervening opaque occluder (a blindfold).  #@NEW_LINE#@#  In particular, 18-month-old children did not follow the gaze of an adult who made a head movement toward an object while wearing a blindfold.  #@NEW_LINE#@#  Younger children (12-month-olds) did mistakenly follow the gaze of a blindfolded adult.  #@NEW_LINE#@#  However, after these younger children were given the self-experience of a blindfold blocking their own view, they no longer followed the gaze of the blindfolded adult.  #@NEW_LINE#@#  
These results highlight the importance of self-experience in the development of goal inference capabilities and goal-based imitation.  #@NEW_LINE#@#  They are consistent with Meltzoffs Like-Me hypothesis [7] which states that children utilize internal models learned through self-experience to interpret the acts and intentions of others, and with increasing experience, acquire increasingly sophisticated intent recognition abilities.  #@NEW_LINE#@#  Such an approach is different from but incorporates elements of two previous well-known theories of intent inference: (i) simulation theory [8], which proposes that the same mental resources we use for thinking, decision-making, and emotional responses are redeployed in imagination to provide an understanding of others, and (ii) theory theory [9, 10], which advocates that children develop theories about the world and about others, make predictive inferences about behaviors and inner states of others using this network of beliefs (theories), and revise theories according to new observations.  #@NEW_LINE#@#  The approach taken here differs by providing a rigorous mathematical framework based on Bayesian inference of goals and actions, and the bootstrapping of Bayesian models through learning from self-experience.  #@NEW_LINE#@#  

Methods  #@NEW_LINE#@#  
Our hypothesis is that humans use a goal-directed mechanism for planning motor acts.  #@NEW_LINE#@#  A goal, provided by either an internal desire or external stimulus, together with the current state, determines what the action should bethis is called a policy.  #@NEW_LINE#@#  An action executed in a given state in turn determines the next state (probabilistically).  #@NEW_LINE#@#  We represent these dependencies using the graphical model shown in Fig 1a where G is the goal, A is the action, Xi is the current (or initial) state, and Xf is the next (or final) state.  #@NEW_LINE#@#  Throughout this paper, we use upper case letters to denote random variables and lower case letters to denote specific values of the variables.  #@NEW_LINE#@#  
(a) The graphical model shows the probabilistic dependencies between different random variables in the model: G = goal, A = action, Xi = current state, and Xf = final state.  #@NEW_LINE#@#  The model captures how actions depend on goals and states, and how the state changes as a consequence of executing an action; (b) incorporates the influence of blindfold self-experience on the model using the random variable B; (c) shows the combined graphical models, one for the agent and a copy for the mentor (superscript m), for following the gaze of a mentor.  #@NEW_LINE#@#  Shaded variables denote observed variables.  #@NEW_LINE#@#  The darker shading indicates that B is an observed discrete variable, while the rest of the nodes are continuous.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0141965.g001  #@NEW_LINE#@#  
Consider our two example scenarios.  #@NEW_LINE#@#  In the context of gaze following, the goal is a desired fixation location, the action is a vector of motor commands to move the head, and the state represents head position and orientation.  #@NEW_LINE#@#  We discuss the model assuming movement of the head but the model can be generalized to embody movement of the eyes or movement of both the head and eyes to achieve a spatial target.  #@NEW_LINE#@#  In this case, the random variables (states, actions, goals) are continuous-valued and we define the relationships between them using (learned) probabilistic functions.  #@NEW_LINE#@#  In the case of our example of actions on objects, we assume the states represent discrete locations on a table, actions comprise of high-level commands such as pick and push, and the goal is a desired (discrete) location on the table.  #@NEW_LINE#@#  We use this example to illustrate the discrete formulation of the model in which the relationships between the discrete-valued random variables are captured by (learned) probability tables.  #@NEW_LINE#@#  
Computational_Model  #@NEW_LINE#@#  

Case_II__Discrete-Valued_Random_Variables  #@NEW_LINE#@#  
When the random variables for states, actions, and goals are discrete-valued, the model can be expressed in terms of conditional probability tables for the transition probabilities P(XfXi, A) and the policy P(AG, X).  #@NEW_LINE#@#  
For concreteness, we describe this case in the context of a simple tabletop task involving a set of small objects on a tabletop which can be moved around by a human or a robotic arm as shown in Fig 2.  #@NEW_LINE#@#  The discretized position of an object defines its state and a goal corresponds to the object reaching a particular state.  #@NEW_LINE#@#  The robotic arm can manipulate the state of any object using a set of actions.  #@NEW_LINE#@#  We define X to be the set of discrete states in the environment, A to be the set of all possible actions available to the robot (these can be different from the possible actions of the human demonstrator) and G to be the set of possible goal states.  #@NEW_LINE#@#  We assume all three sets are finite.  #@NEW_LINE#@#  
Each goal g  G represents an abstract task which can be achieved using one or more actions in A.  #@NEW_LINE#@#  For example, a goal can be moving an object to location A regardless of its starting location.  #@NEW_LINE#@#  The object could be picked and placed at A, pushed to A, or transported using some other action, but the goal remains the same.  #@NEW_LINE#@#  The dynamics of the action and its effect are modeled as a Markov state-transition model (Fig 3a); when the robot in a state xi executes an action ai, it enters a state xf with the transition probability P(Xf = xfXi = xi, A = ai).  #@NEW_LINE#@#  

Robot  #@NEW_LINE#@#  
We use the Gambit robot arm-and-gripper (Fig 2a) designed at Intel Labs Seattle.  #@NEW_LINE#@#  Gambit is well-suited to tabletop manipulation tasks with small objects and has previously been shown to perform well in tasks with humans in the loop [18].  #@NEW_LINE#@#  


Results  #@NEW_LINE#@#  
Gaze_Following__Model_Simulations  #@NEW_LINE#@#  
To test our model, we randomly sample goal positions and then compute the action required to fixate on this goal.  #@NEW_LINE#@#  We add Gaussian noise to this action, compute the resulting gaze vector if this action were taken, and add Gaussian noise to this gaze vector.  #@NEW_LINE#@#  This method is equivalent to training the model with rejection sampling wherein the agent rejects all samples that do not result in successful fixation on the goal position.  #@NEW_LINE#@#  The Gaussian processes are trained on this randomly generated data and then tested on separate test data.  #@NEW_LINE#@#  The default reference frame for both agent and mentor is at the origin gazing along the x-axis.  #@NEW_LINE#@#  Each agent has their own reference frame and we assume that we know the transformation from the mentors reference frame to the agents.  #@NEW_LINE#@#  This transformation is not learned by our model but we believe that this is a minor assumption, especially since we already assume the agent can observe the mentors position and head pose.  #@NEW_LINE#@#  
The mentor and agent are positioned as shown in Fig 4.  #@NEW_LINE#@#  Goal locations for the training data were generated uniformly at random from the area between the agent and the mentor (within the rectangle formed by x in [100, 500] and y in [500,500], where the agent is at (0,0) and the mentor is at (600,0)).  #@NEW_LINE#@#  We used Gaussian noise with standard deviation of 3 degrees for angles and a standard deviation of 10 cm for locations and distances.  #@NEW_LINE#@#  For reverse inference, the prior goal state P(G) is a Gaussian centered halfway between the two agents along the x-axis.  #@NEW_LINE#@#  While this prior is quite weak, a single observation of (xi, xf) is insufficient to overcome the prior in reverse inference.  #@NEW_LINE#@#  Instead, we use a single observation of xi and five observations of xf to get an accurate estimate of the goal distribution P(Gxi, xf).  #@NEW_LINE#@#  More precisely, we run reverse inference with the observed values  to compute , and then use this as the prior for a second run of reverse inference to compute .  #@NEW_LINE#@#  We repeat this five times to compute .  #@NEW_LINE#@#  We believe such an inference process could be executed within the short amount of time taken for gaze following.  #@NEW_LINE#@#  

Robotic_Tabletop_Manipulation_Task  #@NEW_LINE#@#  
To illustrate the discrete-valued version of the model, we used a tabletop manipulation with three toy objects of different shapes: a pear, a lemon, and a miniature broiled chicken.  #@NEW_LINE#@#  These objects could be in one of three different states: LEFT, RIGHT, and OFFTABLE, as defined in the previous section.  #@NEW_LINE#@#  The robots aim is to learn the consequences of actions on these objects, infer the goals of human actions, and imitate the actions of humans manipulating these objects on the table.  #@NEW_LINE#@#  

Fig 7 shows the learned transition probabilities for manipulating these objects based on the robots self-discovery phase.  #@NEW_LINE#@#  The transition models are learned by performing 10 trials for each initial state and action pair (xi, a) for each object type.  #@NEW_LINE#@#  We deliberately used a small number of trials to test whether the method could cope with less training data and more uncertainty in the transition model.  #@NEW_LINE#@#  
Since the state space is small, we are able to enumerate and test all of the interesting and possible human demonstrations.  #@NEW_LINE#@#  By interesting, we mean that the state changes after action execution (for example, from RIGHT to OFFTABLE) and the initial state is not OFFTABLE.  #@NEW_LINE#@#  There are in total four interesting state changes for each object that can be demonstrated by a human: LEFT to RIGHT, LEFT to OFFTABLE, RIGHT to LEFT, and RIGHT to OFFTABLE.  #@NEW_LINE#@#  Note that our current implementation does not allow the robot to pick up objects that are located OFFTABLE  #@NEW_LINE#@#  

Fig 8a shows the inferred goals given all possible interesting state changes, using the graphical model in Fig 3d.  #@NEW_LINE#@#  For all cases, our model correctly infers the intended goal state of the human.  #@NEW_LINE#@#  Fig 8b shows the maximum a posteriori probability (MAP) action for a given initial state and goal.  #@NEW_LINE#@#  Our model correctly identifies whether a place or a push action is better, given the dynamics of the object as encoded in the learned probabilistic model for the object.  #@NEW_LINE#@#  Note that the two most preferred actions are always push or place actions in the correct direction.  #@NEW_LINE#@#  
Finally, Fig 8c shows the predicted state distribution given an action and an initial state.  #@NEW_LINE#@#  The robot calculates the posterior probability of getting to the desired state, and executes the action if this probability is above a predetermined threshold (%50).  #@NEW_LINE#@#  Otherwise, it asks the human collabo For example, the predicted output of moving a pear from RIGHT to LEFT (1st row of 3rd column in Fig 8c) using the pushing left action is lower than %50, and therefore the robot will ask request the human help.  #@NEW_LINE#@#  

Table 1 compares trajectory-based imitation of the human demonstration with our proposed goal-based approach.  #@NEW_LINE#@#  The trajectory-based approach simply mimics the human action without considering the goal or uncertainty, i.e., it executes a place action if the human executes a place, and a push action if the human executes a push.  #@NEW_LINE#@#  The goal-based approach on the other hand recognizes the goal and uses the best action it has available to achieve the goal.  #@NEW_LINE#@#  
Using our computed transition probabilities, we can calculate the hypothetical success rate of a purely trajectory-based approach.  #@NEW_LINE#@#  For our goal-based approach, we use the posterior distribution shown in Fig 8b.  #@NEW_LINE#@#  Finally, the Interactive Goal-Based mode assumes that the robot may ask a human for help, with a 100% success rate when the human executes the requested action.  #@NEW_LINE#@#  The third column in Table 1 shows what the performance would be if we require the robot to be 50% sure of reaching a desired state.  #@NEW_LINE#@#  We do not see perfect imitation results on the third column because the robot does not ask the human for help in every case.  #@NEW_LINE#@#  In some cases, the probability of success will surpass the confidence threshold, but the goal state may not be reached after the action is executed.  #@NEW_LINE#@#  
The results demonstrate the expected behavior of the goal-based method.  #@NEW_LINE#@#  The success rates in goal-based method (Table 1, second column) are identical across the different actions demonstrated (e.g.  #@NEW_LINE#@#  pick & place row vs. push row).  #@NEW_LINE#@#  In addition, the results demonstrate the advantage of a goal-based approach over purely trajectory-based imitation, and its potential as a human-robot collaboration mechanism.  #@NEW_LINE#@#  


Contributions_and_Comparisons_with_Related_Work  #@NEW_LINE#@#  
Our work makes contributions both to the fields of robotics and cognitive modeling.  #@NEW_LINE#@#  In robotics, our approach contributes to the growing number of efforts leveraging biological paradigms and developmental science to design new methods for social robotics and human-robot interaction [17, 1930].  #@NEW_LINE#@#  Specifically, our proposed framework for goal inference and imitation lays the foundation for a new approach to designing social robots as robots that learn internal models of their environment through self-experience and utilize these models for human intent recognition, skill acquisition from human observation, and human-robot collaboration.  #@NEW_LINE#@#  Although our results are based on proof-of-concept systems, the underlying Bayesian framework is general.  #@NEW_LINE#@#  We anticipate being able to progressively increase the sophistication of our models by leveraging the rapid advances being made in probabilistic reasoning, Bayesian modeling, and learning.  #@NEW_LINE#@#  As an illustrative example, the success of simultaneous localization and mapping (SLAM) algorithms in robotics [31] in the past decade can be largely attributed to the availability of efficient algorithms and computing platforms for probabilistic reasoning, within the broader context of probabilistic robotics [32].  #@NEW_LINE#@#  
Our approach to robotic imitation emphasizes the importance of goal inference in imitation learning, compared to traditional methods for programming-by-demonstration that have relied on following action trajectories.  #@NEW_LINE#@#  Previous approaches to robotic imitation that have also relied on high-level goals (e.g., [33, 34]) have not emphasized self-discovery of probabilistic models, a central tenet of the developmental approach proposed here for bootstrapping goal-based imitation.  #@NEW_LINE#@#  Other robotics work has focused on attempting to model continuous low-level goals [35].  #@NEW_LINE#@#  Neither of these approaches adopt probabilistic models for human-robot interaction tasks.  #@NEW_LINE#@#  Approaches that do utilize Bayesian methods for goal inference [36, 37] and robotic imitation [38] have done so without the developmental science perspective that we bring to the problem.  #@NEW_LINE#@#  
In the context of cognitive modeling, our model can be regarded as a Bayesian instantiation of the Like-Me developmental hypothesis [7, 39].  #@NEW_LINE#@#  It acknowledges a role for a certain type of mental simulation [8] as well as a learned theory [9, 10] of actions and their consequences using a probabilistic model learned from experience (see also [40]).  #@NEW_LINE#@#  The model is closely related to the goal-based imitation model of Verma and Rao [3, 17, 41] and the inverse planning model of Baker et al.  #@NEW_LINE#@#  [42].  #@NEW_LINE#@#  Also related to our approach are models for planning based on probabilistic inference [3, 4346].  #@NEW_LINE#@#  
Meltzoffs Like-Me hypothesis has previously been applied in robotics to tackle the important problem of whom to imitate [47]: In this case, a robot first builds a self model, and then uses this self-model to discover self-other equivalences with other robots so as to distinguish an appropriate from an inappropriate mentor robot.  #@NEW_LINE#@#  Note that in this case as well as in our own work, the mentor agent is not necessarily an active teacher but is only being observed (cf.  #@NEW_LINE#@#  [48]).  #@NEW_LINE#@#  A recent developmental approach to robotic imitation [49] also focuses on tabletop manipulation tasks, with greater complexity than our experiments, but without the benefits of bootstrapping derived from the Like-Me hypothesis inherent in our framework.  #@NEW_LINE#@#  

Summary_and_Conclusion  #@NEW_LINE#@#  
Our results suggest that the process of imitation-based learning can be bootstrapped by (i) learning a probabilistic model of the perceptual consequences of ones own actions through self-experience, (ii) using this learned model to infer the goals of actions of others, and (iii) using the inferred goals to perform goal-based imitation.  #@NEW_LINE#@#  Such a strategy works even when the imitators actuators are different from the demonstrators.  #@NEW_LINE#@#  We first showed how such a model can emulate infant development of gaze following by implementing gaze following via goal inference in learned graphical models based on Gaussian processes.  #@NEW_LINE#@#  Using a table-top robotic system, we demonstrated that a robot can learn probabilistic models of actions on objects and utilize these models for inferring the intent of human actions on objects.  #@NEW_LINE#@#  Additionally, we showed that the approach facilitates human-robot collaboration by allowing the robot to predict the success probability of achieving a goal and to seek human help when success probability is low.  #@NEW_LINE#@#  
Our results point to a number of interesting open issues.  #@NEW_LINE#@#  For the studies in this paper, we used a simple exhaustive exploration strategy, but for larger state spaces, a more sophisticated approach based on reward functions (e.g., [50]) could be employed.  #@NEW_LINE#@#  Additionally, following the example of human infants, some form of directed self-exploration based on observing human mentors (e.g., [51]) may be desirable.  #@NEW_LINE#@#  The model assumes that states in the environment are known (corresponding to the case of MDPs or Markov decision processes)an interesting direction for future work is extending the model to the more realistic case where only observations of states are available (partially observable MDPs or POMDPs) and where learning involves optimizing rewards.  #@NEW_LINE#@#  Finally, our current model involves inference over a single time-step, which simplifies the inference problem.  #@NEW_LINE#@#  We hope to explore the applicability of our model for multi-step inference problems in future work.  #@NEW_LINE#@#  
The approach we have presented lends itself naturally to generalization based on relational probabilistic models [52, 53] and hierarchical Bayesian representations [54].  #@NEW_LINE#@#  Such models have the potential to significantly increase the scalability and applicability of our suggested approach to large-scale scenarios, besides facilitating transfer of learned skills across tasks and domains.  #@NEW_LINE#@#  We intend to investigate such relational models in future work.  #@NEW_LINE#@#  

Acknowledgments  #@NEW_LINE#@#  
This research was supported by ONR Science of Autonomy grant no.  #@NEW_LINE#@#  N000141310817, ONR Cognitive Science program grant no.  #@NEW_LINE#@#  N000140910097, NSF grant no.  #@NEW_LINE#@#  SMA-1540619 and NSF grant no.  #@NEW_LINE#@#  IIS-1318733, by the Intel Science and Technology Center (ISTC), Seattle, and by an NSERC fellowship to A. Friesen.  #@NEW_LINE#@#  The authors would like to thank Marc Deisenroth, Cynthia Matuszek and Kendall Lowrey for their advice, sharing code, and for help during the initial stages of this project.  #@NEW_LINE#@#  

Author_Contributions  #@NEW_LINE#@#  
Conceived and designed the experiments: RR MC AF AM DF.  #@NEW_LINE#@#  Performed the experiments: MC AF.  #@NEW_LINE#@#  Analyzed the data: MC AF.  #@NEW_LINE#@#  Contributed reagents/materials/analysis tools: MC AF.  #@NEW_LINE#@#  Wrote the paper: RR MC AF AM DF.  #@NEW_LINE#@#  

References  #@NEW_LINE#@#  



