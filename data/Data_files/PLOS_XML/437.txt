article id="http://dx.doi.org/10.1371/journal.pcbi.1004896"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Deep Neural Networks as a Computational Model for Human Shape Sensitivity  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Theories of object recognition agree that shape is of primordial importance, but there is no consensus about how shape might be represented, and so far attempts to implement a model of shape perception that would work with realistic stimuli have largely failed.  #@NEW_LINE#@#  Recent studies suggest that state-of-the-art convolutional deep neural networks (DNNs) capture important aspects of human object perception.  #@NEW_LINE#@#  We hypothesized that these successes might be partially related to a human-like representation of object shape.  #@NEW_LINE#@#  Here we demonstrate that sensitivity for shape features, characteristic to human and primate vision, emerges in DNNs when trained for generic object recognition from natural photographs.  #@NEW_LINE#@#  We show that these models explain human shape judgments for several benchmark behavioral and neural stimulus sets on which earlier models mostly failed.  #@NEW_LINE#@#  In particular, although never explicitly trained for such stimuli, DNNs develop acute sensitivity to minute variations in shape and to non-accidental properties that have long been implicated to form the basis for object recognition.  #@NEW_LINE#@#  Even more strikingly, when tested with a challenging stimulus set in which shape and category membership are dissociated, the most complex model architectures capture human shape sensitivity as well as some aspects of the category structure that emerges from human judgments.  #@NEW_LINE#@#  As a whole, these results indicate that convolutional neural networks not only learn physically correct representations of object categories but also develop perceptually accurate representational spaces of shapes.  #@NEW_LINE#@#  An even more complete model of human object representations might be in sight by training deep architectures for multiple tasks, which is so characteristic in human development.  #@NEW_LINE#@#  

Author_Summary  #@NEW_LINE#@#  
Shape plays an important role in object recognition.  #@NEW_LINE#@#  Despite years of research, no models of vision could account for shape understanding as found in human vision of natural images.  #@NEW_LINE#@#  Given recent successes of deep neural networks (DNNs) in object recognition, we hypothesized that DNNs might in fact learn to capture perceptually salient shape dimensions.  #@NEW_LINE#@#  Using a variety of stimulus sets, we demonstrate here that the output layers of several DNNs develop representations that relate closely to human perceptual shape judgments.  #@NEW_LINE#@#  Surprisingly, such sensitivity to shape develops in these models even though they were never explicitly trained for shape processing.  #@NEW_LINE#@#  Moreover, we show that these models also represent categorical object similarity that follows human semantic judgments, albeit to a lesser extent.  #@NEW_LINE#@#  Taken together, our results bring forward the exciting idea that DNNs capture not only objective dimensions of stimuli, such as their category, but also their subjective, or perceptual, aspects, such as shape and semantic similarity as judged by humans.  #@NEW_LINE#@#  

Citation: Kubilius J, Bracci S, Op de Beeck HP (2016) Deep Neural Networks as a Computational Model for Human Shape Sensitivity.  #@NEW_LINE#@#  PLoS Comput Biol 12(4):  #@NEW_LINE#@#  
           e1004896.  #@NEW_LINE#@#  

        https://doi.org/10.1371/journal.pcbi.1004896  #@NEW_LINE#@#  
Editor: Matthias Bethge, University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY  #@NEW_LINE#@#  
Received: August 26, 2015; Accepted: March 30, 2016; Published:  April 28, 2016  #@NEW_LINE#@#  
Copyright:  © 2016 Kubilius et al.  #@NEW_LINE#@#  This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.  #@NEW_LINE#@#  
Data Availability: All data files are available from the Open Science Framework (https://osf.io/jf42a).  #@NEW_LINE#@#  
Funding: This work has been funded by the The Belgian Science Policy (IUAP P7/11, http://www.belspo.be/iap/) and the European Research Council (ERC-2011-Stg-284101; http://erc.europa.eu/) grants awarded to HPOdB.  #@NEW_LINE#@#  JK is a research assistant of the Research FoundationFlanders (FWO; http://www.fwo.be/) and holds a Postdoctoral Mandate from the Internal Funds KU Leuven (http://www.kuleuven.be/research/support/).  #@NEW_LINE#@#  The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.  #@NEW_LINE#@#  
Competing interests:  The authors have declared that no competing interests exist.  #@NEW_LINE#@#  
Introduction  #@NEW_LINE#@#  
Understanding how the human visual system processes visual information involves building models that would account for human-level performance on a multitude of tasks.  #@NEW_LINE#@#  For years, despite the best efforts, computational understanding of even the simplest everyday tasks such as object and scene recognition have been limited to toy datasets and poor model performances.  #@NEW_LINE#@#  For instance, hierarchical architecture HMAX [1], once known as the standard model of vision [2], worked successfully on a stimulus set of paper clips and could account for some rapid categorization tasks [3] but failed to capture shape and object representations once tested more directly against representations in the visual cortex (e.g., [46]).  #@NEW_LINE#@#  
Recently, however, deep neural networks (DNNs) brought a tremendous excitement and hope to multiple fields of research.  #@NEW_LINE#@#  For the first time, a dramatic increase in performance has been observed on object and scene categorization tasks [7,8], quickly reaching performance levels rivaling humans [9].  #@NEW_LINE#@#  More specifically in the context of object recognition, stimulus representations developed by the deep nets have been shown to account for neural recordings in monkey inferior temporal cortex and functional magnetic resonance imaging data throughout the human ventral visual pathway (e.g., [6,10,11]), suggesting that some fundamental processes, shared across different hardware, have been captured by deep nets.  #@NEW_LINE#@#  
The stimulus sets on which DNNs have been tested in these previous studies allow the inference that there is a general correspondence between the representations developed within DNNs and important aspects of human object representations at the neural level.  #@NEW_LINE#@#  However, these stimulus sets were not designed to elucidate specific aspects of human representations.  #@NEW_LINE#@#  In particular, a long tradition in human psychophysics and primate physiology has pointed towards the processing of shape features as the underlying mechanism behind human object recognition (e.g., [1215]).  #@NEW_LINE#@#  Cognitive as well as computational models of object recognition have mainly focused upon the hierarchical processing of shape (e.g., [1,16,17]).  #@NEW_LINE#@#  There are historical and remaining controversies about the exact nature of these shape representations, such as about the degree of viewpoint invariance and the role of structural information in the higher levels of representation (e.g., [18,19]).  #@NEW_LINE#@#  Still, all models agree on the central importance of a hierarchical processing of shape.  #@NEW_LINE#@#  For this reason we hypothesized that the general correspondence between DNNs representations and human object representations might be related to a human-like sensitivity for shape properties in the DNNs.  #@NEW_LINE#@#  
Here we put this hypothesis to the test through a few benchmark stimulus sets, which have highlighted particular aspects of human shape perception in the past.  #@NEW_LINE#@#  We first demonstrate that convolutional neural networks (convnets), the most common kind of DNN models in image processing, can recognize objects based upon shape also when all other cues are removed, as humans can.  #@NEW_LINE#@#  Moreover, we show that despite being trained solely for object categorization, higher layers of convnets develop a surprising sensitivity for shape that closely follows human perceptual shape judgments.  #@NEW_LINE#@#  When we dissociate shape from category membership, then abstract categorical information is available to a limited extent in these networks, suggesting that a full model of shape and category perception might require richer training regimes for convnets.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Experiment_1__Recognition_from_shape  #@NEW_LINE#@#  
If convnets are indeed extracting perceptually relevant shape dimensions, they should be able to utilize shape for object recognition.  #@NEW_LINE#@#  This ability should extend to specific stimulus formats that highlight shape and do not include many other cues, such as silhouettes.  #@NEW_LINE#@#  The models have been trained for object recognition with natural images, how would they perform when all non-shape cues are removed?  #@NEW_LINE#@#  In order to systematically evaluate how convnet recognition performance depends on the amount of available shape and non-shape (e.g., color or texture) information, we employed the colorized version of the Snodgrass and Vanderwart stimulus set of common everyday objects [20,21].  #@NEW_LINE#@#  This stimulus set consists of 260 line drawings of common objects that are easily recognizable to human observers and has been used extensively in a large number of studies (Google Scholar citations: over 4000 to [20]; over 500 to [21]).  #@NEW_LINE#@#  In our experiments, we used a subset of this stimulus set (see Methods), consisting of 61 objects (Fig 1A).  #@NEW_LINE#@#  Three variants of the stimulus set were used: original color images, greyscale images, and silhouettes.  #@NEW_LINE#@#  
First, we asked 30 human observers (10 per variant of the stimulus set) to choose a name of each object, presented for 100 ms, from a list of 657 options, corresponding to the actual of these objects and their synonyms as defined by observers in [20].  #@NEW_LINE#@#  Consistent with previous studies [15,21], participants were nearly perfect in naming color objects, slightly worse for grayscale objects, and considerably worse for silhouettes (Fig 1B, gray bands).  #@NEW_LINE#@#  Moreover, we found that participants were very consistent in their responses (Fig 1C, gray bands).  #@NEW_LINE#@#  
We then presented three convnets with the stimuli and asked them to produce a single best guess of what might be depicted in the image.  #@NEW_LINE#@#  A correct answer was counted if the label exactly matched the actual label.  #@NEW_LINE#@#  We found that all deep nets exhibited a robust categorization performance on the original color stimulus set, reaching about 8090% accuracy (Fig 1B, with the best model (GoogLeNet) reaching human level of performance.  #@NEW_LINE#@#  Given that the models have not been trained at all on abstract line drawings, we found it an impressive demonstration of convnet feature generalization.  #@NEW_LINE#@#  
As textural cues were gradually removed, convnets still performed reasonably well.  #@NEW_LINE#@#  In particular, switching to grayscale decreased the performance by about 15%, whereas a further decrease by 30% occurred when inner gradients were removed altogether (silhouette condition).  #@NEW_LINE#@#  In other words, even when an object is defined solely by its shape, convnets maintain a robust and highly above-chance performance.  #@NEW_LINE#@#  Notably, a similar pattern of performance was observed when humans were asked to categorize these objects, suggesting that models are responding similarly to humans but are overall less accurate (irrespective of stimulus variant).  #@NEW_LINE#@#  
To investigate the consistency between human and model responses in more detail, we computed a squared Euclidean distance between the average human accuracy and a model accuracy, and normalized it to the range [0, 1], such that a consistency of .5 means that a model responded correctly where a human responded correctly and made a mistake where a human made a mistake about half of the time (Fig 1C; see Methods for reasoning behind this choice of consistency).  #@NEW_LINE#@#  Overall, the consistency was substantial and nearly reached between-human consistency for color objects for our best model (GoogLeNet).  #@NEW_LINE#@#  To visualize the amount of consistency, we depicted The best models (GoogLeNet) performance on silhouettes against human performance (Fig 1D).  #@NEW_LINE#@#  The performances are well correlated as indicated by the slope of the logistic regression being reliably above zero (Fig 1D; z-test on GoogLeNet: z = 2.549, p = .011; CaffeNet: z = 2.393, p = .017; VGG-19: z = 2.323, p = .020).  #@NEW_LINE#@#  Furthermore, we computed consistency between models and found that for each variant of the stimulus set, the models appear to respond similarly and commit similar mistakes (the between-model consistency is about .8 for each pairwise comparison), indicating that the models learn similar features.  #@NEW_LINE#@#  
Fig 1D also shows that the models sometimes outperformed humans, seemingly in those situations where a model could take an advantage of a limited search space (e.g., it is much easier to say there is an iron when you do not know about hats).  #@NEW_LINE#@#  Overall, however, despite the moderate yet successful performance on silhouettes, it is obvious from Fig 1D that there are quite some stimuli on which the models fail but which are recognized perfectly by human observers.  #@NEW_LINE#@#  Common mistakes could be divided into two groups: (i) similar shape (grasshopper instead of bee), and (ii) completely wrong answers where the reason behind models response is not so obvious (whistle instead of lion).  #@NEW_LINE#@#  We think that the former scenario further supports the idea that models base their decisions primarily on shape and are not easily distracted by the lack of other features.  #@NEW_LINE#@#  In either case, the errors might be remedied by model exposure to cartoons and drawings.  #@NEW_LINE#@#  Moreover, we do not think that these discrepancies might be primarily due to the lack of recurrent processes in these models since we tried to minimize influences of possible recurrent processes during human categorization by presenting stimuli for 100 ms to human observers.  #@NEW_LINE#@#  It is also possible that better naturalistic training sets in general are necessary where objects would be decoupled from background.  #@NEW_LINE#@#  For instance, lions always appear in savannahs, so models might be putting too much weight on savannahs features for detecting a lion, which would be a poor strategy in the case of this stimulus set.  #@NEW_LINE#@#  Nonetheless, even in the absence of such training, convnets generalize well to such unrealistic stimuli, demonstrating that they genuinely learn some abstract shape representations.  #@NEW_LINE#@#  

Experiment_2__Physical_vs_perceived_shape  #@NEW_LINE#@#  
In Experiment 2, we wanted to understand whether convolutional neural networks develop representations that capture the shape dimensions that dominate perception, the so-called perceived shape dimensions, rather than the physical (pixel-based) form.  #@NEW_LINE#@#  In most available stimulus sets these two dimensions are naturally correlated because the physical form and the perceived shape are nearly or completely identical.  #@NEW_LINE#@#  In order to disentangle the relative contributions of each of these dimensions, we needed stimulus sets where a great care was taken to design perceptual dimensions that would differ from physical dimensions.  #@NEW_LINE#@#  

Experiment_3__Non-accidental_properties  #@NEW_LINE#@#  
In 1987, Biederman put forward the Recognition-by-Components (RBC) theory [16] that proposed that objects recognition might be based on shape properties known as non-accidental.  #@NEW_LINE#@#  Under natural viewing conditions, many objects properties are changing, depending on lighting, clutter, viewpoint and so on.  #@NEW_LINE#@#  In order to recognize objects robustly, Biederman proposed that the visual system might utilize those properties that remain largely invariant under possible natural variations.  #@NEW_LINE#@#  In particular, Biederman focused on those properties of object shape that remain unchanged when the three-dimensional shape of an object is projected to the two-dimensional surface on the eyes retina, such as curved versus straight object axis, parallel versus converging edges, and so on [23].  #@NEW_LINE#@#  Importantly, RBC theory predicts that observers should notice a change in a non-accidental property more readily than an equivalent change in a metric property.  #@NEW_LINE#@#  Consider, for example, geons shown in Fig 4A, top row.  #@NEW_LINE#@#  Both the non-accidental and the metric variant differ by the same amount from the base geon (as measured by some linear metric, such a pixelwise or GaborJet difference), yet the non-accidental one appears more distinct to us.  #@NEW_LINE#@#  
Over years, Biederman and others consistently found such preference to hold in a large number of studies across species [2528], age groups [2931], non-urban cultures [32], and even in the selectivity of inferior temporal neurons in monkeys [24,33].  #@NEW_LINE#@#  This idea of invariants has also been shown to play an important role in scene categorization [34] and famously penetrated computer vision literature when David Lowe developed his SIFT (Scale-Invariant Feature Transform) descriptor that attempted to capture invariant features in an image [35].  #@NEW_LINE#@#  
Thus, the sensitivity for non-accidental properties presents an important and well-tested line of research where the physical size of differences between shapes is dissociated from the effect of specific shape differences on perception.  #@NEW_LINE#@#  We tested the sensitivity for non-accidental properties using a previously developed stimulus set of geon triplets where the metric variant is as distinct or, typically, even more distinct from the base than the non-accidental variant as measured in the metric (physical) space.  #@NEW_LINE#@#  Nevertheless, humans and other species report perceiving non-accidental shapes as more dissimilar from the base than the metric ones, presenting us with a perfect test case where, similar to Exp.  #@NEW_LINE#@#  2, physical shape similarity is different from the perceived one.  #@NEW_LINE#@#  
We evaluated model performance on this set of 22 geons (Fig 4A) that have been used previously in behavioral [31,32,36] and neurophysiological studies.  #@NEW_LINE#@#  A models response was counted as accurate if the response to a non-accidental stimulus was more dissimilar from the base than the metric one.  #@NEW_LINE#@#  
We found that all deep but not shallow or HMAX models (except for HMAX99) showed a higher than chance performance (Fig 4B) with performance typically improving gradually throughout the architecture (Fig 4C; bootstrapped related samples significance test for deep vs. shallow, one-tailed: p less_than .001; deep vs. HMAX: p = .011).  #@NEW_LINE#@#  Moreover, deeper networks tended to perform slightly better than shallower ones, in certain layers even achieving perfect performance.  #@NEW_LINE#@#  Overall, there was not any clear pattern in mistakes across convnets, except for a tendency towards mistakes in the main axis curvature, that is, convnets did not seem to treat straight versus curved edges as very distinct.  #@NEW_LINE#@#  In contrast, humans consistently show a robust sensitivity to changes in the main axis curvature [31,36].  #@NEW_LINE#@#  Note that humans are also not perfect at detecting NAPs as reported by [36].  #@NEW_LINE#@#  Thus, we do not go further into these differences because the RBC theory and most previous behavioral and neural studies only address a general preference for NAP changes, and hence do not provide a systematic framework for interpreting the presence or absence of such preference for specific NAPs.  #@NEW_LINE#@#  

Experiment_4__The_role_of_category_when_dissociated_from_shape  #@NEW_LINE#@#  
In the first three experiments, we demonstrated convnet sensitivity to shape properties.  #@NEW_LINE#@#  However, these convnets have been explicitly trained to optimize not for shape but rather category, that is, to provide a correct semantic label.  #@NEW_LINE#@#  Apparently, categorization is aided by developing sensitivity to shape.  #@NEW_LINE#@#  But is there anything beyond sensitivity to shape then that convnets develop?  #@NEW_LINE#@#  In other words, to what extent do these networks develop semantic representations similar to human categorical representations over and above mere shape information?  #@NEW_LINE#@#  
Typically, object shape and semantic properties are correlated, such that objects from the same category (e.g., fruits) share some shape properties as well (all have smooth roundish shape) that may distinguish them from another category (e.g., cars that have more corners), making it difficult to investigate the relative contributions of these two dimensions.  #@NEW_LINE#@#  To overcome these limitations, Bracci and Op de Beeck [37] recently designed a new stimulus set, comprised of 54 photographs of objects, where shape and category dimensions are orthogonal to each other as much as possible (Fig 5A).  #@NEW_LINE#@#  In particular, objects from six categories have been matched in such a way that any one exemplar from a particular category would have a very similar shape to an exemplar from another category.  #@NEW_LINE#@#  Thus, the dissociation between shape and category is more prominent and can be meaningfully measured by asking participants to judge similarity between these objects based either on their shape or on their category.  #@NEW_LINE#@#  By correlating the resulting dissimilarity matrices to human neural data, Bracci and Op de Beeck [37]found that perceived shape and semantic category are represented in parallel in the visual cortex.  #@NEW_LINE#@#  
We employed this stimulus set to explore how categorical information is represented by convnets.  #@NEW_LINE#@#  As before, participants were asked to judge similarity among stimuli based either on their shape or on their category.  #@NEW_LINE#@#  Note that even for categorical judgments, participants were asked to rate categorical similarity rather than divide stimulus set into six categories, resulting in idiosyncratic categorical judgments and consistency between humans not reaching ceiling.  #@NEW_LINE#@#  
First, we found that convnets represented shape fairly well, correlating with perceptual human shape judgments between .3 and .4, nearly reaching the human performance limit (Fig 5C and 5D).  #@NEW_LINE#@#  Unlike before, the effect was not specific to deep models but was also observed in HMAX and even shallow models.  #@NEW_LINE#@#  This observation is expected because, unlike in previous experiments, in this stimulus set physical form and perceived shape are well correlated.  #@NEW_LINE#@#  Instead, the purpose of this stimulus set was to investigate to what extent semantic human category judgments are captured by convnets, since here category is dissociated from shape.  #@NEW_LINE#@#  We found that all deep but not shallow or HMAX models captured at least some semantic structure in our stimuli (Fig 5D and 5E; bootstrapped related samples significance test for deep vs. shallow and deep vs. HMAX: p less_than .001), indicating that representations in convnets contain both shape and category information.  #@NEW_LINE#@#  Similar to Exp.  #@NEW_LINE#@#  1, comparable correlations were observed even when the models were provided only with silhouettes of the objects (no texture), indicating that such categorical decisions appear to rely mainly on the shape contour and not internal features.  #@NEW_LINE#@#  
The abundance of categorical information in convnet outputs is most strikingly illustrated in Fig 5B where a multidimensional scaling plot depicts overall stimulus similarity.  #@NEW_LINE#@#  A nearly perfect separation between natural and manmade objects is apparent.  #@NEW_LINE#@#  Note that less than a half of these objects (23 out of 54) were known to GoogLeNet, but even completely unfamiliar objects are nonetheless correctly situated.  #@NEW_LINE#@#  This is quite surprising given that convnets were never trained to find associations between different categories.  #@NEW_LINE#@#  In other words, there is no explicit reason why a convnet should learn to represent guitars and flutes similarly (the category of musical instruments is not known to the model).  #@NEW_LINE#@#  We speculate that these associations might be learned implicitly, since during training objects of the same superordinate category (musical instruments) might co-occur in images.  #@NEW_LINE#@#  Further tests would be necessary to establish the extent of such implicit learning in convnets.  #@NEW_LINE#@#  
Despite its significance, the correlation with categorical judgments was much weaker than with shape, even after we restricted stimuli to the 23 objects in the ImageNet, meaning that the learned representations in convnets are largely based on shape and not category.  #@NEW_LINE#@#  In other words, categorical information is not as dominant in convnets as in humans, in agreement with [6] where deep nets were shown to account for categorical representations in humans only when categorical training was introduced on top of the outputs of convnets.  #@NEW_LINE#@#  (See also Discussion where we talk about the availability of information in models.)  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
Here we demonstrated that convolutional neural networks are not only superior in object recognition but also reflect perceptually relevant shape dimensions.  #@NEW_LINE#@#  In particular, convnets demonstrated robust similarities with human shape sensitivities in three demanding stimulus sets: (i) object recognition based solely on shape (Exp.  #@NEW_LINE#@#  1), (ii) correlations with perceived rather than physical shape dimensions (Exp.  #@NEW_LINE#@#  2), and (iii) sensitivity to non-accidental shape properties (Exp.  #@NEW_LINE#@#  3).  #@NEW_LINE#@#  Notably, these shape-based representations emerged without convnets being trained for shape processing or recognition and without any explicit knowledge of our stimuli.  #@NEW_LINE#@#  
Furthermore, we demonstrated that convnets also develop abstract, or superordinate, category representations, but to a much smaller extent than shape (Exp.  #@NEW_LINE#@#  4).  #@NEW_LINE#@#  In particular, we found that objects belonging to the same superordinate category (e.g., bananas and oranges belong to fruits) are represented more similarly than objects from different superordinate categories (e.g., bananas and violins).  #@NEW_LINE#@#  
These results expand the growing literature that convnets reflect human visual processing [10,11].  #@NEW_LINE#@#  More specifically, the correspondence between object representations in primates and convnets has been investigated in the context of object recognition.  #@NEW_LINE#@#  Our study adds the new information that the powerful object recognition performance of convnets is related to a human-like sensitivity for shape and, to a lesser extent, perceptually-adequate semantic spaces of the objects they learn.  #@NEW_LINE#@#  
We emphasize that the reported sensitivity to shape reflects the representational spaces learned in convnets, as opposed to the information that is in principle available in them.  #@NEW_LINE#@#  Both approaches are useful and valid for evaluating how good a particular model is in processing visual information, but they provide different kinds of information.  #@NEW_LINE#@#  Available information means that a linear classifier, such as a Support Vector Machine, can learn (with supervision) to correctly classify stimuli into predefined categories.  #@NEW_LINE#@#  Thus, if an above-chance classification can be performed from a models output, it means that a model is making this the information necessary to perform a task explicit (object manifolds become untangled, as described in [38]).  #@NEW_LINE#@#  For instance, object categorization based on their pixel values does not work but after a series of transformations in convnets, categorical information becomes available.  #@NEW_LINE#@#  A better model for a particular task is then the one that has task-relevant information explicitly available.  #@NEW_LINE#@#  
While this approach provides a valuable information about models behavior, it does not directly provide evidence if it is a good model of human visual perception.  #@NEW_LINE#@#  In other words, the fact that a model is categorizing objects very well does not imply that it represents these categories similarly to humans.  #@NEW_LINE#@#  An explicit evaluation how well a model explains human data needs to be performed.  #@NEW_LINE#@#  One approach is to use model outputs to predict neural data, as used in [6,10].  #@NEW_LINE#@#  However, in this case model outputs are combined in a supervised fashion, so this approach also resembles the available information approach in that we use external knowledge to investigate the amount of information available in the model outputs.  #@NEW_LINE#@#  
Another approach is to compare the representational spaces in models and humans, as proposed in [39] and used in this study.  #@NEW_LINE#@#  By computing the similarity of stimulus representations in models and humans we can understand if models implicitly develop representations that match human representations.  #@NEW_LINE#@#  Critically, we are not asking if a model contains necessary information that could be used for a particular task.  #@NEW_LINE#@#  Rather, we are asking if a model is already representing this information in a similar way independent of a task.  #@NEW_LINE#@#  To illustrate this difference, consider, for instance, stimuli in Exp.  #@NEW_LINE#@#  2a.  #@NEW_LINE#@#  Even a very simple shallow model, such as GaborJet, can learn to classify these nine stimuli into three perceptual categories (spikies, smoothies, and cubies) correctly because these dimensions are very simple and are easily accessible.  #@NEW_LINE#@#  Nonetheless, we found that representations in such simple models do not match human perception.  #@NEW_LINE#@#  Even though one could decode all the necessary information in stimuli from Exp.  #@NEW_LINE#@#  2a to perform classification into perceptual categories, this kind of information is not made dominant in shallow models.  #@NEW_LINE#@#  
Again, both approaches are valid and meaningful, but it is important to be explicit about the implications that they bring.  #@NEW_LINE#@#  We emphasize that, despite only being trained on a task that has a clear correct answer to it (i.e., object recognition), convnets also develop representations that reflect subjective judgments of human observers.  #@NEW_LINE#@#  In our opinion, this difference is critical to the success of deep nets.  #@NEW_LINE#@#  Observe that in the typical convnet training, a particular image can have only a cat or a dog, or a car, or something else, always with a correct answer for any given image, and deep nets have been shown to learn this correspondence very well.  #@NEW_LINE#@#  However, this is very different from what humans typically do.  #@NEW_LINE#@#  The human visual system is particularly adept at making stable judgments about the environment when there is no clear or single answer to a given problem.  #@NEW_LINE#@#  For example, most individuals would report seeing a house rather that a stack of bricks even though both answers are technically correct.  #@NEW_LINE#@#  As a recent blunder of Google Photos app that labelled black people as gorillas [40] illustrates, a machine that is incapable of arriving to perceptually congruent decisions might be unacceptable in social settings where such common sense is expected.  #@NEW_LINE#@#  Although more data would clearly help, it is hard if not impossible to have training data for every possible situation.  #@NEW_LINE#@#  Uncertainty is bound to occur in natural scenarios (e.g., due to the lack of training samples or poor visibility), so a more error-prone strategy is making sure that machines are learning perceptually relevant dimensions that will generalize properly to unfamiliar settings.  #@NEW_LINE#@#  Such machines can become better than humans, but critically when they are wrong, their mistakes will be human-like.  #@NEW_LINE#@#  We therefore expect that further advancement of computer vision and artificial intelligence at large are critically dependent not only on improvement in benchmark performance but also in matching human perceptual judgments.  #@NEW_LINE#@#  
Our data also provide insights how convnets relate to the information processing across the visual cortex.  #@NEW_LINE#@#  First, we observed that early layers of convnets tended to relate to the physical stimulus dimensions, consistent with the known properties and models of early visual cortex [41].  #@NEW_LINE#@#  Second, the output layer of convnets related to the perceived shape properties.  #@NEW_LINE#@#  Earlier human neuroimaging studies and monkey electrophysiological recordings revealed that these perceptual shape representations are implemented in human occipitotemporal cortex and in monkey inferotemporal cortex.  #@NEW_LINE#@#  Specifically, fMRI experiments with the stimulus set used in our Exp.  #@NEW_LINE#@#  2a have shown that the shape-selective lateral occipital area might be mostly involved in this shape processing [5].  #@NEW_LINE#@#  Moreover, the geon stimulus set used in Exp.  #@NEW_LINE#@#  3 also showed disproportionate sensitivity to non-accidental properties in monkey physiological recordings [24].  #@NEW_LINE#@#  Finally, the stimulus set used in Exp.  #@NEW_LINE#@#  4 showed a co-existence of shape and category information in human visual cortex and the dominance of categorical representations in the most anterior parts of it [37].  #@NEW_LINE#@#  Taken together, these results suggest that the shape representations in output layers of convnets relate to shape processing in higher visual areas in primates and their behavioral responses.  #@NEW_LINE#@#  
However, note that it is not necessarily the output layer which provides the best fit with shape representations in the primate brain.  #@NEW_LINE#@#  Given that the output layer is directly optimized to produce a correct category label rather than to represent shapes, it is possible that earlier layers are in fact better at capturing shape dimensions.  #@NEW_LINE#@#  Our results appear to be broadly consistent with this notion.  #@NEW_LINE#@#  However, these differences between layers seem to be small and the best intermediate layer is not consistent across experiments.  #@NEW_LINE#@#  Moreover, shape itself is a hierarchical concept that can be understood at multiple scales of analysis (e.g., local, global) and at multiple layers of abstraction [42], so it may not be possible to pinpoint an exact locus of shape representations neither in convnets nor in the human visual system.  #@NEW_LINE#@#  Rather, different dimensions of shape features might be distributed across multiple areas.  #@NEW_LINE#@#  
Causes_for_shape_sensitivity_in_convnets  #@NEW_LINE#@#  
Our results suggest that a human-like sensitivity to shape features is a quite common property shared by different convnets, at least of the type that we tested.  #@NEW_LINE#@#  However, the three convnets were also very similar, since all of them very trained on the same dataset and used the same training procedure.  #@NEW_LINE#@#  Which convnet properties are important in developing such shape sensitivity?  #@NEW_LINE#@#  
One critical piece of information is offer by the comparison to HMAX models.  #@NEW_LINE#@#  Despite a similar architecture, in most experiments we observed that overall HMAX models failed to capture shape sensitivity to the same extent as convnets.  #@NEW_LINE#@#  The most obvious difference lies in the depth of the architecture.  #@NEW_LINE#@#  There are at most four layers in HMAX models but at least eight layers in the simplest of our convnets, CaffeNet.  #@NEW_LINE#@#  However, HMAX99 (that has two layers) did not seem to perform consistently worse than HMAX-PNAS (that has four layers).  #@NEW_LINE#@#  Another important difference is the lack of supervision during training.  #@NEW_LINE#@#  As has been demonstrated before with object categorization [6], unsupervised training does not seem to be sufficiently robust, at least the way it is implemented in HMAX.  #@NEW_LINE#@#  
Another hint that supervision might be the critical component in learning universal shape dictionaries comes from comparing our results to the outputs obtained via the Hierarchical Modular Optimization (HMO) that was recently reported to correspond well to primate neural responses [10].  #@NEW_LINE#@#  For Exps.  #@NEW_LINE#@#  2a and 4, where we could obtain the outputs of the HMO layer that corresponds best to monkey neural data, we found largely similar pattern of results, despite differences in depth, training procedure, and training dataset.  #@NEW_LINE#@#  The only clear similarity between the tested convnets and HMO was supervised learning.  #@NEW_LINE#@#  
Finally, part of convnet power might also be attributed to the fully-connected layers.  #@NEW_LINE#@#  Both in CaffeNet and VGG-19, the critical preference for perceived shape emerges at the fully-connected layers.  #@NEW_LINE#@#  In GoogLeNet, the preference to perceptual dimensions is typically the strongest at the last layer that is also fully-connected, though earlier layers that are not fully-connected also exhibit a robust preference for perceived shape.  #@NEW_LINE#@#  
Other parameters, such as the naturalness of the training dataset or the task that convnet is optimized for, might also contribute to the representations that convnets develop.  #@NEW_LINE#@#  In short, the tests and the models that we have included in the present paper provide a general answer to our hypotheses about shape representations in convnets, but there are many specific questions about the role of individual variables that remain to be answered.  #@NEW_LINE#@#  

Relation_to_theories_of_shape_processing  #@NEW_LINE#@#  
In the literature, at least two theoretical approaches to shape processing have played an important role: image-based theories [19], which capitalize on processing image features without an explicit encoding of the relation between them, and structure-based theories [18], which emphasize the role of explicit structural relations in shape processing.  #@NEW_LINE#@#  Our results do not necessarily provide support for particular theories of shape processing.  #@NEW_LINE#@#  Of course, in their spirit convnets are closer to image-based theories since there is no explicit shape representation computed.  #@NEW_LINE#@#  On the other hand, in Exp.  #@NEW_LINE#@#  3 we also found that convnets were sensitive to non-accidental properties even without ever being trained to use these properties.  #@NEW_LINE#@#  While in principle HMAX architectures can also develop sensitivity to non-accidental properties when a temporal association rule is introduced [43], the fact that such sensitivity automatically emerges in convnets when training for object categorization provides indirect support that non-accidental properties are diagnostic in defining object categories, as proposed by the RBC theory [16].  #@NEW_LINE#@#  
Of course, a mere sensitivity to non-accidental properties does not imply that convnets must actually utilize the object recognition scheme proposed by the RBC theory [16].  #@NEW_LINE#@#  For instance, according to this theory, objects are subdivided into sets of shape primitives, known as geons, and recognized based on which geons compose that particular object, referred to as a structural description of the object.  #@NEW_LINE#@#  Finding an increased sensitivity for non-accidental properties does not necessarily imply that all these other assertions of the RBC theory are correct, and it does not by itself settle the controversy between image-based and structure-based models of object recognition.  #@NEW_LINE#@#  

Are_convnets_the_model_of_human_visual_processing?  #@NEW_LINE#@#  
While we demonstrate an unprecedented match between convnet representations and human shape perception, our experiments only capture a tiny fraction of the rich repertoire of human shape processing.  #@NEW_LINE#@#  It is clear from Exp.  #@NEW_LINE#@#  1 that despite a strong performance, convnets remain about 20% worse than human observers at object recognition on silhouettes.  #@NEW_LINE#@#  Given that convnets are already very deep and were trained exhaustively, it may be a sign that in order to bridge this gap, convnets need additional layers dedicated to developing more explicit structural representations.  #@NEW_LINE#@#  
Another, more fundamental limitation is their feedforward architecture.  #@NEW_LINE#@#  Whereas humans are thought to be able to perform many object and scene recognition tasks in a feedforward manner [4446], they are certainly not limited to feedforward processing and in many scenarios will benefit from recurrent processing [47].  #@NEW_LINE#@#  The role of such recurrent processes has been particularly debated in understanding perceptual organization, where the visual system is actively organizing the incoming information into larger entities [48].  #@NEW_LINE#@#  For instance, monkey neurophysiology revealed that figure-ground segmentation benefits both from feedforward and feedback processes [49], and many models of segmentation utilize recurrent loops (for an in-depth discussion, see [50]).  #@NEW_LINE#@#  In contrast, despite their superior object categorization abilities, vanilla convnets show rather poor object localization results, with the top-performing model (GoogLeNet) in the ImageNet Large Scale Visual Recognition Challenge 2014 scoring 93.3% on a single object categorization task, yet localizing that object with only 73.6% accuracy [51].  #@NEW_LINE#@#  
In other words, we showed that convnets sensitivity to shape that reflects human judgments once the object itself can be easily extracted from the image.  #@NEW_LINE#@#  However, as soon as segmentation and other perceptual organization processes become more complicated, humans but not convnets can benefit from recurrent connections.  #@NEW_LINE#@#  Thus, recurrent neural networks, which incorporate the feedforward complexity of the tested convnets, might provide an even better fit to human perception than purely feedforward convnets.  #@NEW_LINE#@#  
Finally, we have also argued in [50] that feedforward architectures such as convnets might be lacking critical mechanisms that could contribute to the initial image segmentation.  #@NEW_LINE#@#  In our view, high performance at object recognition and shape processing tasks should not be taken as evidence that the convolution-non-linearity-pooling stack at the heart of convnets is necessarily the right or the full solution of feedforward visual processing yet.  #@NEW_LINE#@#  Small modifications to this architecture, such as adding feature map correlations [52,53] or performing VLAD [54] or Fisher Vector [55] pooling already provides convnets with the ability to segment input images and represent textures and artistic style, all of which might be the part of feedforward computations in human visual cortex.  #@NEW_LINE#@#  
Taken together, we demonstrated that convolutional neural networks trained for multiclass object categorization implicitly learn representations of shape that reflect human shape perception.  #@NEW_LINE#@#  Moreover, we showed that convnets also develop abstract semantic spaces independent of shape representations that provide a good, albeit weaker, match to human categorical judgments.  #@NEW_LINE#@#  Overall, our results provide an important demonstration that convnets are not limited to only extracting objective information from the visual inputs (such as object category) but can also represent the subjective aspects of visual information in accordance to human judgments.  #@NEW_LINE#@#  In other words, our work suggests that convnets might be a good candidate model for understanding various perceptual qualities of visual information.  #@NEW_LINE#@#  


Methods  #@NEW_LINE#@#  
Ethics_statement  #@NEW_LINE#@#  
Studies reported here were approved by the Social and Societal Ethic Committee at KU Leuven (Exp.  #@NEW_LINE#@#  1 and 2b) and the Massachusetts Institute of Technologys Committee on the Use of Humans as Experimental Subjects (Exp.  #@NEW_LINE#@#  1).  #@NEW_LINE#@#  

Materials  #@NEW_LINE#@#  
Almost all simulations were run with Python using the psychopy-ext package [56] that provides several simple shallow models and bindings to the Caffe library [57] and to several popular computer vision models (PHOG, PHOW, HMAX-HMIN, and HMAX-PNAS), written in MATLAB/C.  #@NEW_LINE#@#  For online data collection, we used mturkutils, a Python interface package to Amazon Mechanical Turk.  #@NEW_LINE#@#  For data collection in in Exp.  #@NEW_LINE#@#  2b, we used similarity rating interface in MATLAB from [58].  #@NEW_LINE#@#  
For data analysis, we used several popular free and open source Python packages, including numpy, scipy, scikits-learn, scikits-image [59], pandas, seaborn, statsmodels, and NLTK [60].  #@NEW_LINE#@#  The code and stimulus sets for all of our simulations are available publicly at https://osf.io/jf42a, except in the cases when the stimulus set is already available online (links to these stimulus sets are provided in the repository and in the text) or subject to copyright restrictions (stimulus set for Exp.  #@NEW_LINE#@#  4).  #@NEW_LINE#@#  For a maximal reproducibility, all results reported in this manuscript can be generated with a single command: python run.py reportbootstrap.  #@NEW_LINE#@#  

Stimulus_sets  #@NEW_LINE#@#  
All stimuli were scaled to 256×256 px size.  #@NEW_LINE#@#  Convnets further downsampled these images to their own predefined image sizes (typically around 224×224 px).  #@NEW_LINE#@#  Stimuli pixel intensities were rescaled to the range between 0 and 1, where 0 corresponds to a black pixel and 1 corresponds to a white pixel, and, for deep models, the mean of the ImageNet training set was subtracted.  #@NEW_LINE#@#  No further processing was done.  #@NEW_LINE#@#  

Models  #@NEW_LINE#@#  
We used three groups of models: shallow, HMAX, and deep.  #@NEW_LINE#@#  Shallow models consist of a single layer of processing, and all features are built manually (i.e., there is no training).  #@NEW_LINE#@#  In contrast, HMAX and deep networks have a hierarchical feedforward architecture and have been trained for object categorization.  #@NEW_LINE#@#  However, HMAX models are not as deep (up to four layers) and have not been trained very optimally (either by manual feature selection or by imprinting stimulus selectivity), whereas deep nets acquire their features through training by backpropagation, which operates on all the weights in the network.  #@NEW_LINE#@#  

Correlational_analyses  #@NEW_LINE#@#  
We computed correlations between a particular layer of a model and behavioral data by first computing a dissimilarity matrix between stimuli and then correlating the resulting dissimilarity matrices.  #@NEW_LINE#@#  We also used a one minus a Pearson correlation as a distance between stimuli as a metric.  #@NEW_LINE#@#  Since these dissimilarity matrices are symmetric, we used only the upper triangle to compute correlations.  #@NEW_LINE#@#  Both correlations were computed using the following formula:  #@NEW_LINE#@#  

where x and y correspond to either the outputs of a given model to two different stimuli (for the dissimilarity matrix computation) or the values in the upper triangle of these dissimilarity matrices (for correlational analyses).  #@NEW_LINE#@#  We also conducted the analyses using a normalized Euclidean distance as a metric for producing dissimilarity matrices, but pattern of results remained the same, indicating that the choice of a metric has little effect on our findings.  #@NEW_LINE#@#  
The upper and lower bounds of the ceiling performance (shown as a gray band in figures) were estimated using the procedure described in [70].  #@NEW_LINE#@#  The upper bound was estimated by computing the average Pearson correlation between each participants dissimilarity matrix and the average across all participants after z-scoring data.  #@NEW_LINE#@#  The lower bound was estimated by computing the average Pearson correlation between each participants dissimilarity matrix and the average across the remaining participants after z-scoring data.  #@NEW_LINE#@#  

Consistency_analyses  #@NEW_LINE#@#  
In Exp.  #@NEW_LINE#@#  1, the consistency between human and model (or between two models) was computed as one minus a normalized squared Euclidean distance between the corresponding accuracy vectors x and y:  #@NEW_LINE#@#  

This metric is a version of the Matching distance that is used for estimating dissimilarity in binary data generalized to the non-logistic case.  #@NEW_LINE#@#  We chose this metric due to the largely (but not fully) logistic nature of our data (which was not the case in correlational analyses).  #@NEW_LINE#@#  In particular, this consistency measure is high when most of the responses match, whereas a correlation is very low as there is little variance in the data (i.e., mostly 1s).  #@NEW_LINE#@#  The same consistency measure was used for estimating upper and lower bounds of the ceiling performance based on human accuracy instead of the Pearson correlation as described in the correlational analyses.  #@NEW_LINE#@#  

Bootstrapping  #@NEW_LINE#@#  
In order to estimate the reliability of effects in our experiments, we used bootstrapping approach (number of iterations was always 1000).  #@NEW_LINE#@#  In Fig 1C and Fig 4, we computed 95% confidence intervals by resampling with replacement model responses (that is, the vector of 0s and 1s that indicates whether models response was correct or not), and computing the mean accuracy at each iteration.  #@NEW_LINE#@#  The 95% confidence interval was the reported as the 2.5% and 97.5% percentiles of the bootstrapped accuracy distribution  #@NEW_LINE#@#  
To estimate the reliability of correlations in the correlational analyses (all other figures), we computed bootstrapped confidence intervals as described in [6].  #@NEW_LINE#@#  In particular, dissimilarity matrices were resampled with replacement, and the resulting matrices where correlated for a total of 1000 iterations.  #@NEW_LINE#@#  Note that diagonal entries in the original dissimilarity matrix were undefined, so these entries were removed from the resampled matrices as well.  #@NEW_LINE#@#  Again, as done in [70], only the upper triangle was used for correlations.  #@NEW_LINE#@#  In Exp.  #@NEW_LINE#@#  2b and Exp.  #@NEW_LINE#@#  4, the bootstrapping procedure was carried out in a stratified manner, such that a resampling was done only within each class of stimuli, as done in [6].  #@NEW_LINE#@#  In particular, for shape, stimuli were resampled from the same shape class (e.g., a new sample for elongated vertical shapes was sampled from the nine available elongated vertical shapes only), whereas for semantical category, stimuli were resampled from the same category (e.g., a new sample for fruits category was sampled from the six available fruit images only).  #@NEW_LINE#@#  Resampling without stratification yielded qualitatively comparable results.  #@NEW_LINE#@#  In Exp.  #@NEW_LINE#@#  2a and Exp.  #@NEW_LINE#@#  3, no stratification was used because there were too few stimuli (Exp.  #@NEW_LINE#@#  2a) or no categories (Exp.  #@NEW_LINE#@#  3).  #@NEW_LINE#@#  Confidence intervals were computed as the 2.5% and 97.5% percentiles of the bootstrap distribution.  #@NEW_LINE#@#  
To estimate whether shallow, HMAX, and deep models differed, we used a bootstrapped paired-samples significance test (independent-samples significance test gave largely similar results).  #@NEW_LINE#@#  For each bootstrap resampling, model performance (correlation with the behavioral or pixelwise dissimilarity matrices) was averaged across models within a group, and the difference in average performance was computed for each pair of groups (3 pairwise comparisons in total).  #@NEW_LINE#@#  Across iterations, this yielded many such differences, which together formed the distribution used for statistical inference (one for each pair of groups).  #@NEW_LINE#@#  The percentile of scores below zero in each such distribution of differences was reported as the p-value.  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
We thank Dan Yamins for useful discussions and assistance in obtaining the HMO outputs, Ori Amir for his assistance in providing the behavioral data for non-accidental properties, and Pieter Moors for providing insights into computing statistics.  #@NEW_LINE#@#  We also thank Kstutis Kubilius and Vilnius University Institute of Mathematics and Informatics (Vilnius, Lithuania) for technical support.  #@NEW_LINE#@#  

Author_Contributions  #@NEW_LINE#@#  
Conceived and designed the experiments: JK HPOdB.  #@NEW_LINE#@#  Performed the experiments: JK.  #@NEW_LINE#@#  Analyzed the data: JK SB HPOdB.  #@NEW_LINE#@#  Contributed reagents/materials/analysis tools: JK SB HPOdB.  #@NEW_LINE#@#  Wrote the paper: JK SB HPOdB.  #@NEW_LINE#@#  

References  #@NEW_LINE#@#  



