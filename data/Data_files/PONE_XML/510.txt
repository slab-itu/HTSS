article id="http://dx.doi.org/10.1371/journal.pone.0172827"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
Tablet computer enhanced training improves internal medicine exam performance  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
Background  #@NEW_LINE#@#  
Traditional teaching concepts in medical education do not take full advantage of current information technology.  #@NEW_LINE#@#  We aimed to objectively determine the impact of Tablet PC enhanced training on learning experience and MKSAP® (medical knowledge self-assessment program) exam performance.  #@NEW_LINE#@#  

Methods  #@NEW_LINE#@#  
In this single center, prospective, controlled study final year medical students and medical residents doing an inpatient service rotation were alternatingly assigned to either the active test (Tablet PC with custom multimedia education software package) or traditional education (control) group, respectively.  #@NEW_LINE#@#  All completed an extensive questionnaire to collect their socio-demographic data, evaluate educational status, computer affinity and skills, problem solving, eLearning knowledge and self-rated medical knowledge.  #@NEW_LINE#@#  Both groups were MKSAP® tested at the beginning and the end of their rotation.  #@NEW_LINE#@#  The MKSAP® score at the final exam was the primary endpoint.  #@NEW_LINE#@#  

Results  #@NEW_LINE#@#  
Data of 55 (tablet n = 24, controls n = 31) male 36.4%, median age 28 years, 65.5% students, were evaluable.  #@NEW_LINE#@#  The mean MKSAP® score improved in the tablet PC (score  + 8 SD: 11), but not the control group (score - 7, SD: 11), respectively.  #@NEW_LINE#@#  After adjustment for baseline score and confounders the Tablet PC group showed on average 11% better MKSAP® test results compared to the control group (pless_than0.001).  #@NEW_LINE#@#  The most commonly used resources for medical problem solving were journal articles looked up on PubMed or Google®, and books.  #@NEW_LINE#@#  

Conclusions  #@NEW_LINE#@#  
Our study provides evidence, that tablet computer based integrated training and clinical practice enhances medical education and exam performance.  #@NEW_LINE#@#  Larger, multicenter trials are required to independently validate our data.  #@NEW_LINE#@#  Residency and fellowship directors are encouraged to consider adding portable computer devices, multimedia content and introduce blended learning to their respective training programs.  #@NEW_LINE#@#  


Introduction  #@NEW_LINE#@#  
Traditional teaching concepts in medical education do not take full advantage of information technology, despite the fact that modern clinical medicine and biomedical science are packed with digital media resources reaching from multidimensional virtual imaging data of the human body to complex video animations of human physiology.  #@NEW_LINE#@#  
Medical education ideally happens at the bedside[1], not in lecture halls.  #@NEW_LINE#@#  Although the use of wireless enabled mobile communication devices[2] including (tablet) computers, personal digital assistants[2] and smartphones[3]that can help incorporate, process and deliver the ever increasing rich media and information content at the point of care in real timeis substantially increasing[46], scientific data on their efficacy in medical education and clinical training is limited.  #@NEW_LINE#@#  
Here we present prospective data demonstrating that Tablet PC enabled eLearning significantly impacts on exam performance and prospect for future medical trainees.  #@NEW_LINE#@#  

Methods  #@NEW_LINE#@#  
This single center, prospective, controlled study was conducted on an internal medicine ward at Charité Medical Centers Virchow Hospital, Medical School of the Humboldt-University of Berlin.  #@NEW_LINE#@#  For the purpose of the study the ward was equipped with three wireless access points (Enterasys, Salem, NH, USA) linking it to the hospitals intra- and global internet as well as a Net Education Center (Hewlett Packard, Palo Alto, CA, USA), a cart housing and charging Tablet PCs.  #@NEW_LINE#@#  
Active_participants_and_controls  #@NEW_LINE#@#  
Participation was voluntary and in accordance with both institutional policies and all applicable laws including data privacy legislation.  #@NEW_LINE#@#  The institutional review board of Charité University Hospital in Berlin confirmed the information provided to participants was in line with the local ethical requirements (No.  #@NEW_LINE#@#  : EA1/386/16).  #@NEW_LINE#@#  Eligible participants included consenting medical students in their final year of medical school (acting interns) and postgraduate year 1 to 3 residents doing a rotation on the selected internal medicine ward as a mandatory part of their training curriculum.  #@NEW_LINE#@#  All participants signed a contract consenting to and detailing the conditions of the study.  #@NEW_LINE#@#  Timing and duration of their rotation were predetermined by medical school, hospital and physician board rules and regulations.  #@NEW_LINE#@#  Final year medical students (acting interns) interns did four month rotations, while residents did 6 month rotations.  #@NEW_LINE#@#  The consecutive cohort of all participants was alternatingly assigned to either the active test (tablet) or traditional education (control) group, respectively.  #@NEW_LINE#@#  
The active test group was profiled and examined (see below), received a Tablet PC to keep for the entire duration of their rotation and use the multimedia training and education package (see below) in- and outside the medical center campus (i.e.  #@NEW_LINE#@#  at home and commuting to work).  #@NEW_LINE#@#  
The control group did not receive a Tablet PC and was only profiled and examined (see below) and had access to all conventional education and training resources (i.e.  #@NEW_LINE#@#  library, books, journals) on campus.  #@NEW_LINE#@#  

Objectives_and_outcomes  #@NEW_LINE#@#  
The primary objective was to test the hypothesis that Tablet PC enhanced education significantly impacts on participants performance in medical board exams.  #@NEW_LINE#@#  The final MKSAP® exam score was the primary endpoint.  #@NEW_LINE#@#  Moreover, we aimed to identify participants characteristics impacting on the final exam score.  #@NEW_LINE#@#  

Participant_profiling  #@NEW_LINE#@#  
Both control and tablet group participants had to complete an extensive questionnaire to collect their demographic data and evaluate their educational status, computer affinity and skills, problem solving strategy, eLearning knowledge and judge their self-estimated medical knowledge to assess potential confounding factors on the overall outcome, respectively.  #@NEW_LINE#@#  

Tablet_computers  #@NEW_LINE#@#  
The HP Compaq tc4200[7] (Hewlett Packard, Palo Alto, CA, USA) and IBM ThinkPad X41[8] (IBM, Armonk, NY, USA) are ultraportable notebooks that also convert into tablets.  #@NEW_LINE#@#  They incorporate technology to provide wireless connectivity and improved battery performance.  #@NEW_LINE#@#  Tablet PCs are fully functional personal computers delivering performance and compatibility in an innovative form factor.  #@NEW_LINE#@#  They offer wide-viewing angle displays on protective glass featuring a digital eraser pen that writes like an actual pen.  #@NEW_LINE#@#  

Custom_software_package_and_programming  #@NEW_LINE#@#  
We developed a custom, mostly open source (Open Source Initiative, East Palo Alto, CA, USA) software package and named it Mobile Medical Educator (MME).  #@NEW_LINE#@#  A local Apache (Apache Software Foundation, Delaware, USA) server connected a MySQL database (Oracle, San Francisco, CA, US) with local media content, applications and a graphical user interface (GUI).  #@NEW_LINE#@#  The GUI was programmed using Java (Oracle, San Francisco, CA, US), CSS and HTML to provide kiosk mode web browser (Firefox, Mozilla Foundation, Mountain View, CA, USA) access for participants to interact with the Tablet PCs.  #@NEW_LINE#@#  
Through this central interface all participants could register, complete their profile questionnaire, take the initial and final knowledge assessment exams, access a variety of multimedia training and education resources as well as the medical centers electronic patient care systems.  #@NEW_LINE#@#  
The American College of Physician (Philadelphia, PA, USA) kindly provided us with a special electronic version (in XML format) of their MKSAP® 14 software that allowed integration into our database system and parsing with a random generator.  #@NEW_LINE#@#  
The multimedia package included access to the institutional collaborative online course management systems (Moodle and Blackboard), eBooks (Springer Nature Science and Business Media, New York, NY, USA), eJournals, educational slide kits, podcasts, videos, animations, images from major biomedical and scientific publishers or professional societies as well as twitter feeds and selected hyperlinks to biomedical and scientific web resources.  #@NEW_LINE#@#  

Initial_and_final_knowledge_assessment  #@NEW_LINE#@#  
To determine the impact of Tablet PC based education we decided to objectively assess and compare all participants knowledge in internal medicine at two time points.  #@NEW_LINE#@#  Importantly, none of the participants had access to or were able to practice the exam questions used in this study or underwent any kind of special knowledge exam preparation.  #@NEW_LINE#@#  
New medical knowledge recognition[9] and concept identification[10] can be computationally evaluated with the American College of Physicians (ACP) Medical Knowledge Self-Assessment Program MKSAP® first introduced in the 1970s[11, 12].  #@NEW_LINE#@#  MKSAP®[11] closely resembles the official American Board of Internal Medicine (ABIM) multiple choice question format and style and has been successfully used to evaluate knowledge and analyze currency of ABIM® diplomats[13].  #@NEW_LINE#@#  Predictive validity for the ABIM® exam has been demonstrated in the past[14].  #@NEW_LINE#@#  The internal medicine exam performance of both, the control and the tablet group was tested by administration of 215 out of 1400 random generator selected, equally distributed questions from all eleven MKSAP® categories (Foundations of Internal Medicine, Cardiovascular Medicine, Gastroenterology and Hepatology, Rheumatology, Neurology, Hematology and Oncology, Infectious Diseases, Pulmonary and Critical Care Medicine, General Internal Medicine, Endocrinology and Metabolism) parsed from the current ACPs MKSAP® digital edition pool.  #@NEW_LINE#@#  
Our rationale for using MKSAP® was its proven track record in evaluating internal medicine knowledge.  #@NEW_LINE#@#  Although primarily designed for resident use, we felt that final year medical students, i.e.  #@NEW_LINE#@#  acting first year medical residents could be reliably subjected to it as well.  #@NEW_LINE#@#  In our opinion the benefit of using a vetted, validated questionnaire such as MKSAP® would outweigh its potential limitations and was preferable to designing a brand new knowledge assessment tool.  #@NEW_LINE#@#  

Data_processing_and_statistical_analysis  #@NEW_LINE#@#  
All statistical analyses were performed with SPSS 22 (IBM, Armonk, NY, USA) software.  #@NEW_LINE#@#  For descriptive statistics, means and standard deviations, medians and inter quartile ranges (IQR) or absolute and relative frequencies were reported where applicable.  #@NEW_LINE#@#  Data are expressed in box plots.  #@NEW_LINE#@#  Both, the Mann-Whitney-U-test[15] and Fishers-exact-test[16] were used to compare participants profile data.  #@NEW_LINE#@#  The t-test for independent samples or one-way ANOVA was used to test associations of participants characteristics with their final score.  #@NEW_LINE#@#  All variables with a p-value less_than 0.1 were also tested in a multiple regression model for the final score values.  #@NEW_LINE#@#  For the multiple regression model self-rated knowledge was dichotomized into excellent/good vs. passable/adequate.  #@NEW_LINE#@#  Additionally t-tests for related samples were employed to check for significant differences between the mean initial and final exam scores.  #@NEW_LINE#@#  A two-sided significance level of 0.05 was used.  #@NEW_LINE#@#  The main hypothesis was the existence of group differences in final scores after accounting for baseline scores and possible confounders.  #@NEW_LINE#@#  All other tests were secondary.  #@NEW_LINE#@#  No adjustment for multiple testing was applied.  #@NEW_LINE#@#  


Results  #@NEW_LINE#@#  
Participant_flow_and_recruitment  #@NEW_LINE#@#  
We recruited 80 participants for this study between 2008 and 2012.  #@NEW_LINE#@#  Data of 55 participants (tablet n = 24, 50% male; controls, n = 31, 25.8% male; median age 28 years) were evaluable and analyzed.  #@NEW_LINE#@#  The remaining participants data was incomplete and was excluded from the analysis.  #@NEW_LINE#@#  Fig 1  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0172827.g001  #@NEW_LINE#@#  

Participant_profiles  #@NEW_LINE#@#  
Socio-demographics  #@NEW_LINE#@#  
Most participants were German nationals.  #@NEW_LINE#@#  There were no statistically significant differences in age, gender or educational background.  #@NEW_LINE#@#  Table 1  #@NEW_LINE#@#  

Exposure_to_US_or_other_foreign_medical_education  #@NEW_LINE#@#  
A fifth of participants had received medical education in foreign countries such as Argentine, Chile, France, Iceland, Italy, Malawi, Russia, Spain, Sweden, Switzerland, The United Kingdom and The United States.  #@NEW_LINE#@#  However, while many participants were familiar with the term US medical licensing exam (USMLE®), only three participants had actually received medical training in the US.  #@NEW_LINE#@#  None had ever taken the exam.  #@NEW_LINE#@#  Table 1  #@NEW_LINE#@#  

Computer_affinity_and_skills  #@NEW_LINE#@#  
Most participants owned at least one computer which was a notebook or laptop in half of the cases.  #@NEW_LINE#@#  However, they mostly used it at home or work and only in less in quarter of cases in other campus locations.  #@NEW_LINE#@#  Table 1  #@NEW_LINE#@#  

Currently_exposure_to_eLearning_and_preferred_problem_medical_solving_resources  #@NEW_LINE#@#  
Participants exposure to eLearning prior to this study was very limited with one year of experience on average.  #@NEW_LINE#@#  Their favorite source for medical problem solving were still articles that they preferably looked up on PubMed or Google, and books.  #@NEW_LINE#@#  Table 1  #@NEW_LINE#@#  

Self-rated_internal_medicine_knowledge  #@NEW_LINE#@#  
The majority of participants in both the control and Table PC groups rated their internal medicine knowledge as passable or good at entry into the study.  #@NEW_LINE#@#  Only one participant (control group) rated its knowledge as excellent.  #@NEW_LINE#@#  Table 1  #@NEW_LINE#@#  


Outcomes_and_estimation  #@NEW_LINE#@#  
Improved_exam_performance_in_the_tablet_group  #@NEW_LINE#@#  
The final mean MKSAP® score was higher in the tablet group (mean (SD): 59 (19)) compared to the control group (mean (SD): 48 (10)) (pless_than0.001) Table 2, Fig 2.  #@NEW_LINE#@#  

Characteristics_associated_with_improved_exam_performance  #@NEW_LINE#@#  
At bivariate level baseline of all variables tested only tablet pc use and self-rated excellent internal medicine knowledge at baseline had a significant impact of the final exam score.  #@NEW_LINE#@#  Table 2 After adjustment for baseline score, tablet pc knowledge and self-rated excellent internal medicine knowledge the tablet group showed on average 11% higher MKSAP test results compared to the control group (pless_than0.001, main hypothesis) Table 3  #@NEW_LINE#@#  



Discussion  #@NEW_LINE#@#  
We demonstrate for the first time that in a prospective cohort of final year medical students and residents doing an internal medicine inpatient service rotation at an academic medical center the use of a wireless tablet computer based integrated education and portable hospital workstation significantly improves board style exam (MKSAP®) performance.  #@NEW_LINE#@#  This was true even after adjustment for baseline score, Tablet PC knowledge and self-rated excellent internal medicine knowledge.  #@NEW_LINE#@#  
Unsurprisingly, the overall absolute MKSAP® scores at both time points and in both the control and the tablet group were lower compared with the US national average[17].  #@NEW_LINE#@#  This is likely owed to the fact that none of the participants in our study were native English speakers and their exposure to US medical education was very limited.  #@NEW_LINE#@#  Furthermore, unlike US medical students, residents and foreign (international) medical graduates in the US none had ever taken MKSAP® or ABIM® exams before or participated in regular in-house exams with comparable questions very commonly administered in the US.  #@NEW_LINE#@#  Moreover, none of the participants practiced MKSAP®, USMLE® or ABIM® style exams before or during this study either.  #@NEW_LINE#@#  
Being naïve regarding this exam type and in relation to prior US medical education can also be considered a strength of our study.  #@NEW_LINE#@#  Achieving a maximum score was not the goal here, but rather to investigate if the educational tablet system would improve exam performance, i.e.  #@NEW_LINE#@#  has a significant impact on internal medicine knowledge, which was shown in our results.  #@NEW_LINE#@#  
Interestingly, the scores significantly worsened in the control group.  #@NEW_LINE#@#  Perhaps their motivation was lower due to the lack of the incentive of an otherwise desired technical device, which may have been an additional stimulus beyond the actual education software in the tablet computer group.  #@NEW_LINE#@#  While commonly employed and well proven according to some[18, 19], measurements and metrics may actually also deteriorate individual physician performance[20, 21].  #@NEW_LINE#@#  Our study design was unable to detect any such an effect.  #@NEW_LINE#@#  The difference between Tablet PC and control group could however not be attributed to other socio-demographic factors or computer affinity surrogates either.  #@NEW_LINE#@#  
Our study furthermore demonstrates that the improved exam performance was significantly associated with the self-rated internal medicine baseline knowledge of participants.  #@NEW_LINE#@#  This appears plausible as a technical education tool can obviously not replace prior factual medical knowledge acquisition nor supersede basic pedagogic principles.  #@NEW_LINE#@#  
Our work has limitations.  #@NEW_LINE#@#  The number of evaluable cases was small and thus the data of this pilot study needs to be validated in larger series before conclusions can be generalized.  #@NEW_LINE#@#  We also experienced the problem of participant attrition, well known from major educational research studies[22].  #@NEW_LINE#@#  The high drop-out rate may relate to the voluntary nature of study participation and perhaps conceiving the extra exams or device as a burden.  #@NEW_LINE#@#  At the same time the Hawthorne effect [23] also known as observer[24] effect, i.e.  #@NEW_LINE#@#  the reactivity in which study participants modify or improve aspects of their behavior (exam performance) in response to their awareness of being observed.  #@NEW_LINE#@#  We have not controlled our analysis for this effect and sample size was likely to small to address this issue.  #@NEW_LINE#@#  Moreover, only a computer defined random selection of 215 out of all 1400 MKSAP® questions was administered per exam.  #@NEW_LINE#@#  To avoid skewing of the selection their category distribution was maintained.  #@NEW_LINE#@#  Still, the MKSAP® edition we used was designed for residents and could have potentially overwhelmed some of the participating final year medical students.  #@NEW_LINE#@#  
The impact of computer enhanced education on board style exams has been studied before.  #@NEW_LINE#@#  One group compared scores on preceptor evaluations with National Board of Medical Examiners (NBME) Subject Exam, and a standardized patient (SP)-based exam to complete assigned web cases versus students not completing the assignment.  #@NEW_LINE#@#  The authors controlled for prior academic performance and clerkship timing using US Medical Licensing Exam (USMLE) Step 1 scores and rotation order.  #@NEW_LINE#@#  They reported that students completing the web case assignment scored higher on the NBME subject exam and the SP-based exam [25].  #@NEW_LINE#@#  Another study examined the impact of a computer-based program where residents receive a score on a Likert-type scale from an attending for each precept based on their knowledge base.  #@NEW_LINE#@#  The authors found a significant correlation between the residents Likert scale scores and their American Board of Family Medicine In-Training Exam scores[26].  #@NEW_LINE#@#  Judging from a study in emergency medicine it appears that positive impact of computers is probably independent of the exam style (computerized vs. oral).  #@NEW_LINE#@#  The authors observed no differences between virtual and traditional groups on critical action scores or scores on eight competency categories[27].  #@NEW_LINE#@#  
The use of multimedia materials was also studied in dental students, who often have difficulty understanding the importance of basic science classes, such as physiology, for their future careers.  #@NEW_LINE#@#  The authors reported a significant improvement in unit exam scores[28]  #@NEW_LINE#@#  
Exam performance without a practical clinical skill level assessment does not automatically translate into superior performance in residency or fellowship programs[29].  #@NEW_LINE#@#  The utility of educational games (although our system was not programmed as a game) as a teaching strategy for healthcare professionals remains undetermined according to a recent Cochrane analysis[30].  #@NEW_LINE#@#  Moreover, different types of physicians have different needs and preferences for evidence-based resources and handheld devices [31].  #@NEW_LINE#@#  Another aspect we could not address in our study was demonstrating the link to improved patient outcomes[3234].  #@NEW_LINE#@#  
In summary, our study provides evidence, that tablet computer based integrated training and clinical practice enhances medical education and exam performance.  #@NEW_LINE#@#  Larger, multicenter trials are required to independently validate our data.  #@NEW_LINE#@#  Residency and fellowship directors are encouraged to consider adding computer devices, multimedia content and introduce blended learning to their respective training programs.  #@NEW_LINE#@#  

Supporting_information  #@NEW_LINE#@#  
S1_File_Raw_data  #@NEW_LINE#@#  
This file contains the study raw data, except for any potentially personally identifying information to meet German Federal privacy legislation requirements.  #@NEW_LINE#@#  
https://doi.org/10.1371/journal.pone.0172827.s001  #@NEW_LINE#@#  
(SAV)  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
D.C.B.  #@NEW_LINE#@#  is a fellow of the Berlin Institute of Health (BIH), supported by StiftungCharité.  #@NEW_LINE#@#  

Author_Contributions  #@NEW_LINE#@#  


Conceived and designed the experiments: DCB IW.  #@NEW_LINE#@#  
Performed the experiments: DCB IW.  #@NEW_LINE#@#  
Analyzed the data: IW UG DCB.  #@NEW_LINE#@#  
Contributed reagents/materials/analysis tools: IW UG.  #@NEW_LINE#@#  
Wrote the paper: DCB IW UG.  #@NEW_LINE#@#  



References  #@NEW_LINE#@#  


