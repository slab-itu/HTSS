article id="http://dx.doi.org/10.1371/journal.pone.0141965"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
A Bayesian Developmental Approach to Robotic Goal-Based Imitation Learning  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
A fundamental challenge in robotics today is building robots that can learn new skills by observing humans and imitating human actions.  #@NEW_LINE#@#  We propose a new Bayesian approach to robotic learning by imitation inspired by the developmental hypothesis that children use self-experience to bootstrap the process of intention recognition and goal-based imitation.  #@NEW_LINE#@#  Our approach allows an autonomous agent to: (i) learn probabilistic models of actions through self-discovery and experience, (ii) utilize these learned models for inferring the goals of human actions, and (iii) perform goal-based imitation for robotic learning and human-robot collaboration.  #@NEW_LINE#@#  Such an approach allows a robot to leverage its increasing repertoire of learned behaviors to interpret increasingly complex human actions and use the inferred goals for imitation, even when the robot has very different actuators from humans.  #@NEW_LINE#@#  We demonstrate our approach using two different scenarios: (i) a simulated robot that learns human-like gaze following behavior, and (ii) a robot that learns to imitate human actions in a tabletop organization task.  #@NEW_LINE#@#  In both cases, the agent learns a probabilistic model of its own actions, and uses this model for goal inference and goal-based imitation.  #@NEW_LINE#@#  We also show that the robotic agent can use its probabilistic model to seek human assistance when it recognizes that its inferred actions are too uncertain, risky, or impossible to perform, thereby opening the door to human-robot collaboration.  #@NEW_LINE#@#  

Introduction  #@NEW_LINE#@#  
Considerable progress has been made in robotics in recent years, particularly in the area of human-robot interaction.  #@NEW_LINE#@#  Techniques have been proposed to impart new skills to robots via programming by demonstration [1] and imitation learning [2].  #@NEW_LINE#@#  An important remaining challenge is endowing a robot with the ability to infer the intentions of humans and to learn new skills not by naively following demonstrated action trajectories (trajectory-based or action imitation) but through goal-based imitation [3, 4].  #@NEW_LINE#@#  
Developmental scientists have shown that infants can infer the goal of an adults actions.  #@NEW_LINE#@#  In one set of experiments with 18-month old infants [5], an adult actor demonstrated an act in which the goal-state was not achieved; infants were able to read through the literal body movements to infer the underlying goal and execute the intended act.  #@NEW_LINE#@#  In other experiments, children employed different means to achieve the same goal as adults.  #@NEW_LINE#@#  When given a barbell-shaped object, adults used their hands to pull apart the object, but children who could not grasp the end of an oversized barbell used alternative means (e.g., holding one end in both hands and clasping the other end between their knees) to pull it apart.  #@NEW_LINE#@#  The children thus not only inferred the adults goal but could also use novel alternative means to achieve the goal.  #@NEW_LINE#@#  
Clues to how children acquire the ability to infer goals have come from studies on gaze following, the ability of humans to follow the line of regard of another human.  #@NEW_LINE#@#  A naive interpretation of gaze following is that it is simply the imitation of head movement.  #@NEW_LINE#@#  However, Meltzoff and Brooks [6] controlled for head movements and showed that infants tendency to follow gaze varied as a function of whether or not the actors view was blocked by an intervening opaque occluder (a blindfold).  #@NEW_LINE#@#  In particular, 18-month-old children did not follow the gaze of an adult who made a head movement toward an object while wearing a blindfold.  #@NEW_LINE#@#  Younger children (12-month-olds) did mistakenly follow the gaze of a blindfolded adult.  #@NEW_LINE#@#  However, after these younger children were given the self-experience of a blindfold blocking their own view, they no longer followed the gaze of the blindfolded adult.  #@NEW_LINE#@#  
These results highlight the importance of self-experience in the development of goal inference capabilities and goal-based imitation.  #@NEW_LINE#@#  They are consistent with Meltzoffs Like-Me hypothesis [7] which states that children utilize internal models learned through self-experience to interpret the acts and intentions of others, and with increasing experience, acquire increasingly sophisticated intent recognition abilities.  #@NEW_LINE#@#  Such an approach is different from but incorporates elements of two previous well-known theories of intent inference: (i) simulation theory [8], which proposes that the same mental resources we use for thinking, decision-making, and emotional responses are redeployed in imagination to provide an understanding of others, and (ii) theory theory [9, 10], which advocates that children develop theories about the world and about others, make predictive inferences about behaviors and inner states of others using this network of beliefs (theories), and revise theories according to new observations.  #@NEW_LINE#@#  The approach taken here differs by providing a rigorous mathematical framework based on Bayesian inference of goals and actions, and the bootstrapping of Bayesian models through learning from self-experience.  #@NEW_LINE#@#  

Methods  #@NEW_LINE#@#  
Our hypothesis is that humans use a goal-directed mechanism for planning motor acts.  #@NEW_LINE#@#  A goal, provided by either an internal desire or external stimulus, together with the current state, determines what the action should bethis is called a policy.  #@NEW_LINE#@#  An action executed in a given state in turn determines the next state (probabilistically).  #@NEW_LINE#@#  We represent these dependencies using the graphical model shown in Fig 1a where G is the goal, A is the action, Xi is the current (or initial) state, and Xf is the next (or final) state.  #@NEW_LINE#@#  Throughout this paper, we use upper case letters to denote random variables and lower case letters to denote specific values of the variables.  #@NEW_LINE#@#  
(a) The graphical model shows the probabilistic dependencies between different random variables in the model: G = goal, A = action, Xi = current state, and Xf = final state.  #@NEW_LINE#@#  The model captures how actions depend on goals and states, and how the state changes as a consequence of executing an action; (b) incorporates the influence of blindfold self-experience on the model using the random variable B; (c) shows the combined graphical models, one for the agent and a copy for the mentor (superscript m), for following the gaze of a mentor.  #@NEW_LINE#@#  Shaded variables denote observed variables.  #@NEW_LINE#@#  The darker shading indicates that B is an observed discrete variable, while the rest of the nodes are continuous.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0141965.g001  #@NEW_LINE#@#  
Consider our two example scenarios.  #@NEW_LINE#@#  In the context of gaze following, the goal is a desired fixation location, the action is a vector of motor commands to move the head, and the state represents head position and orientation.  #@NEW_LINE#@#  We discuss the model assuming movement of the head but the model can be generalized to embody movement of the eyes or movement of both the head and eyes to achieve a spatial target.  #@NEW_LINE#@#  In this case, the random variables (states, actions, goals) are continuous-valued and we define the relationships between them using (learned) probabilistic functions.  #@NEW_LINE#@#  In the case of our example of actions on objects, we assume the states represent discrete locations on a table, actions comprise of high-level commands such as pick and push, and the goal is a desired (discrete) location on the table.  #@NEW_LINE#@#  We use this example to illustrate the discrete formulation of the model in which the relationships between the discrete-valued random variables are captured by (learned) probability tables.  #@NEW_LINE#@#  
Computational_Model  #@NEW_LINE#@#  
Case_I__Continuous-Valued_Random_Variables  #@NEW_LINE#@#  
We begin by showing how an agent can learn the mapping between continuous-valued goals, states, and actions which can later be used for planning and intent inference.  #@NEW_LINE#@#  The agent first learns a transition model f, (e.g., through self-exploration using its own body movements, which [11] dubbed body babbling).  #@NEW_LINE#@#  This translates an initial state, xi, and an action, a, to a final state, xf:  #@NEW_LINE#@#  

where (, ) signifies the normal distribution with mean  and covariance matrix .  #@NEW_LINE#@#  
To make the exposition concrete, we focus here on the specific case of modeling gaze following, although the framework is general enough to apply to other types of behaviors.  #@NEW_LINE#@#  In gaze following, the initial and final state correspond to head poses while the action corresponds to motor commands to move the head.  #@NEW_LINE#@#  The learned transition model can in turn be used to learn a policy function, , which maps an initial state and a goal location g to an appropriate action:  #@NEW_LINE#@#  

This equation essentially determines the rotation required to turn the head from its current position to face the goal.  #@NEW_LINE#@#  
We use two separate Gaussian processes (GPs) [12], GPf and GP, to learn the two nonlinear functions f and  in the equations above.  #@NEW_LINE#@#  GPs are commonly used in machine learning to infer a function h from noisy observations yi:  #@NEW_LINE#@#  

GPs are both flexible and robust.  #@NEW_LINE#@#  First, they are nonparametric so they do not limit the set of functions that h can be chosen from.  #@NEW_LINE#@#  Second, they estimate a probability distribution over functions instead of choosing a single most-likely function.  #@NEW_LINE#@#  This allows all plausible functions to be incorporated into the estimation process and reduces model bias.  #@NEW_LINE#@#  For these reasons, GPs are effective with small numbers of training samples, increasing their biological plausibility.  #@NEW_LINE#@#  We now show how these GPs can be used for planning, goal inference, and gaze-following.  #@NEW_LINE#@#  

Goal-directed_Planning  #@NEW_LINE#@#  
GPs are trained via supervised learning: given a training dataset with noisy labels, the GP learns the functional mapping between the input data and the labels (output).  #@NEW_LINE#@#  The training data could be obtained, for instance, through a reinforcement-based paradigm that combines exploration of the goal-action-state space for training the transition GP with selection of data from successful trials for training the policy GP (see [3]).  #@NEW_LINE#@#  
After training, it is simple for the agent to fixate on a goal location.  #@NEW_LINE#@#  We assume that the agent knows its current state xi and the goal g. Given these, the agent uses its learned distribution over functions, GP, to compute a probability distribution over actions, P(a) = (a, a).  #@NEW_LINE#@#  This distribution can then be passed through GPf to estimate the probability of the resulting state P(xf)  (xf, xf).  #@NEW_LINE#@#  Thus, our model provides both a prediction of the final position and an estimate of the uncertainty in the prediction.  #@NEW_LINE#@#  We define the combined inference process P(XfXi = xi, G = g), as Forward Inference.  #@NEW_LINE#@#  

Goal_Inference  #@NEW_LINE#@#  
One can also infer the goal of an observed action (e.g., head movement), given starting and ending states (e.g., head poses), xi and xf, respectively.  #@NEW_LINE#@#  To accomplish this, the agent must be able to recover the inputs to each GP, given the outputs.  #@NEW_LINE#@#  Fortunately, results from [13] allow us to estimate a distribution over the inputs given the outputs.  #@NEW_LINE#@#  We follow the technique in [13] to infer a distribution over actions given xi and xf and then use this to estimate a distribution over goals.  #@NEW_LINE#@#  We define the inference process P(GXi = xi, Xf = xf) as Reverse Inference.  #@NEW_LINE#@#  

Gaze_Following  #@NEW_LINE#@#  
Gaze following of a mentor using the above model is accomplished by invoking Meltzoffs Like-Me hypothesis.  #@NEW_LINE#@#  The agent learns a model of its own head movements and assumes that a mentor uses this same (or similar) model.  #@NEW_LINE#@#  As suggested by the graphical model in Fig 1c, the agent observes the starting and ending states (head poses) of a mentor and then infers the goal location indicated by the mentor, gm, by using a copy of its own learned model and inferring what it would be looking at if it were in the mentors position.  #@NEW_LINE#@#  After inferring the mentors goal, the agent transforms that goal into its own coordinate frame and then infers how to fixate that goal.  #@NEW_LINE#@#  For this paper, we assume the agent has acquired the ability to transform between coordinate frames through prior experience.  #@NEW_LINE#@#  We acknowledge that the problem of transformation between the agent and mentor coordinate frames can be challenging in more complex tasks, and we refer the reader to relevant work in this area for further information [1416].  #@NEW_LINE#@#  

Modeling_Blindfold_Experiments_in_Human_Infants  #@NEW_LINE#@#  
We first tested the continuous-valued version of the model on a gaze-following task previously used in experiments in human infants [6].  #@NEW_LINE#@#  One set of experiments showed that 14- and 18-month olds do not follow the gaze of an adult who is wearing a blindfold, although they follow gaze if the adult wears the same band as a headband.  #@NEW_LINE#@#  This suggests that these children did not follow gaze because they are able to take into account the consequences of wearing a blindfold (i.e., occlusion) and unlike the 12-month olds, make the inference that the adult is not looking at an object.  #@NEW_LINE#@#  This observation is closely related to Meltzoffs Like me hypothesis [7].  #@NEW_LINE#@#  In particular, self-experience with own eye closure and occluders may influence gaze-following behavior.  #@NEW_LINE#@#  To test this hypothesis, Meltzoff and Brooks provided one group of 12-month olds with self-experience with an opaque blindfold while two other groups either had no self-experience or had self-experience with a windowed blindfold.  #@NEW_LINE#@#  On seeing an adult with a blindfold turn towards an object, most of the children with self-experience with blindfolds did not turn to the object while the other two groups did [6].  #@NEW_LINE#@#  This highlights self-experience as a learning mechanism that can used to interpret the behaviors of other agents.  #@NEW_LINE#@#  
To model these results, we incorporate a binary random variable B  {0,1} in the graphical model, as shown Fig 1b, which denotes whether or not a blindfold is being worn, and allows the agent to learn the effects of being blindfolded.  #@NEW_LINE#@#  Our model learns a new Gaussian process ( in place of GP) which is used when the agent is blindfolded.  #@NEW_LINE#@#  When the opaque blindfold is in place, regardless of the current value of the goal state, no action leads to that goal location being fixated.  #@NEW_LINE#@#  Goals in this case are not causally linked to states (head poses) or actions.  #@NEW_LINE#@#  The agent can learn this and then apply this knowledge to a mentor agent to infer that the mentor does not have a goal when blindfolded (i.e., the inferred distribution over goals approximates a uniformly random distribution).  #@NEW_LINE#@#  However, if this alternate Gaussian process is not learned, the agent does not know the consequences of wearing a blindfold and follows the mentors head movement even if the mentor is blindfolded.  #@NEW_LINE#@#  Fig 1c shows the combined graphical model for following the gaze of the mentor, based on combining a model for the agent and a copy for the mentor.  #@NEW_LINE#@#  


Case_II__Discrete-Valued_Random_Variables  #@NEW_LINE#@#  
When the random variables for states, actions, and goals are discrete-valued, the model can be expressed in terms of conditional probability tables for the transition probabilities P(XfXi, A) and the policy P(AG, X).  #@NEW_LINE#@#  
For concreteness, we describe this case in the context of a simple tabletop task involving a set of small objects on a tabletop which can be moved around by a human or a robotic arm as shown in Fig 2.  #@NEW_LINE#@#  The discretized position of an object defines its state and a goal corresponds to the object reaching a particular state.  #@NEW_LINE#@#  The robotic arm can manipulate the state of any object using a set of actions.  #@NEW_LINE#@#  We define X to be the set of discrete states in the environment, A to be the set of all possible actions available to the robot (these can be different from the possible actions of the human demonstrator) and G to be the set of possible goal states.  #@NEW_LINE#@#  We assume all three sets are finite.  #@NEW_LINE#@#  
(a) The robot is located on the left side of the work area and the Kinect looks down from the left side from the robot perspective.  #@NEW_LINE#@#  The three predefined areas that distinguish object states are notated.  #@NEW_LINE#@#  (b) Toy tabletop objects.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0141965.g002  #@NEW_LINE#@#  
Each goal g  G represents an abstract task which can be achieved using one or more actions in A.  #@NEW_LINE#@#  For example, a goal can be moving an object to location A regardless of its starting location.  #@NEW_LINE#@#  The object could be picked and placed at A, pushed to A, or transported using some other action, but the goal remains the same.  #@NEW_LINE#@#  The dynamics of the action and its effect are modeled as a Markov state-transition model (Fig 3a); when the robot in a state xi executes an action ai, it enters a state xf with the transition probability P(Xf = xfXi = xi, A = ai).  #@NEW_LINE#@#  
(a) through (f) illustrate the use of graphical models for learning state-transitions, action inference, goal inference, goal-based imitation, and state prediction.  #@NEW_LINE#@#  Shaded nodes denote observed variables.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0141965.g003  #@NEW_LINE#@#  
Learning_Through_Self-Experience  #@NEW_LINE#@#  
We assume that the transition probability distribution is initially unknown to the robot and must be learned through exploration, similar to the manner in which infants explore the consequences of their actions through exploration and body babbling [11, 17].  #@NEW_LINE#@#  The robot collects training data tuples (x, a, x) for the graphical model in Fig 3a by executing a random action a from a random initial state x and observing a resulting final state x multiple times.  #@NEW_LINE#@#  Given this training data, maximum likelihood parameter estimation is used to learn parameters for the transition probability distribution P(XfXi, A).  #@NEW_LINE#@#  

Goals_and_Goal_Inference  #@NEW_LINE#@#  
After the transition parameters are learned, a goal-based graphical model, , is created by augmenting the initial model in Fig 3a with a new node G as shown in Fig 3c.  #@NEW_LINE#@#  The robot then engages in goal-based exploration in which a goal state g is chosen at random and for any given initial state Xi = xi, the robot performs Bayesian inference to infer P(AXi = xi, Xf = g).  #@NEW_LINE#@#  It samples an action A* from this distribution and executes this action.  #@NEW_LINE#@#  If the goal g is reached, it increases the policy probability P(A = A*G = g, Xi = xi) by a small amount, and decreases P(A = AG = g, Xi = xi) for all other actions A.  #@NEW_LINE#@#  An alternate approach, which we follow here, is to directly set P(AG = g, Xi = xi) = P(AXi = xi, Xf = g).  #@NEW_LINE#@#  In this case, the policy is based purely on the plan inferred from a learned transition model without verification that a goal can be reached, so the policys accuracy will depend on the accuracy of the transition model.  #@NEW_LINE#@#  For the simple tabletop experiment we illustrate here, we adopt the latter approach given that accurate transition models can be learned for the small state and action space.  #@NEW_LINE#@#  
For goal inference, the robot observes object states  and  (e.g., object locations) from a human demonstration.  #@NEW_LINE#@#  By invoking the Like me hypothesis, the robot uses its goal-based graphical model to compute the posterior distribution over goals G given , and , as depicted in Fig 3d (note that the variable A is marginalized out during goal inference).  #@NEW_LINE#@#  

Goal-Based_Imitation_and_Action_Selection  #@NEW_LINE#@#  
Goal-based imitation is implemented as a two-stage process: (i) the robot infers the likely goal of the human demonstration using the goal-based graphical model described above, and (ii) either executes the action most likely to achieve this goal, or seeks human assistance if the goal is found to be unlikely to be achieved.  #@NEW_LINE#@#  Specifically, the robot senses the current state  using its sensors and infers the humans goal gMAP by taking the mode of the posterior distribution of G from the goal-inference step.  #@NEW_LINE#@#  It then computes the posterior over actions A as shown in Fig 3e and selects the maximum a posteriori action aMAP.  #@NEW_LINE#@#  Since aMAP is not guaranteed to succeed, the robot predicts the probability of reaching the most probable final state  using aMAP by computing the posterior probability of Xf as shown in Fig 3f.  #@NEW_LINE#@#  If this probability of reaching the desired state is above a prespecified threshold, , the robot executes aMAP, otherwise it executes the Ask human action to request human assistance.  #@NEW_LINE#@#  


Robot  #@NEW_LINE#@#  
We use the Gambit robot arm-and-gripper (Fig 2a) designed at Intel Labs Seattle.  #@NEW_LINE#@#  Gambit is well-suited to tabletop manipulation tasks with small objects and has previously been shown to perform well in tasks with humans in the loop [18].  #@NEW_LINE#@#  
Sensing  #@NEW_LINE#@#  
For sensing the current state of objects on the table, for example, during human demonstrations, we use the Microsoft Kinect RGBD camera.  #@NEW_LINE#@#  The Kinect is mounted on the base frame of Gambit and looks down on the table surface.  #@NEW_LINE#@#  The robot takes as input the stream of RGB and depth images from the Kinect and first segments out the background and the hand of the human holding the small objects.  #@NEW_LINE#@#  The remaining pixels are then used to determine the state of objects on the table using a simple heuristic based on centroids.  #@NEW_LINE#@#  We define three discrete areas that are used for defining the state of the objects as shown in Fig 2a(1) LEFT signifying that the object is on the left side of the blueline on the table; (2) RIGHT denoting that the object is on the right side of the blueline; and (3) OFFTABLE signifying that the object is not in the tabletop work area.  #@NEW_LINE#@#  

Robot_Action  #@NEW_LINE#@#  
We assume that the robot possesses a fixed set of six high-level actions for manipulating objects: place LEFT (PlaceL), place RIGHT (PlaceR), place OFFTABLE (PlaceOt), push to LEFT (pUshL), push to RIGHT (pUshR), and push OFFTABLE (pUshOt).  #@NEW_LINE#@#  For the place actions, the robot first attempts to pick up the object by moving its end effector above the centroid of the object and rotating the gripper to align itself perpendicular to the major axis of the object.  #@NEW_LINE#@#  If the robot successfully picked up the object, it places the object down at the location (LEFT, RIGHT, or OFFTABLE) indicated by the place command.  #@NEW_LINE#@#  For the push actions, the robot first positions its end effector behind the object based on its centroid and direction of the push.  #@NEW_LINE#@#  For pUshL and pUshR, the gripper yaw angle is rotated perpendicular to the major axis of the table, while for pUshOt, it is rotated parallel to the major axis of the table.  #@NEW_LINE#@#  This ensures that object contact area is maximized to reduce the chance of the object slipping while pushing.  #@NEW_LINE#@#  The robot pushes the object until it changes state (or the inverse kinematics solver fails to find a possible solution).  #@NEW_LINE#@#  



Results  #@NEW_LINE#@#  
Gaze_Following__Model_Simulations  #@NEW_LINE#@#  
To test our model, we randomly sample goal positions and then compute the action required to fixate on this goal.  #@NEW_LINE#@#  We add Gaussian noise to this action, compute the resulting gaze vector if this action were taken, and add Gaussian noise to this gaze vector.  #@NEW_LINE#@#  This method is equivalent to training the model with rejection sampling wherein the agent rejects all samples that do not result in successful fixation on the goal position.  #@NEW_LINE#@#  The Gaussian processes are trained on this randomly generated data and then tested on separate test data.  #@NEW_LINE#@#  The default reference frame for both agent and mentor is at the origin gazing along the x-axis.  #@NEW_LINE#@#  Each agent has their own reference frame and we assume that we know the transformation from the mentors reference frame to the agents.  #@NEW_LINE#@#  This transformation is not learned by our model but we believe that this is a minor assumption, especially since we already assume the agent can observe the mentors position and head pose.  #@NEW_LINE#@#  
The mentor and agent are positioned as shown in Fig 4.  #@NEW_LINE#@#  Goal locations for the training data were generated uniformly at random from the area between the agent and the mentor (within the rectangle formed by x in [100, 500] and y in [500,500], where the agent is at (0,0) and the mentor is at (600,0)).  #@NEW_LINE#@#  We used Gaussian noise with standard deviation of 3 degrees for angles and a standard deviation of 10 cm for locations and distances.  #@NEW_LINE#@#  For reverse inference, the prior goal state P(G) is a Gaussian centered halfway between the two agents along the x-axis.  #@NEW_LINE#@#  While this prior is quite weak, a single observation of (xi, xf) is insufficient to overcome the prior in reverse inference.  #@NEW_LINE#@#  Instead, we use a single observation of xi and five observations of xf to get an accurate estimate of the goal distribution P(Gxi, xf).  #@NEW_LINE#@#  More precisely, we run reverse inference with the observed values  to compute , and then use this as the prior for a second run of reverse inference to compute .  #@NEW_LINE#@#  We repeat this five times to compute .  #@NEW_LINE#@#  We believe such an inference process could be executed within the short amount of time taken for gaze following.  #@NEW_LINE#@#  
The agent and mentor face each other in a 2D simulated environment.  #@NEW_LINE#@#  Goal positions inferred by the model from mentor observations are shown in red next to true goal positions (blue) for sampled goal positions both to the left and to the right of the agent.  #@NEW_LINE#@#  Black arrows represent the initial and final gaze vectors of the agent and mentor for one of these test data points.  #@NEW_LINE#@#  In this simple example, the goal locations were equidistant from the agent and mentor but the model readily generalizes to other cases as well.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0141965.g004  #@NEW_LINE#@#  
Gaze_Following__Model_Performance  #@NEW_LINE#@#  
We found that the model learns accurate transition and policy functions from small amounts of noisy training data (n = 200 data points in our accuracy tests).  #@NEW_LINE#@#  The nonparametric nature of Gaussian processes ensured very little customization was required.  #@NEW_LINE#@#  The model runs in sub-minute times for the dimensionality we are using, though additional approximations may be necessary to scale well to high dimensions.  #@NEW_LINE#@#  We also found that the Gaussian approximations we are making have little effect because the data is generally unimodal and close to symmetric.  #@NEW_LINE#@#  
Figs 4 and 5 show performance results for the model as it performs forward inference, reverse inference, and gaze following (combined reverse and forward inference).  #@NEW_LINE#@#  The model is robust to noise and is able to provide accurate gaze following results even though additional levels of uncertainty are introduced by the second level of inference.  #@NEW_LINE#@#  

Blindfold_Self-Experience_Task  #@NEW_LINE#@#  
In order to test the cognitive plausibility of our model, we recreated the experiments from [6], where infants self-experience with a blindfold affects whether or not they follow the gaze of blindfolded adults (see Fig 1c).  #@NEW_LINE#@#  We trained 60 separate agents with our model on randomly generated training data.  #@NEW_LINE#@#  Similar to the infant experiments, one third of these agents were given additional experience with a blindfold wherein they train an additional GP for their model so that they now have GP and , where GP is the original GP of the model and  is the GP with blindfold experience.  #@NEW_LINE#@#  The agents with the blindfold experience learn the consequences of wearing an opaque blindfold in .  #@NEW_LINE#@#  The other 40 agents were trained normally although one group is the baseline group and the other is the windowed blindfold group.  #@NEW_LINE#@#  In our simulations, the windowed blindfold group experiences the same training data as the no-blindfold case since, in both cases, the overall result is that the target is visible to the agent.  #@NEW_LINE#@#  So these two groups will be identical except for noise.  #@NEW_LINE#@#  
Each agent is presented with 4 trials where it observes a mentor make a head turn to face either 45 degrees to the left or 45 degrees to the right (plus noise).  #@NEW_LINE#@#  Trials are scored as +1 if the agent turns its head at least 30 degrees in the direction of the correct target and 1 if it turns its head at least 30 degrees in the direction of the wrong target.  #@NEW_LINE#@#  The agents with no blindfold experience use the basic model (which contains no blindfold knowledge) and thus assume that the mentor is fixating on an object to the left or to the right.  #@NEW_LINE#@#  Those with blindfold experience observe that the mentor is wearing a blindfold and use their learned  for the reverse inference (applying their model to the mentor).  #@NEW_LINE#@#  For this group, we expect the agent to have learned through self-experience that when blindfolded, there is no correlation between a head movement and any particular goal position.  #@NEW_LINE#@#  The blindfold-experienced agent then uses GP for the forward inference (because the agent is not wearing a blind-fold).  #@NEW_LINE#@#  In order to simulate infants with little experience with blindfolds, we provided the agents in this experiment with very little training data (n = 15).  #@NEW_LINE#@#  If we use more training data, the agents perform almost perfectly in this task.  #@NEW_LINE#@#  
Results are shown in Fig 6 and match the pattern of results in Meltzoff and Brooks (reproduced in Fig 6a for comparison).  #@NEW_LINE#@#  This suggests that gaze following involves understanding the underlying intention or goal of the mentor, and that self-experience plays a major role in learning the consequences of intentions and related actions.  #@NEW_LINE#@#  


Robotic_Tabletop_Manipulation_Task  #@NEW_LINE#@#  
To illustrate the discrete-valued version of the model, we used a tabletop manipulation with three toy objects of different shapes: a pear, a lemon, and a miniature broiled chicken.  #@NEW_LINE#@#  These objects could be in one of three different states: LEFT, RIGHT, and OFFTABLE, as defined in the previous section.  #@NEW_LINE#@#  The robots aim is to learn the consequences of actions on these objects, infer the goals of human actions, and imitate the actions of humans manipulating these objects on the table.  #@NEW_LINE#@#  

Fig 7 shows the learned transition probabilities for manipulating these objects based on the robots self-discovery phase.  #@NEW_LINE#@#  The transition models are learned by performing 10 trials for each initial state and action pair (xi, a) for each object type.  #@NEW_LINE#@#  We deliberately used a small number of trials to test whether the method could cope with less training data and more uncertainty in the transition model.  #@NEW_LINE#@#  
Each row represents different types of objects, and each column represents the different initial states Xi.  #@NEW_LINE#@#  The colors of bars represent different final states Xf.  #@NEW_LINE#@#  The y-axis represents the range of probabilities and the x-axis represents the six different manipulation actions available to the robot (PL = place LEFT, PR = place RIGHT, PO = place OFFTABLE, UL = pUsh LEFT, UR = pUsh RIGHT, UO = pUsh OFFTABLE).  #@NEW_LINE#@#  We do not show actions that cause self-transitions given an initial state.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0141965.g007  #@NEW_LINE#@#  
Since the state space is small, we are able to enumerate and test all of the interesting and possible human demonstrations.  #@NEW_LINE#@#  By interesting, we mean that the state changes after action execution (for example, from RIGHT to OFFTABLE) and the initial state is not OFFTABLE.  #@NEW_LINE#@#  There are in total four interesting state changes for each object that can be demonstrated by a human: LEFT to RIGHT, LEFT to OFFTABLE, RIGHT to LEFT, and RIGHT to OFFTABLE.  #@NEW_LINE#@#  Note that our current implementation does not allow the robot to pick up objects that are located OFFTABLE  #@NEW_LINE#@#  

Fig 8a shows the inferred goals given all possible interesting state changes, using the graphical model in Fig 3d.  #@NEW_LINE#@#  For all cases, our model correctly infers the intended goal state of the human.  #@NEW_LINE#@#  Fig 8b shows the maximum a posteriori probability (MAP) action for a given initial state and goal.  #@NEW_LINE#@#  Our model correctly identifies whether a place or a push action is better, given the dynamics of the object as encoded in the learned probabilistic model for the object.  #@NEW_LINE#@#  Note that the two most preferred actions are always push or place actions in the correct direction.  #@NEW_LINE#@#  
(a) Most likely goals: Initial and final states are at the top of each column.  #@NEW_LINE#@#  The height of the bar represents the posterior probability of each goal state, with the true goal state marked by an asterisk.  #@NEW_LINE#@#  (b) Inferring actions: For each initial and desired final state, the plots show the posterior probability of each of the six actions, with the MAP action indicated by an asterisk.  #@NEW_LINE#@#  (c) Predicting final state: The plots show the posterior probability of reaching the desired final state, given the initial state and the corresponding MAP action shown in (b).  #@NEW_LINE#@#  The red bar marks 0.5, the threshold below which the robot asks for human help in the Interactive Goal-Based mode.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0141965.g008  #@NEW_LINE#@#  
Finally, Fig 8c shows the predicted state distribution given an action and an initial state.  #@NEW_LINE#@#  The robot calculates the posterior probability of getting to the desired state, and executes the action if this probability is above a predetermined threshold (%50).  #@NEW_LINE#@#  Otherwise, it asks the human collabo For example, the predicted output of moving a pear from RIGHT to LEFT (1st row of 3rd column in Fig 8c) using the pushing left action is lower than %50, and therefore the robot will ask request the human help.  #@NEW_LINE#@#  

Table 1 compares trajectory-based imitation of the human demonstration with our proposed goal-based approach.  #@NEW_LINE#@#  The trajectory-based approach simply mimics the human action without considering the goal or uncertainty, i.e., it executes a place action if the human executes a place, and a push action if the human executes a push.  #@NEW_LINE#@#  The goal-based approach on the other hand recognizes the goal and uses the best action it has available to achieve the goal.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0141965.t001  #@NEW_LINE#@#  
Using our computed transition probabilities, we can calculate the hypothetical success rate of a purely trajectory-based approach.  #@NEW_LINE#@#  For our goal-based approach, we use the posterior distribution shown in Fig 8b.  #@NEW_LINE#@#  Finally, the Interactive Goal-Based mode assumes that the robot may ask a human for help, with a 100% success rate when the human executes the requested action.  #@NEW_LINE#@#  The third column in Table 1 shows what the performance would be if we require the robot to be 50% sure of reaching a desired state.  #@NEW_LINE#@#  We do not see perfect imitation results on the third column because the robot does not ask the human for help in every case.  #@NEW_LINE#@#  In some cases, the probability of success will surpass the confidence threshold, but the goal state may not be reached after the action is executed.  #@NEW_LINE#@#  
The results demonstrate the expected behavior of the goal-based method.  #@NEW_LINE#@#  The success rates in goal-based method (Table 1, second column) are identical across the different actions demonstrated (e.g.  #@NEW_LINE#@#  pick & place row vs. push row).  #@NEW_LINE#@#  In addition, the results demonstrate the advantage of a goal-based approach over purely trajectory-based imitation, and its potential as a human-robot collaboration mechanism.  #@NEW_LINE#@#  


Contributions_and_Comparisons_with_Related_Work  #@NEW_LINE#@#  
Our work makes contributions both to the fields of robotics and cognitive modeling.  #@NEW_LINE#@#  In robotics, our approach contributes to the growing number of efforts leveraging biological paradigms and developmental science to design new methods for social robotics and human-robot interaction [17, 1930].  #@NEW_LINE#@#  Specifically, our proposed framework for goal inference and imitation lays the foundation for a new approach to designing social robots as robots that learn internal models of their environment through self-experience and utilize these models for human intent recognition, skill acquisition from human observation, and human-robot collaboration.  #@NEW_LINE#@#  Although our results are based on proof-of-concept systems, the underlying Bayesian framework is general.  #@NEW_LINE#@#  We anticipate being able to progressively increase the sophistication of our models by leveraging the rapid advances being made in probabilistic reasoning, Bayesian modeling, and learning.  #@NEW_LINE#@#  As an illustrative example, the success of simultaneous localization and mapping (SLAM) algorithms in robotics [31] in the past decade can be largely attributed to the availability of efficient algorithms and computing platforms for probabilistic reasoning, within the broader context of probabilistic robotics [32].  #@NEW_LINE#@#  
Our approach to robotic imitation emphasizes the importance of goal inference in imitation learning, compared to traditional methods for programming-by-demonstration that have relied on following action trajectories.  #@NEW_LINE#@#  Previous approaches to robotic imitation that have also relied on high-level goals (e.g., [33, 34]) have not emphasized self-discovery of probabilistic models, a central tenet of the developmental approach proposed here for bootstrapping goal-based imitation.  #@NEW_LINE#@#  Other robotics work has focused on attempting to model continuous low-level goals [35].  #@NEW_LINE#@#  Neither of these approaches adopt probabilistic models for human-robot interaction tasks.  #@NEW_LINE#@#  Approaches that do utilize Bayesian methods for goal inference [36, 37] and robotic imitation [38] have done so without the developmental science perspective that we bring to the problem.  #@NEW_LINE#@#  
In the context of cognitive modeling, our model can be regarded as a Bayesian instantiation of the Like-Me developmental hypothesis [7, 39].  #@NEW_LINE#@#  It acknowledges a role for a certain type of mental simulation [8] as well as a learned theory [9, 10] of actions and their consequences using a probabilistic model learned from experience (see also [40]).  #@NEW_LINE#@#  The model is closely related to the goal-based imitation model of Verma and Rao [3, 17, 41] and the inverse planning model of Baker et al.  #@NEW_LINE#@#  [42].  #@NEW_LINE#@#  Also related to our approach are models for planning based on probabilistic inference [3, 4346].  #@NEW_LINE#@#  
Meltzoffs Like-Me hypothesis has previously been applied in robotics to tackle the important problem of whom to imitate [47]: In this case, a robot first builds a self model, and then uses this self-model to discover self-other equivalences with other robots so as to distinguish an appropriate from an inappropriate mentor robot.  #@NEW_LINE#@#  Note that in this case as well as in our own work, the mentor agent is not necessarily an active teacher but is only being observed (cf.  #@NEW_LINE#@#  [48]).  #@NEW_LINE#@#  A recent developmental approach to robotic imitation [49] also focuses on tabletop manipulation tasks, with greater complexity than our experiments, but without the benefits of bootstrapping derived from the Like-Me hypothesis inherent in our framework.  #@NEW_LINE#@#  

Summary_and_Conclusion  #@NEW_LINE#@#  
Our results suggest that the process of imitation-based learning can be bootstrapped by (i) learning a probabilistic model of the perceptual consequences of ones own actions through self-experience, (ii) using this learned model to infer the goals of actions of others, and (iii) using the inferred goals to perform goal-based imitation.  #@NEW_LINE#@#  Such a strategy works even when the imitators actuators are different from the demonstrators.  #@NEW_LINE#@#  We first showed how such a model can emulate infant development of gaze following by implementing gaze following via goal inference in learned graphical models based on Gaussian processes.  #@NEW_LINE#@#  Using a table-top robotic system, we demonstrated that a robot can learn probabilistic models of actions on objects and utilize these models for inferring the intent of human actions on objects.  #@NEW_LINE#@#  Additionally, we showed that the approach facilitates human-robot collaboration by allowing the robot to predict the success probability of achieving a goal and to seek human help when success probability is low.  #@NEW_LINE#@#  
Our results point to a number of interesting open issues.  #@NEW_LINE#@#  For the studies in this paper, we used a simple exhaustive exploration strategy, but for larger state spaces, a more sophisticated approach based on reward functions (e.g., [50]) could be employed.  #@NEW_LINE#@#  Additionally, following the example of human infants, some form of directed self-exploration based on observing human mentors (e.g., [51]) may be desirable.  #@NEW_LINE#@#  The model assumes that states in the environment are known (corresponding to the case of MDPs or Markov decision processes)an interesting direction for future work is extending the model to the more realistic case where only observations of states are available (partially observable MDPs or POMDPs) and where learning involves optimizing rewards.  #@NEW_LINE#@#  Finally, our current model involves inference over a single time-step, which simplifies the inference problem.  #@NEW_LINE#@#  We hope to explore the applicability of our model for multi-step inference problems in future work.  #@NEW_LINE#@#  
The approach we have presented lends itself naturally to generalization based on relational probabilistic models [52, 53] and hierarchical Bayesian representations [54].  #@NEW_LINE#@#  Such models have the potential to significantly increase the scalability and applicability of our suggested approach to large-scale scenarios, besides facilitating transfer of learned skills across tasks and domains.  #@NEW_LINE#@#  We intend to investigate such relational models in future work.  #@NEW_LINE#@#  

Acknowledgments  #@NEW_LINE#@#  
This research was supported by ONR Science of Autonomy grant no.  #@NEW_LINE#@#  N000141310817, ONR Cognitive Science program grant no.  #@NEW_LINE#@#  N000140910097, NSF grant no.  #@NEW_LINE#@#  SMA-1540619 and NSF grant no.  #@NEW_LINE#@#  IIS-1318733, by the Intel Science and Technology Center (ISTC), Seattle, and by an NSERC fellowship to A. Friesen.  #@NEW_LINE#@#  The authors would like to thank Marc Deisenroth, Cynthia Matuszek and Kendall Lowrey for their advice, sharing code, and for help during the initial stages of this project.  #@NEW_LINE#@#  

Author_Contributions  #@NEW_LINE#@#  
Conceived and designed the experiments: RR MC AF AM DF.  #@NEW_LINE#@#  Performed the experiments: MC AF.  #@NEW_LINE#@#  Analyzed the data: MC AF.  #@NEW_LINE#@#  Contributed reagents/materials/analysis tools: MC AF.  #@NEW_LINE#@#  Wrote the paper: RR MC AF AM DF.  #@NEW_LINE#@#  

References  #@NEW_LINE#@#  


