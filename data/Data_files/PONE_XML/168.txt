article id="http://dx.doi.org/10.1371/journal.pone.0147215"  #@NEW_LINE#@#  
title  #@NEW_LINE#@#  
When Quality Beats Quantity: Decision Theory, Drug Discovery, and the Reproducibility Crisis  #@NEW_LINE#@#  

Abstract  #@NEW_LINE#@#  
A striking contrast runs through the last 60 years of biopharmaceutical discovery, research, and development.  #@NEW_LINE#@#  Huge scientific and technological gains should have increased the quality of academic science and raised industrial R&D efficiency.  #@NEW_LINE#@#  However, academia faces a "reproducibility crisis"; inflation-adjusted industrial R&D costs per novel drug increased nearly 100 fold between 1950 and 2010; and drugs are more likely to fail in clinical development today than in the 1970s.  #@NEW_LINE#@#  The contrast is explicable only if powerful headwinds reversed the gains and/or if many "gains" have proved illusory.  #@NEW_LINE#@#  However, discussions of reproducibility and R&D productivity rarely address this point explicitly.  #@NEW_LINE#@#  The main objectives of the primary research in this paper are: (a) to provide quantitatively and historically plausible explanations of the contrast; and (b) identify factors to which R&D efficiency is sensitive.  #@NEW_LINE#@#  We present a quantitative decision-theoretic model of the R&D process.  #@NEW_LINE#@#  The model represents therapeutic candidates (e.g., putative drug targets, molecules in a screening library, etc.)  #@NEW_LINE#@#  within a measurement space", with candidates' positions determined by their performance on a variety of assays (e.g., binding affinity, toxicity, in vivo efficacy, etc.)  #@NEW_LINE#@#  whose results correlate to a greater or lesser degree.  #@NEW_LINE#@#  We apply decision rules to segment the space, and assess the probability of correct R&D decisions.  #@NEW_LINE#@#  We find that when searching for rare positives (e.g., candidates that will successfully complete clinical development), changes in the predictive validity of screening and disease models that many people working in drug discovery would regard as small and/or unknowable (i.e., an 0.1 absolute change in correlation coefficient between model output and clinical outcomes in man) can offset large (e.g., 10 fold, even 100 fold) changes in models brute-force efficiency.  #@NEW_LINE#@#  We also show how validity and reproducibility correlate across a population of simulated screening and disease models.  #@NEW_LINE#@#  We hypothesize that screening and disease models with high predictive validity are more likely to yield good answers and good treatments, so tend to render themselves and their diseases academically and commercially redundant.  #@NEW_LINE#@#  Perhaps there has also been too much enthusiasm for reductionist molecular models which have insufficient predictive validity.  #@NEW_LINE#@#  Thus we hypothesize that the average predictive validity of the stock of academically and industrially "interesting" screening and disease models has declined over time, with even small falls able to offset large gains in scientific knowledge and brute-force efficiency.  #@NEW_LINE#@#  The rate of creation of valid screening and disease models may be the major constraint on R&D productivity.  #@NEW_LINE#@#  

Introduction  #@NEW_LINE#@#  
The scope, quality and cost efficiency of the scientific and technological tools that are widely believed to be important for progress in biopharmaceutical discovery and research have improved spectacularly.  #@NEW_LINE#@#  To quote a review from 2012 [1]:  combinatorial chemistry increased the number of drug-like molecules that could be synthesized per chemist per year by perhaps 800 times through the 1980s and 1990s [2] [3] [4] and greatly increased the size of chemical libraries [5].  #@NEW_LINE#@#  DNA sequencing has become over a billion times faster since the first genome sequences were determined in the 1970s [6] [7] aiding the identification of new drug targets.  #@NEW_LINE#@#  It now takes at least three orders of magnitude fewer man-hours to calculate three-dimensional protein structure via x-ray crystallography than it did 50 years ago [8] [9], and databases of three-dimensional protein structure have 300 times more entries than they did 25 years ago [10] [9], facilitating the identification of improved lead compounds through structure-guided strategies.  #@NEW_LINE#@#  High throughput screening (HTS) has resulted in a tenfold reduction in the cost of testing compound libraries against protein targets since the mid-1990s [11].  #@NEW_LINE#@#  Added to this are new inventions (such as the entire field of biotechnology, computational drug design and screening, and transgenic mice) and advances in scientific knowledge (such as an understanding of disease mechanisms, new drug targets, biomarkers, and surrogate endpoints).  #@NEW_LINE#@#  
These kinds of improvements should have allowed larger biological and chemical spaces to be searched for therapeutic conjunctions with ever higher reliability and reproducibility, and at lower unit cost.  #@NEW_LINE#@#  That is, after all, why many of the improvements were funded in the first place.  #@NEW_LINE#@#  However, in contrast [12], many results derived with todays powerful tools appear irreproducible[13] [14][15] [16]; todays drug candidates are more likely to fail in clinical trials than those in the 1970s [17] [18]; R&D costs per drug approved roughly doubled every ~9 years between 1950 and 2010 [19] [20] [1], with costs dominated by the cost of failures [21]; and some now even doubt the economic viability of R&D in much of the drug industry [22] [23].  #@NEW_LINE#@#  
The contrasts [12] between huge gains in input efficiency and quality, on one hand, and a reproducibility crisis and a trend towards uneconomic industrial R&D on the other, are only explicable if powerful headwinds have outweighed the gains [1], or if many of the gains have been illusory [24] [25] [26].  #@NEW_LINE#@#  
We believe that a variety of standard tools from the fields of decision theory and decision analysis (DT) [27] [28] [29] [30] [31] shed light on the headwinds and may help distinguish the kind of gains that are likely to be real.  #@NEW_LINE#@#  The Methods and Results section of the paper presents a DT-based model of biopharmaceutical R&D and quantitative analyses that explore the factors to which R&D decisions are sensitive.  #@NEW_LINE#@#  The model is described in terms of commercial R&D, but we think the framework and the results are generalizable to the academic setting, and to translation from academia to industry; in fact to many situations where positives (e.g., good drug targets, good candidate therapeutic mechanisms) are rare and where a large universe of possibilities is filtered via a series of measurements and decisions to a small set of possibilities.  #@NEW_LINE#@#  In statistical or DT terms, the mechanics of the model are fairly standard.  #@NEW_LINE#@#  The model is a classifier in the presence of multiple, or multistep, predictors.  #@NEW_LINE#@#  However, the application is, we think, novel.  #@NEW_LINE#@#  
Readers who are less familiar with statistics and DT may prefer to read the Discussion section before returning to the Methods and Results.  #@NEW_LINE#@#  The Discussion is in three parts.  #@NEW_LINE#@#  Part 1 frames headwinds to R&D productivity in terms of the progressive exploitation, exhaustion, and abandonment of disease models with high predictive validity (PV).  #@NEW_LINE#@#  Part 2 considers the reproducibility crisis in similar terms.  #@NEW_LINE#@#  Part 3 sets out some practical suggestion to improve PV evaluation and raise PV.  #@NEW_LINE#@#  

Methods_and_Results  #@NEW_LINE#@#  
Terminology_and_Model_Structure  #@NEW_LINE#@#  
We begin by introducing and defining our terms and the basic structure of the model we use to represent the process of discovery, research, and development (Table 1, Fig 1).  #@NEW_LINE#@#  The code of the programmes that we used to implement our model is in S1 File.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0147215.t001  #@NEW_LINE#@#  
(A) The process starts with a large set of therapeutic possibilities (light blue oval).  #@NEW_LINE#@#  These could be putative disease mechanisms or candidate drug targets, in either an academic or commercial setting.  #@NEW_LINE#@#  However, we discuss them as if they are molecules in a commercial R&D campaign (e.g., compounds in a screening library and the analogues that could be reasonably synthesized to create leads).  #@NEW_LINE#@#  There are A candidates that with perfect R&D decision making and an unlimited R&D budget would eventually be approved by the drug regulator for the indication or indications.  #@NEW_LINE#@#  There are U candidates that would not succeed given similar skill and investment.  #@NEW_LINE#@#  In general, U  A.  #@NEW_LINE#@#  The Discovery (D), Preclinical (P), and Clinical Trial (C) diamonds are classifiers (Table 1).  #@NEW_LINE#@#  Each takes decision variables (X, Y, Z) from predictive models for some or all of the candidates and tests the variables against a decision threshold, yielding yeses which receive further scrutiny or noes which are abandoned.  #@NEW_LINE#@#  The unit cost per surviving candidate increases through the process [21].  #@NEW_LINE#@#  Given serial decisions, only yeses from C face the gold standard reference test; the drug regulator (e.g., the Food and Drug Administration, or FDA).  #@NEW_LINE#@#  The other decisions face imperfect reference tests [33] [34] [27], the next steps in the process, which are mere proxies for the gold standard.  #@NEW_LINE#@#  The imperfect reference test for yeses from D is provided by P. The imperfect reference test for yeses from P is provided by C. (B) Decision variables X, Y, and Z, will correlate to a greater or lesser extent with each other and with the gold standard reference variable R. The correlation coefficient between X and Y is X,Y, the correlation coefficient between Y and Z is Y,Z, etc.  #@NEW_LINE#@#  Most of these correlations will never be measured directly during the R&D process.  #@NEW_LINE#@#  If X,R is very low, the Discovery stage will not enrich the Preclinical stage for approvable candidates, even if X,Y is high and decisions from D initially appear to have been successful.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0147215.g001  #@NEW_LINE#@#  
We note that DT-related ideas, sometimes with a different intellectual heritage, are already applied in many technical activities in drug R&D.  #@NEW_LINE#@#  For example, they are used extensively with respect to clinical trial design towards the end of the process (e.g., references: [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49]), to chemistry and screening near the start of the process (e.g., references: [50] [51] [52] [53] [54] [32] [55] [56] [57] [58] [59] [60]), but more rarely elsewhere (but see, e.g., references: [61] [62] [63]).  #@NEW_LINE#@#  
A prerequisite for the effective application of DT is the correct representation of the system in which decisions are made [64].  #@NEW_LINE#@#  Thus Fig 1B emphasizes the fact that translational medicine in general and commercial drug R&D in particular both involve a set of measurements that are often intended to co-vary or correlate with one another to a greater or lesser degree.  #@NEW_LINE#@#  The purpose of molecular assays is often to predict in vivo potency or toxicity.  #@NEW_LINE#@#  The purpose of animal efficacy studies is to predict clinical activity in man.  #@NEW_LINE#@#  This important feature of the process is not captured by some R&D productivity frameworks [21] [61] [65], although it is often reflected in the qualitative discussions that accompany them [21] [65], and is clearly apparent in parts of the clinical trial literature [37] [35] [36].  #@NEW_LINE#@#  The frameworks that ignore the correlation between different measures miss, for example, the fact that changing the decision threshold (i.e., stringency), throughput, or model quality at one step in the process implies changes elsewhere in the process (see later).  #@NEW_LINE#@#  
Also central to our analysis is the concept of predictive validity (PV).  #@NEW_LINE#@#  We define the PV of a decision variable (e.g., a binding affinity measures in a high-throughput drug screen, the gut feelings of an expert medicinal chemist, the rank-ordering of drug candidates in an R&D portfolio management meeting, Phase II results, etc.)  #@NEW_LINE#@#  as the degree to which the ordering of the population of candidates on the decision variable would match the ordering of the candidates on a corresponding reference variable, in the limit case when sample sizes are large.  #@NEW_LINE#@#  PV is high when there is a high probability that the ordering of drug candidate a and drug candidate b on the reference variable is the same as the ordering of a and b on the decision variable (Table 1).  #@NEW_LINE#@#  The reference variable is the more definitiveand generally more expensive [21]measure that is made later in the R&D process, with the ultimate gold standard reference often being regulatory approval.  #@NEW_LINE#@#  Note that nearly all decisions in R&D are tested against an imperfect reference [33] [34] [27], the next step of the R&D process, and not against gold standards such as regulatory approval or therapeutic and commercial success (Fig 1).  #@NEW_LINE#@#  
We use the term PV because general terms such as validity and validation have a range of different meanings in the biomedical literature (see, for example: [66] [53] [67] [68] [69] [70] [71]).  #@NEW_LINE#@#  Our definition of PV also distinguishes PV from reliability (Table 1).  #@NEW_LINE#@#  Reliability is something that is, in principle at least, amenable to conventional statistical management and can be increased by increasing sample size [29] [13] [72].  #@NEW_LINE#@#  While we frame our analyses in terms of PV, one could conduct similar analyses of decision variables reliability.  #@NEW_LINE#@#  We will also use the term predictive model, or PM (Table 1), to refer to a screening or disease model when it is used to generate a decision variable for one or more therapeutic candidates.  #@NEW_LINE#@#  Again, this is because the term model has various different meanings [73] [69].  #@NEW_LINE#@#  

The_Compounding_Effects_of_True_and_False_Positive_Rates  #@NEW_LINE#@#  
Fig 1A shows a series of decisions acting on an initial sample of therapeutic candidates of which A would be approved if fully developed and then scrutinized by the regulator, and of which U would not.  #@NEW_LINE#@#  The objective of the subsequent R&D process is to increase the ratio of approvable to unapprovable candidates.  #@NEW_LINE#@#  
The ratios of approvable to unapprovable candidates through the process are given by Eqs 14.  #@NEW_LINE#@#  The equations show the importance of the spread between the TPR and FPR of each decision, and the compounding effect of sequential TPRs and FPRs, in achieving the objective.  #@NEW_LINE#@#  
Here, Qstart is the ratio of approvable to unapprovable candidates in the initial starting set; Q DP is the ratio among candidates leaving Discovery and entering Preclinical; and Q PC is the ratio leaving Preclinical and entering Clinical Trials; etc.  #@NEW_LINE#@#  TPRD and FPRD are true and false positive rates for classifier D using the gold standard of regulatory approval (the FDA) as the reference (Fig 1A); TPRP and FPRP are stepwise true and false positive rates for classifier P using the FDA as the reference; etc.  #@NEW_LINE#@#  
With a series of high TPRs and low FPRs, Q will tend to be high.  #@NEW_LINE#@#  With a series of low TPRs and high FPRs, Q will tend to be low.  #@NEW_LINE#@#  While this is clearly apparent in some R&D productivity analyses [61] [49], the importance of the TPR versus FPR spread is not captured by other sets of metrics that have been influential in the drug industry[21] [65].  #@NEW_LINE#@#  As Cook et al.  #@NEW_LINE#@#  [65] point out, management metrics that focus on the quantity of R&D activity, not on decision quality, have sometimes proven counterproductive.  #@NEW_LINE#@#  
Eqs 14 also show the importance of starting with the right set of therapeutic candidates (i.e., a sufficiently high A to U ratio).  #@NEW_LINE#@#  This topic is already the focus of a large body of literature in, for example, the fields of chemoinformatics, screening library design, and structure-based design, and we do not consider it further in this paper.  #@NEW_LINE#@#  

Presentation_of_the_Quantitative_Decision_Model  #@NEW_LINE#@#  
We have produced a quantitative decision model that can be applied to the process shown in Fig 1.  #@NEW_LINE#@#  Each decision or reference variable (the random variables X, Y, Z, , R, Table 1) corresponds to one axis of a multidimensional measurement space.  #@NEW_LINE#@#  The individual scores of the therapeutic candidates, molecules a, b, c, d, etc., on each variable are coordinates in the space.  #@NEW_LINE#@#  Thus candidate molecule a occupies position (xa, ya, za), molecule b occupies position (xb, yb, zb), etc.  #@NEW_LINE#@#  One can apply one or more decision thresholds (thresholds xt, yt, zt, etc.  #@NEW_LINE#@#  )or other decision rulesto divide the space and to assess the quantitative relationships between decision performance (e.g., PPV, FDR, or TPR), and a variety of factors such as the proportion of positives at the start of the process (i.e., A/(A + U) in Fig 1), the throughput or brute-force power of each PM, and the degree to which each PM yields decision variables that are correlated with other decision variables and with R, the gold standard reference variable (Fig 1B).  #@NEW_LINE#@#  
For the analyses shown in the body of this paper, the probability density of molecules within the measurement space is a multivariate normal distribution.  #@NEW_LINE#@#  More formally, we use a random vector of standardized covariates x = [X, Y, Z, , R] distributed as a multivariate normal distribution,  where  = [0, 0, 0,, 0] and the covariance matrix, , is equal to the correlation matrix, corr[X, Y, Z, , R]:  #@NEW_LINE#@#  
(5)  #@NEW_LINE#@#  
We have repeated the analysis for other probability density functions, with sometimes identical, often similar, but sometimes predictably different results (S2 File).  #@NEW_LINE#@#  
The model can be applied to multiple decision variables and classification steps (see later), but we start with a single decision step (Fig 2).  #@NEW_LINE#@#  Here, the random vector x = [Y, R] is distributed as a bivariate normal distribution, and the correlation coefficient between decision variable Y and reference variable R is Y,R.  #@NEW_LINE#@#  The correlation parameter, Y,R, (Fig 2, Eq 8) operationalises the concept of the predictive validity (PV) of the reference variable.  #@NEW_LINE#@#  When the correlation between the reference variable and decision variable is high, the ordering of candidates on the decision variable will tend to match the ordering of candidates on the reference variable.  #@NEW_LINE#@#  It would, of course, be possible to operationalize the concept of PV in other ways (Table 1).  #@NEW_LINE#@#  
Bivariate normal probability density function determined by the correlation, Y,R, between decision variable, Y, and reference variable, R. Lighter colours indicate high probability density (candidate molecules more likely to lie here), and darker colours indicate a low probability density (molecules less likely to lie here).  #@NEW_LINE#@#  The units on the horizontal and vertical axes are one standard deviation.  #@NEW_LINE#@#  We apply a decision threshold, yt (vertical dotted line) to the decision variable and then apply a reference test and a reference threshold, rt,(horizontal dotted line) to molecules that exceed the decision threshold yt.  #@NEW_LINE#@#  In the sensitivity analyses (see later) decision and reference thresholds are varied as is Y,R.  #@NEW_LINE#@#  True positives (TP) and false positives (FP) correspond to the probability mass in the upper right and lower right quadrants, respectively.  #@NEW_LINE#@#  (A) When Y,R is high, PPV is high.  #@NEW_LINE#@#  (B) When Y,R is low, PPV tends to be low.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0147215.g002  #@NEW_LINE#@#  
A molecule will be classified as a yes, and receive further scrutiny, if its score on the decision variable meets or exceeds a threshold yt (Fig 2).  #@NEW_LINE#@#  The decision threshold yt can be regarded both as a measure of the rate of attrition or stringency of the decision and also as a measure of throughput.  #@NEW_LINE#@#  
This point may not be obvious, but it is important.  #@NEW_LINE#@#  As yt rises, fewer candidate molecules are deemed to be yeses, so one has to screen more therapeutic candidates for each yes.  #@NEW_LINE#@#  When yt = 2.32 standard deviation units (horizontal axis, Fig 2), only the top hundredth of molecules will be yeses.  #@NEW_LINE#@#  One would expect to screen one hundred candidates per yes.  #@NEW_LINE#@#  When yt = 3.09 standard deviation units (Fig 2), only the top thousandth of molecules will be yeses.  #@NEW_LINE#@#  One would expect to screen one thousand molecules per yes.  #@NEW_LINE#@#  Thus, higher decision thresholds depend on higher throughput, and it is higher throughput that makes higher decision thresholds possible.  #@NEW_LINE#@#  
In some parts of the paper we express stringency or throughput in terms of the probability that a randomly selected candidate lies at or above the decision threshold, yt.  #@NEW_LINE#@#  This is shown in Eq 6, where  is the cumulative distribution function of the standard normal distribution:  #@NEW_LINE#@#  
(6)  #@NEW_LINE#@#  
To be deemed to be a true positive, a candidate that is a yes on the basis of its score on the decision variable must then meet or exceed a threshold rt on the gold standard reference variable R. When rt is high, fewer candidate molecules within the set that is being searched by the R&D process have the potential to succeed (i.e., A/(A + U) declines as rt increases).  #@NEW_LINE#@#  Our definition of rt is statistical and is not discussed in terms of a specific trial endpoint or experimental outcome.  #@NEW_LINE#@#  However rt is realistic in the sense that it will tend to move up and down with common-sense measures of regulatory stringency, or with a common-sense view of the competitive intensity within a therapy area.  #@NEW_LINE#@#  In some parts of the paper we express the difficulty of the search process in terms of the probability that a randomly selected candidate lies at or above the reference threshold, rt:  #@NEW_LINE#@#  
(7)  #@NEW_LINE#@#  

Measures_of_Decision_Quality  #@NEW_LINE#@#  
The proportion of molecules which meets or crosses the decision threshold, yt, and which receives further scrutiny, corresponding to the probability mass to the right of the vertical dotted line in Fig 2, is:  #@NEW_LINE#@#  
(8)  #@NEW_LINE#@#  
The proportion of true positives, corresponding to the probability mass in the upper right quadrant of Fig 2, is given by:  #@NEW_LINE#@#  
(9)  #@NEW_LINE#@#  
The proportion of progression decisions which yield true positives is the positive predictive value, or PPV.  #@NEW_LINE#@#  The PPV of the classifier is:  #@NEW_LINE#@#  
(10)  #@NEW_LINE#@#  
PPV is an important measure of decision quality in drug R&D because the unit costs per surviving therapeutic candidate tend to rise through the R&D process [21].  #@NEW_LINE#@#  Thus, real-world portfolio management processes often seek to maximize PPV.  #@NEW_LINE#@#  Furthermore, PPV is equal to (1-FDR) where FDR is the false discovery rate.  #@NEW_LINE#@#  Health authorities such as the FDA and the European Medicines Agency (EMA) are often concerned to minimise the FDR, which is equivalent to maximising PPV.  #@NEW_LINE#@#  

A_Single_Decision_Step  #@NEW_LINE#@#  
Fig 3 illustrates of the performance of single decision step.  #@NEW_LINE#@#  When PV is high, the classifier can effectively distinguish between positives and negatives.  #@NEW_LINE#@#  When PV is low, it cannot.  #@NEW_LINE#@#  Fig 3 also illustrates some other typical classifier properties.  #@NEW_LINE#@#  There is usually a trade-off between TPR and FPR.  #@NEW_LINE#@#  When the classifier is stringent (i.e., applies a high decision threshold, which in turn requires a high throughput), the FPR tends to be low, but the TPR tends to be low too  #@NEW_LINE#@#  
(A) The bivariate normal probability density function for decision variable Y (horizontal axis) and reference variable R (vertical axis).  #@NEW_LINE#@#  The correlation between Y and R is high (Y,R = 0.95) so the decision variable has high PV.  #@NEW_LINE#@#  The graph shows only the positive quadrant of the distribution.  #@NEW_LINE#@#  The reference threshold, expressed here in units of standard deviation, is rt = 0.5 (dotted line) so positives are common, accounting for P(R  rt)  30% of the probability mass.  #@NEW_LINE#@#  (B) shows TPR (solid line) and FPR (dotted line) as the decision threshold, yt, varies.  #@NEW_LINE#@#  At some thresholds, the spread between the TPR and FPR is wide.  #@NEW_LINE#@#  (C) shows PPV vs. decision threshold, yt.  #@NEW_LINE#@#  (D) to (F) repeat the analyses with a decision variable with lower PV (Y,R = 0.4).  #@NEW_LINE#@#  PPV declines vs. panel (C) but PPV remains high because positives are common.  #@NEW_LINE#@#  (G) to (I) repeat that analysis at Y,R = 0.95 but with a high reference threshold (2.5 standard deviation units) and rare positives (P(R  rt)  0.6% of the probability mass).  #@NEW_LINE#@#  It is possible to achieve a high PPV, but only at a high decision threshold when the TPR is low, which would require screening a large number of items per positive detected.  #@NEW_LINE#@#  (J) to (L) show the situation with the same high reference threshold (i.e., rare positives) but with a decision variable with low PV.  #@NEW_LINE#@#  In this case, PPV is low, even with a very high decision threshold and a very low TPR.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0147215.g003  #@NEW_LINE#@#  
Fig 3 shows that stringency tends to raise PPV (and lower FDR), but setting a high decision threshold may not, in practical terms at least, rescue the performance of a classifier if the decision variable has low PV (Fig 3L).  #@NEW_LINE#@#  A more effective way to tune the decision process to raise parameter Q, the ratio of approvable to non approvable candidates at each step (Eqs 14), may be to improve the predictive validity of PMs (or to choose therapeutic problems where PV is likely to be high).  #@NEW_LINE#@#  
Fig 3 also shows that decision performance is sensitive to the reference threshold.  #@NEW_LINE#@#  When rt increases and positives become rarer, decision performance tends to becomes worse.  #@NEW_LINE#@#  Thus, as therapeutic standards within a therapy area rise, a constant set of PMs may appear to perform less well.  #@NEW_LINE#@#  

Sensitivity_Analysis_of_a_Single_Decision_Step  #@NEW_LINE#@#  
Fig 4 shows the PPV of the classifier as yt (stringency or throughput) and as Y,R (predictive validity of the decision variable) vary.  #@NEW_LINE#@#  It shows two conditions, one where the positives are relatively common (P(R  rt) = 0.01, or one percent of the candidates entering the classifier) and one where positives are rare (P(R  rt) = 105, or one hundred thousandth of the candidates entering the classifier).  #@NEW_LINE#@#  
Shading shows the PPV of the classifier (log10 units, with lighter shades showing better performance).  #@NEW_LINE#@#  The vertical axis represents both decision threshold and screening throughput.  #@NEW_LINE#@#  The scale is in log10 units.  #@NEW_LINE#@#  7 represents a throughput of 107 and a decision threshold that accepts only the top 107th of candidates (P(Y  yt) = 107, Eq 6); 6 represents a throughput of 106 and a decision threshold that accepts only the top 106th of candidates (P(Y  yt) = 106, Eq 6); etc.  #@NEW_LINE#@#  The horizontal axis represents PV as the correlation coefficient, Y,R, between Y and R, with the right hand end of each axis representing high PV (Y,R = 0.98), and the left hand end of each axis representing low PV (Y,R = 0).  #@NEW_LINE#@#  Our choice of scale for each axis is discussed in the main text.  #@NEW_LINE#@#  In (A), positives are relatively common.  #@NEW_LINE#@#  Here, P(R  rt) = 0.01, or one percent of the candidates entering the classifier.  #@NEW_LINE#@#  In (B), positives are relatively rare.  #@NEW_LINE#@#  Here, P(R  rt) = 105, or one hundred thousandth of the candidates entering the classifier.  #@NEW_LINE#@#  The spacing and orientation of the contours show the degree to which PPV changes with throughput and with Y,R.  #@NEW_LINE#@#  PPV is relatively sensitive to throughput when Y,R is high and when positives are very rare (lower right hand side of panel B.).  #@NEW_LINE#@#  However, PPV is relatively insensitive to throughput when Y,R is low (left hand side of both panels).  #@NEW_LINE#@#  For much of the parameter space illustrated, an absolute 0.1 change in Y,R (e.g., from 0.4 to 0.5, or 0.5 to 0.6 on the horizontal axis) has a larger effect on PPV than a 10x change in throughput (e.g., from 4 log10 units to 5 log10 units on the vertical axis).  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0147215.g004  #@NEW_LINE#@#  
For the single decision step, one can imagine the decision variable, Y, as representing an aggregate measure derived from the progressive screening, optimisation, and preclinical assessment of a large number of potential drug candidates.  #@NEW_LINE#@#  We think such aggregation is reasonable for the purposes of illustration.  #@NEW_LINE#@#  This is for two reasons.  #@NEW_LINE#@#  First, the FPR and TPR of a chain of classifiers are the products of the individual stepwise FPRs and TPRs (Eqs 14).  #@NEW_LINE#@#  Second, we find similar results for combinations of decision variables across multiple classification steps (see later).  #@NEW_LINE#@#  Note also that the results we show use parameters that are relevant for discovery and preclinical phases of commercial drug R&D, from which few candidates are selected for clinical trials and from which few randomly selected candidates would succeed in trials (i.e., P(R  rt)  0.1 and P(Y  yt)  0.1).  #@NEW_LINE#@#  The general model would be applicable to situations where many or even most molecules are positives, in late stage clinical development, for example.  #@NEW_LINE#@#  However, the quantitative results and conclusions would be different.  #@NEW_LINE#@#  Furthermore, there is already a mature literature that applies DT-related ideas to clinical development (see, for example: [35] [37] [36] [49])  #@NEW_LINE#@#  
The scale and range of the vertical axis in Fig 4 can be regarded as representing the range in brute force power or efficiency of PMs in drug R&D.  #@NEW_LINE#@#  One can conceptualize this in several ways, such as the growth over time in size of compound libraries that can be used in a screening campaign (e.g., from in vivo screening in the 1930s to high throughput screening circa 2015), or as the range in the cost efficiency (1/unit cost per therapeutic candidate tested) of PMs today (e.g., from human trials, via in vivo primate disease models, via in vitro cellular models to in silico protein structure based screening) [1] [74].  #@NEW_LINE#@#  
Several of the results in Fig 4 are unsurprising.  #@NEW_LINE#@#  First, PPV increases as Y,R, the correlation between Y and R, increases.  #@NEW_LINE#@#  Second, PPV increases if one applies very high yt thresholds (very high throughputs).  #@NEW_LINE#@#  Third, PPV is higher when the reference threshold for positives, rt, is lower.  #@NEW_LINE#@#  In other words, and rather obviously, there will be a lot of correct decisions to initiate clinical trials when we have PMs with very high PV, which can be reasonably be applied to a very large number of therapeutic candidates, a high proportion of which would have been good enough in the first place to yield successful clinical outcomes.  #@NEW_LINE#@#  
However, there are results which are less obvious but which appear important for the conduct of decision processes such as drug R&D.  #@NEW_LINE#@#  The first is the strength of the effect of Y,R on PPV (see orientation of the PPV contours in Fig 4, and note both the logarithmic vertical axis and the logarithmic colour scale).  #@NEW_LINE#@#  For much of the parameter space illustrated, an absolute 0.1 change in Y,R, the correlation coefficient, has a larger effect on PPV than a ten-fold or 1 log10 unit change in throughput (vertical axis).  #@NEW_LINE#@#  
We suggest that for many, perhaps most, people working with PMs in drug discovery, an 0.1 absolute change in the correlation between the output of two PMs, or between the decision variable from a PM and the reference variable, would ofteneven if it were known or knowablebe viewed as small; a difference that would be lost in the general experimental noise.  #@NEW_LINE#@#  On the other hand, most people would regard a 10 fold increase in throughput or a 10 fold decrease in the unit cost of a PM as a large change.  #@NEW_LINE#@#  
The second important result is the interaction between yt and Y,R on PPV (see how the orientation of the contours changes in Fig 4).  #@NEW_LINE#@#  Increasing throughput by several orders of magnitude has a minimal positive effect on PPV when Y,R is very low.  #@NEW_LINE#@#  Increasing throughput has a large positive effect on PPV only when Y,R is high.  #@NEW_LINE#@#  Modest gains in Y,R can have very large positive effect on PPV when throughput is high.  #@NEW_LINE#@#  
In practical terms, there is little point in investing to increase the throughput of a poor PM or the stringency of the classifier based on that PM.  #@NEW_LINE#@#  It makes more sense to invest to achieve high PV first.  #@NEW_LINE#@#  Furthermore, increasing the throughput of a good PM or the speed or stringency of R&D decisions only makes sense if such changes do not cause a meaningful reduction in PV.  #@NEW_LINE#@#  

Multiple_Decision_Steps  #@NEW_LINE#@#  
With more decision steps, the probability density of candidate molecules within measurement space is determined by the correlation matrix between multiple decision variables, W, X, Y, etc., and the reference variable R (Eq 8).  #@NEW_LINE#@#  Now, the probability that a molecule meets or exceeds a series of decision thresholds on a series of decision variables is given by integrating the probability density function across each variable from the appropriate threshold to infinity (it would, of course, be possible to apply other methods for combining the decision variables, but we do not consider them here).  #@NEW_LINE#@#  The proportion of true positives when applying 2 decision thresholds, xt and yt, to two decision variables, X and Y, corresponds to:  #@NEW_LINE#@#  
(11)  #@NEW_LINE#@#  
Note that the single classifiers quantitative performance depended on only 3 parameters; yt, rt, and Y,R.  #@NEW_LINE#@#  Now with two classifiers and a reference step, there are six parameters.  #@NEW_LINE#@#  These are the decision and reference thresholds (xt, yt, rt) and three correlation coefficients, one for each unique pairwise correlation; X,Y, X,R and Y,R.  #@NEW_LINE#@#  If there are n classification steps including the reference test, the number of parameters is given by:  #@NEW_LINE#@#  
(12)  #@NEW_LINE#@#  
Given the fact that the number of model parameters increases rapidly as the number of decision variables or decision steps increases, we touch on only two relatively simple examples of multiple decision steps here.  #@NEW_LINE#@#  The first is illustrated in Fig 5, which shows the consequence of stacking a series of similar classifiers.  #@NEW_LINE#@#  It is possible to increase PPV with several similar steps, but at the cost of reducing TPR, which means screening more candidates for each positive that the search ultimately yields.  #@NEW_LINE#@#  As with the single classification step (Figs 24) performance can be very sensitive to PV.  #@NEW_LINE#@#  So, for example, for the parameters shown, one classification step when the correlation between Y and R is  = 0.9 outperforms 2 classification steps when the correlations coefficients between X, Y and R are all 0.7 or 3 classification steps when the correlation coefficients are all 0.6.  #@NEW_LINE#@#  The single step at  = 0.9 yields both a higher TPR and a higher PPV having tested far fewer candidates.  #@NEW_LINE#@#  In this case, the number candidates screened per TP (Table 1) for correlations of  = 0.9, 0.7, and 0.6 are 14 (1 step), 33 (2 steps), and 70 (3 steps), respectively.  #@NEW_LINE#@#  
(A) Points represents decision performance with one, two, three, or four, similar classifiers applied in series.  #@NEW_LINE#@#  Each line represents the same value of correlation coefficient, , applied to all pairwise relationships between decision variables and between decision variables and R. Thus in each line, all decision variables are equally correlated with each other and with R. The correlation coefficient between decision variables (X, Y, W, Z) and R vary from 0.9 (high PV, top right line) to 0.3 (low PV, bottom left line).  #@NEW_LINE#@#  The top left point on each line shows a single classifier applied to X, with each additional point towards the bottom and right of each line showing the effects of adding an additional classifier, up to a maximum of 4 classifiers.  #@NEW_LINE#@#  The top decile of candidates in the starting set exceed each decision threshold and the reference threshold (i.e., P(X  xt) = P(Y  yt) = P(W  wt) = P(Z  zt) = P(R  rt) = 0.1).  #@NEW_LINE#@#  In general, adding more steps increases PPV but at the cost of a lower TPR.  #@NEW_LINE#@#  There are diminishing returns from each additional classifier, particularly when the decision variables are highly correlated with one another.  #@NEW_LINE#@#  Furthermore, a single classifier that is highly correlated with R (e.g., the uppermost points on the lines with high correlation coefficients) often outperforms a combination of several classifiers with lower correlations with R in terms of both PPV and TPR.  #@NEW_LINE#@#  Note the logarithmic vertical axis.  #@NEW_LINE#@#  (B) is exactly as (A) but shows on the vertical axis the number of candidates screened per TP (Table 1).  #@NEW_LINE#@#  The number of candidates that must be screened per true positive identified increases as  (PV) declines because positives are wrongly rejected.  #@NEW_LINE#@#  Increasing  (PV) increases search efficiency.  #@NEW_LINE#@#  Note the logarithmic vertical axis.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0147215.g005  #@NEW_LINE#@#  
Fig 5 also illustrates the large effect of correlations between serial decision variables (Fig 1B).  #@NEW_LINE#@#  When the correlation between serial decision and reference variables is high, attrition rates at steps later in the process tend to be low, because a candidate that passes through one decision step is likely to pass the next.  #@NEW_LINE#@#  In Fig 5, stringency, the proportion of the starting candidates that exceed each decision threshold and the reference threshold, is constant across the different conditions.  #@NEW_LINE#@#  However, there are large differences in overall attrition rates, expressed as the number of candidates screened per TP in Fig 5B.  #@NEW_LINE#@#  When the correlation coefficients are 0.9, a four step process would screen 22 candidates per TP, and the FDR would be a mere 9% (PPV = 91%).  #@NEW_LINE#@#  When the correlation coefficients are 0.3, a four step process would screen ~739 candidates per TP, and the FDR would be 52% (PPV = 48%).  #@NEW_LINE#@#  
Fig 6 illustrates some of the effects on decision performance of varying the correlation, Y,X, between two decision variables, X and Y, and varying the correlation, Y,R, between decision variable Y and the reference variable R.  #@NEW_LINE#@#  
The first decision variable was X, and the correlation coefficient between X and R, X,R, was held constant at 0.5.  #@NEW_LINE#@#  The second decision variable was Y which varied in terms of its correlation with X (Y,X, vertical axes) and with reference variable R (Y,R, horizontal axes).  #@NEW_LINE#@#  Some regions of the graphs are empty because certain combinations of correlation coefficients cannot coexist.  #@NEW_LINE#@#  The top decile of candidates in the starting set exceed each decision threshold and the reference threshold (i.e., P(X  xt) = P(Y  yt) = P(R  rt) = 0.1).  #@NEW_LINE#@#  (A) shows PPV.  #@NEW_LINE#@#  Lighter shades indicate higher PPV.  #@NEW_LINE#@#  PPV increases as Y,R increases and as Y,X declines.  #@NEW_LINE#@#  The use of Y may depress PPV if Y is highly correlated with X while having a low correlation with R. (B) shows the number of candidates screened per TP.  #@NEW_LINE#@#  Darker shades indicate fewer candidates per TP.  #@NEW_LINE#@#  Note the log10 colour scale.  #@NEW_LINE#@#  The number increases as Y,R declines and as Y,X declines.  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0147215.g006  #@NEW_LINE#@#  
Fig 6 shows, first, that PPV increases and the number of candidates screened per TP decreases with an increase in Y,R.  #@NEW_LINE#@#  Things are better if the second decision variable is highly correlated with R. This is no great surprise.  #@NEW_LINE#@#  
Second, and less intuitively obvious perhaps, is the fact that PPV increases but the screening effort also increases as the correlation between the two decision variables, Y,X, decreases.  #@NEW_LINE#@#  The effect of changes in Y,X, independent of the degree to which either measure correlates with R, can be powerful (vertical axes in Fig 6).  #@NEW_LINE#@#  This is why counter-screening works [75] [76] and why absorption, distribution, metabolism, and excretion (ADME), toxicology, and efficacy measures, are much more informative when combined.  #@NEW_LINE#@#  However, the cost of combining variables that are uncorrelated with each other can be a large increase in the number of candidates screened per TP, because few candidates will score well on several independent measures.  #@NEW_LINE#@#  
It may also surprise some that the addition of a second decision variable and classifier can depress PPV.  #@NEW_LINE#@#  This occurs if the second decision variable is highly correlated with the first, but has a low correlation with the reference variable, R. In practical terms, this shows that PMs cannot be regarded as valid simply because their output correlates with the output of other PMs.  #@NEW_LINE#@#  It may often make sense to seek out and add PMs that have face validity versus R but which yield decision variables that have a low correlation with other decision variables.  #@NEW_LINE#@#  


Discussion  #@NEW_LINE#@#  
1_The_Exhaustion_and_Abandonment_of_High_PV_Models  #@NEW_LINE#@#  
This paper was motivated by a desire to explain Erooms Law [1]: The approximate halving every 9 years between 1950 and 2010 in the number of new drug molecules approved by the FDA per billion dollars of inflation-adjusted R&D investment by the drug industry, in the face of huge gains in knowledge and in brute-force efficiency.  #@NEW_LINE#@#  
One standard explanation for Erooms Law is that the low hanging fruit have been picked.  #@NEW_LINE#@#  We and others have been critical of such explanations [77] [1].  #@NEW_LINE#@#  First, they generally leave the nature of the fruit undefined (but there are exceptions [78]).  #@NEW_LINE#@#  Second, such explanations may underestimate the difficulty of historical discoveries [77] [24] [1].  #@NEW_LINE#@#  Third, drugs that come to market reduce the incremental economic and therapeutic value of undiscovered or unexploited therapeutic candidates without making such candidates harder to discover per se.  #@NEW_LINE#@#  This is the so-called better than the Beatles problem [1].  #@NEW_LINE#@#  Fourth, low hanging fruit explanations risk tautology, because they use the efficiency of R&D as the measure of the height at which as-yet-unpicked fruits are hanging [1].  #@NEW_LINE#@#  
However, the analyses in this paper suggest what may be an important kind of fruit.  #@NEW_LINE#@#  Changes in the PV of decision variables that many people working in drug discovery would regard as small and/or unknowable (i.e., a 0.1 absolute change in correlation coefficient versus clinical outcome) can offset large (e.g., 10 fold or greater) changes in brute-force efficiency.  #@NEW_LINE#@#  Furthermore, the benefits brute-force efficiency decline as the PV of decision variables declines (left hand side of both panels in Fig 4).  #@NEW_LINE#@#  It is our hypothesis, therefore, that much of the decline in R&D efficiency has been caused by the progressive exhaustion of PMs that are highly predictive of clinical utility in man.  #@NEW_LINE#@#  These models are abandoned because they yield successful treatments.  #@NEW_LINE#@#  Research shifts to diseases for which there are poor PMs with low PV [78].  #@NEW_LINE#@#  Since these diseases remain uncured, people continue to use bad models for want of anything better.  #@NEW_LINE#@#  A decline in the average PV of the stock of unexploited screening and disease models (PMs) can offset huge gains in their brute-force power (Fig 4).  #@NEW_LINE#@#  
We also suspect that there has been too much enthusiasm for highly reductionist PMs with low PV [26] [79] [25] [80] [81] [74] [82].  #@NEW_LINE#@#  The first wave of industrialized target-based drug discovery has been, in many respects, the embodiment of such reductionism [1] [83] [84] [74].  #@NEW_LINE#@#  The problem is not necessarily reductionism itself.  #@NEW_LINE#@#  Rather, it may be that good reductionist models have been difficult to produce, identify, and implement [85] [82], so there has been a tendency to use bad ones instead; particularly for common diseases, which tend to have weak and/or complex genetic risk factors [86] [83] [87].  #@NEW_LINE#@#  After all, brute-force efficiency metrics are relatively easy to generate, to report up the chain of command, and to manage.  #@NEW_LINE#@#  The PV of a new screening technology or animal PM, on the other hand, is an educated guess at best.  #@NEW_LINE#@#  In the practical management of large organisations, what is measureable and concrete can often trump that which is opaque and qualitative [65], even if that which is opaque and qualitative is much more important in quantitative terms.  #@NEW_LINE#@#  
We note here what appears to be a real uptick in drug approvals from ~2012.  #@NEW_LINE#@#  We think this reflects the ability of modern methods to increase the PV of models for specific cancer subtypes and other rare diseases with strong and simple genetic risk factors [83].  #@NEW_LINE#@#  Molecular diagnostics, for example, make it easier to match reductionist PMs domains of validity with human pathology in these rare diseases.  #@NEW_LINE#@#  
The history of drug discovery also points to the importance of PV over throughput.  #@NEW_LINE#@#  During the Golden Age of therapeutic innovation [24], some drug R&D resembled phenotypic screening in man.  #@NEW_LINE#@#  Throughput was low, mechanistic understanding was limited, experimental design and conduct (e.g., randomisation, blinding, etc.)  #@NEW_LINE#@#  often left much to be desired when compared with modern standards, but the decision variables (i.e., observations of clinical responses in humans) had high PV for the reference variable (i.e., clinical responses in humans) [24] [88] [89] [69].  #@NEW_LINE#@#  Even in modern times, field discovery by practicing physicians appears to be a major, if under-appreciated, source of pharmacological innovation [90] that occurs in the face of remarkably low drug throughput.  #@NEW_LINE#@#  There are, after all, only in the order of 1,000 approved drug molecules whose effects in man can be observed by physicians [91].  #@NEW_LINE#@#  
We hypothesize that the rate of creation of valid and reliable PMs may be the major constraint on industrial R&D efficiency today [16] [92].  #@NEW_LINE#@#  If this hypothesis is even partly true, it points to a mismatch between those areas where valuable intellectual property is relatively easy to secure (e.g., novel chemical structures) and those areas where incremental investment would be most useful for the wider good (i.e., good PMs for poorly treated conditions).  #@NEW_LINE#@#  

2_The_Reproducibility_Crisis_and_Predictive_Validity  #@NEW_LINE#@#  
It is common to think of validity and reproducibility or reliability as different things (Table 1).  #@NEW_LINE#@#  After all, the existence of reference tests against which the output of a model may or may not correlate is irrelevant for whether or not the results of that model are consistent when it is repeatedly applied.  #@NEW_LINE#@#  However, as with Erooms Law [1] (above), we hypothesize that the academic reproducibility crisis [13] [92] [93] [94] could reflect the abandonment of models with high PV, for reasons of exhaustion and/or scientific fashion.  #@NEW_LINE#@#  
Our argument is illustrated in Fig 7.  #@NEW_LINE#@#  Imagine retiring the models with high PV, which are those at the right hand end of the horizontal axis in Fig 7D.  #@NEW_LINE#@#  These are the models most likely to give answers that are obvious and useful, thus rendering themselves redundant.  #@NEW_LINE#@#  As the high PV models are progressively retired, the average signal to noise ratio and the average test-retest reliability of the remaining stock of models falls (regression line and vertical axis, Fig 7D).  #@NEW_LINE#@#  With a lower signal to noise ratio in the remaining stock, the play of chance [13] [29] [72] and professional biases [95] [96] [94] can start to exert more visible effects on the quality of the scientific literature.  #@NEW_LINE#@#  
The figure shows the results of a Monte Carlo simulation (see S1 File for code).  #@NEW_LINE#@#  (A) Each small point represents one simulated screening or disease model (PM).  #@NEW_LINE#@#  When testing therapeutic candidates, each PM yields an expected signal which is the sum of two components.  #@NEW_LINE#@#  The first component is the signal from the reference test multiplied by a gain parameter (horizontal axis).  #@NEW_LINE#@#  The second component is a model-specific signal, whose gain is shown on the vertical axis.  #@NEW_LINE#@#  This component can also be thought of as systematic model-specific bias.  #@NEW_LINE#@#  It is real, but it tells us nothing about the reference test.  #@NEW_LINE#@#  (B) Each models PV is determined by the relative strength of the reference component versus the model-specific component of the signal.  #@NEW_LINE#@#  PV is high when the reference component is much larger than the model-specific component of the signal.  #@NEW_LINE#@#  This is because the output of the PM will correlate with the reference test when its signal is dominated by the reference signal.  #@NEW_LINE#@#  (C) Each PMs signal to noise ratio increases with the sum of the reference component and the model-specific component.  #@NEW_LINE#@#  (D) Each point represents the performance of one of the models in Panel A., in two simulated experiments that include sampling and measurement noise.  #@NEW_LINE#@#  The horizontal axis shows the results of the first experiment.  #@NEW_LINE#@#  It is sample predictive validity (the correlation coefficient between the output of the model and the output of the reference test for a sample of therapeutic candidates).  #@NEW_LINE#@#  The vertical axis is the second experiment.  #@NEW_LINE#@#  It is test-retest reliability using the same sample of therapeutic candidates (calculated as the correlation coefficient between the results of the test and retest).  #@NEW_LINE#@#  The symbols (star, diamond, triangle, and cross) show how the space in (A) maps onto the space in (D).  #@NEW_LINE#@#  The line in (D) shows the best fit for the linear regression between sample PV and test-retest reliability.  #@NEW_LINE#@#  For the simulation shown, we sampled 400 therapeutic candidates for each PM.  #@NEW_LINE#@#  Both the reference and model-specific components of PMs signal were drawn from a normally distributed random variable, whose mean was zero and whose standard deviations were equal to the respective gains on the horizontal and vertical axes of (A) to (C).  #@NEW_LINE#@#  


              https://doi.org/10.1371/journal.pone.0147215.g007  #@NEW_LINE#@#  

3_Improving_Predictive_Validity  #@NEW_LINE#@#  
If one accepts the main conclusion we draw from this paper, that PV has a very powerful effect on R&D decision quality and productivity, one is leftas our reviewers pointed outwith a difficult but important question: Can you estimate PV prospectively, or improve the PV of models in as-yet uncharted therapy areas?  #@NEW_LINE#@#  
Measuring and managing PV is difficult for several reasons.  #@NEW_LINE#@#  It is impossible to test a large number of candidates across multiple PMs and then in man.  #@NEW_LINE#@#  It is impossible, therefore, to measure PV with high precision, even in mature therapy areas.  #@NEW_LINE#@#  Furthermore, by the time a therapy area is mature, there is less reason to invest in calibrating PMs.  #@NEW_LINE#@#  This means PV estimates will be, at best, educated guesses.  #@NEW_LINE#@#  None the less, we do have some suggestions.  #@NEW_LINE#@#  
First, we suspect that experienced scientists often have an intuition about the PV of the models at their disposal, but today make the wrong trade-off between PV and unit cost, throughput, convenience, or scientific fashion.  #@NEW_LINE#@#  They should give more weight to their own expert judgement of PV, even if this means screening an order of magnitude fewer therapeutic candidates or writing far fewer papers.  #@NEW_LINE#@#  Funding decisions must support this behaviour by prioritising the quality of argumentation around PM choice and PV.  #@NEW_LINE#@#  
Second, we suspect that much useful information on PV is neither captured, nor systematized, nor communicated to those making R&D decisions.  #@NEW_LINE#@#  Between [97], and even within [71] [98], biomedical disciplines, validity-related and reliability-related terminology and concepts are inconsistently applied.  #@NEW_LINE#@#  This means that groups of people who work together (e.g., when reviewing grant applications or project proposals) should discuss and agree a lingua franca for validity and reliability-related concepts.  #@NEW_LINE#@#  Here we have been struck by work on data pedigree [99] in the field of environmental risk assessment.  #@NEW_LINE#@#  Environmental policy decisions are sometimes science-based but often politically contentious.  #@NEW_LINE#@#  Therefore, it is important to communicate the pedigree of models along with the results that they yield.  #@NEW_LINE#@#  Pedigree would consider factors such as the extent to which the model is based on well-established theoretical frameworks (similar to the concept of construct validity [100]); etc.  #@NEW_LINE#@#  We recommend work to develop and apply concepts of data pedigree to the results derived from screening and disease models.  #@NEW_LINE#@#  
There is a wonderful term, domains of validity that is widely used in physics but which used little, if at all, in biomedical research.  #@NEW_LINE#@#  It refers to the parameter space within which a model is valid.  #@NEW_LINE#@#  For example, classical mechanics has a large and clear domain of PV, which includes the trajectory of a jumping flea, the orbits of the moons of Neptune, but not way stars gravity bends light, nor the way electrons move around atoms.  #@NEW_LINE#@#  People know this and apply classical mechanics accordingly.  #@NEW_LINE#@#  Efficient drug R&D requires domains of PV at each step that extend to clinical utility in man (Fig 1B).  #@NEW_LINE#@#  PMs that may be competently reported and reproducible in a narrow technical sense can fail because their domains of validity are too narrow [101]; they work, but are not usefully generalizable.  #@NEW_LINE#@#  Biomedical journals should therefore require that authors sketch out and justify the domains of validity of the PMs they use.  #@NEW_LINE#@#  
Third, and finally, we recommend investment in empirical studies of the PV of screening and disease models across a diverse set of diseases for which we have at least some approved drugs.  #@NEW_LINE#@#  This should include analysis of the correlations between of the outputs of different, preferably sequential, PMs, and qualitative analyses of the PMs themselves, and of how they are used to make R&D decisions.  #@NEW_LINE#@#  There is already work in this general area (e.g., references [80] [102] [56] [15] [14] [16] [32] [101] [103] [104] [62] [105] [106] [107]), but there is not enough.  #@NEW_LINE#@#  We also suggest the production of standard collections of drugs and chemical probes that can be used, therapy area by therapy area, to cross-calibrate PMs [101].  #@NEW_LINE#@#  The long-run aim should be to derive and back-test meta-modelsqualitative or narrative in the first instance (e.g., references [80] [102])that are themselves predictive of screening and disease models predictive validity.  #@NEW_LINE#@#  


Supporting_Information  #@NEW_LINE#@#  
S1_File_Mathematica_90_code_to_reproduce_analyses_in_Figs_27  #@NEW_LINE#@#  
https://doi.org/10.1371/journal.pone.0147215.s001  #@NEW_LINE#@#  
(ZIP)  #@NEW_LINE#@#  

S2_File_Analysis_of_alternative_probability_density_functions  #@NEW_LINE#@#  
https://doi.org/10.1371/journal.pone.0147215.s002  #@NEW_LINE#@#  
(PDF)  #@NEW_LINE#@#  


Acknowledgments  #@NEW_LINE#@#  
We thank James Geddes for statistical advice.  #@NEW_LINE#@#  We thank Brendan Jackson for introducing us to the term domains of validity.  #@NEW_LINE#@#  We thank William Bains, Nicholas Edwards, Richard Barker, Joyce Tait, Brian Warrington, Geoff Lawton, Chas Bountra, Paul Brennan, Stephan Knapp, Henry Stott, Kostas Paraschakis, and Mark Latham for discussions and comments on drafts of the paper and/or the ideas it contains.  #@NEW_LINE#@#  

Author_Contributions  #@NEW_LINE#@#  
Conceived and designed the experiments: JWS JB.  #@NEW_LINE#@#  Performed the experiments: JWS.  #@NEW_LINE#@#  Analyzed the data: JWS.  #@NEW_LINE#@#  Contributed reagents/materials/analysis tools: JWS JB.  #@NEW_LINE#@#  Wrote the paper: JWS.  #@NEW_LINE#@#  Produced the supplementary materials: JWS.  #@NEW_LINE#@#  

References  #@NEW_LINE#@#  


